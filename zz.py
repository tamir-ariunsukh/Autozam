# -*- coding: utf-8 -*-
# ============================================================
# –ó–∞–º —Ç—ç—ç–≤—Ä–∏–π–Ω –æ—Å–æ–ª ‚Äî Auto ML & Hotspot Dashboard (Streamlit)
# –•—É–≤–∏–ª–±–∞—Ä: 2025-08-17r2 ‚Äî —Ö—É—Ä–¥, —Ç–æ–≥—Ç–≤–æ—Ä—Ç–æ–π –∞–∂–∏–ª–ª–∞–≥–∞–∞ —Å–∞–π–∂—Ä—É—É–ª—Å–∞–Ω
# –ì–æ–ª ”©”©—Ä—á–ª”©–ª—Ç“Ø“Ø–¥:
#  - st.title() –∞–ª–¥–∞–∞, –¥–∞–≤—Ö–∞—Ä–¥—Å–∞–Ω import-—É—É–¥ –∑–∞—Å–∞–≤
#  - @st.cache_data –¥–æ—Ç–æ—Ä—Ö UI/stop-—É—É–¥—ã–≥ –∞–≤—á, –∑”©–≤ –≥–∞–¥–∞–∞ –±–∞—Ä–∏–≤
#  - SHAP-–¥ –¥—ç—ç–∂–ª—ç–ª—Ç–∏–π–Ω —Ö–∞–º–≥–∞–∞–ª–∞–ª—Ç, –∞–ª–¥–∞–∞–Ω—ã —Ö–∞–º–≥–∞–∞–ª–∞–ª—Ç –Ω—ç–º–∂ —Ö—É—Ä–¥–∞—Å–≥–∞–≤
#  - ML scale/reshape –∑”©–≤ –¥–∞—Ä–∞–∞–ª–∞–ª + ensemble-—É—É–¥—ã–≥ n_jobs=-1 –±–æ–ª–≥–æ–∂ –ø–∞—Ä–∞–ª–ª–µ–ª—á–∏–ª—Å–∞–Ω
#  - –ü—Ä–æ–≥–Ω–æ–∑—ã–Ω ‚Äú”©–¥”©—Ä‚Äù –≥–æ—Ä–∏–º—ã–≥ —Ç–∞–π–ª–±–∞—Ä–ª–∞–∂, —Å–∞—Ä—ã–Ω –∞–≥—Ä–µ–≥–∞—Ç—Ç–∞–π –Ω–∏–π—Ü“Ø“Ø–ª—ç–Ω —Ö—ç—Ä—ç–≥–∂“Ø“Ø–ª—ç–≤
#  - DBSCAN eps-–∏–π–≥ –ú–ï–¢–†-—ç—ç—Ä —É–¥–∏—Ä–¥–∞–∂ (–¥–æ—Ç–æ—Ä –Ω—å —Ä–∞–¥–∏–∞–Ω —Ä—É—É —Ö”©—Ä–≤“Ø“Ø–ª–Ω—ç), MarkerCluster –∞—à–∏–≥–ª–∞–≤
#  - Binary importance: -1 –∫–ª–∞—Å—Ç–µ—Ä—ã–≥ —Ö–∞—Å—á, dtype/NA —Ö–∞–º–≥–∞–∞–ª–∞–ª—Ç —Ö–∏–π–≤
# ============================================================

from __future__ import annotations
import io
import numpy as np
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
from pathlib import Path

# Sklearn
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import (
    RandomForestRegressor,
    GradientBoostingRegressor,
    AdaBoostRegressor,
    ExtraTreesRegressor,
    StackingRegressor,
    HistGradientBoostingRegressor,
    VotingRegressor,
)
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.cluster import DBSCAN

# 3rd-party regressors (optional)
try:
    from xgboost import XGBRegressor  # type: ignore
except Exception:
    XGBRegressor = None
try:
    from lightgbm import LGBMRegressor  # type: ignore
except Exception:
    LGBMRegressor = None
try:
    from catboost import CatBoostRegressor  # type: ignore
except Exception:
    CatBoostRegressor = None

from scipy.stats import chi2_contingency

import folium
from folium.plugins import MarkerCluster
from streamlit_folium import st_folium

import matplotlib.cm as cm
import matplotlib.colors as mcolors

# -------------------------- UI setup --------------------------
st.set_page_config(page_title="–û—Å–æ–ª ‚Äî Auto ML & Hotspot (auto-binary)", layout="wide")
st.title("–°.–¶–æ–ª–º–æ–Ω, –ê.–¢–∞–º–∏—Ä –Ω–∞—Ä—ã–Ω —Ö–∞—Ä —Ü—ç–≥–∏–π–Ω —Å—É–¥–∞–ª–≥–∞–∞ 2025-08-18")

# -------------------------- –¢—É—Å–ª–∞—Ö —Ñ—É–Ω–∫—Ü—É—É–¥ --------------------------
def _canon(s: str) -> str:
    return "".join(str(s).lower().split()) if isinstance(s, str) else str(s)

def resolve_col(df: pd.DataFrame, candidates) -> str | None:
    """–ù—ç—Ä—à–ª–∏–π–≥ robust –±–∞–π–¥–ª–∞–∞—Ä –æ–ª–Ω–æ (whitespace/—Ç–æ–º-–∂–∏–∂–∏–≥ “Ø–ª —Ç–æ–æ–Ω–æ)."""
    lut = {_canon(c): c for c in df.columns}
    for name in candidates:
        key = _canon(name)
        if key in lut:
            return lut[key]
    return None

def is_binary_series(s: pd.Series) -> bool:
    vals = pd.to_numeric(s.dropna(), errors="coerce").dropna().unique()
    if len(vals) == 0:
        return False
    return set(np.unique(vals)).issubset({0, 1})

def plot_correlation_matrix(df, title, columns):
    n_unique = df[columns].nunique()
    if all((n_unique == 2) | (n_unique == 1)):
        st.warning("–°–æ–Ω–≥–æ—Å–æ–Ω –±–∞–≥–∞–Ω—É—É–¥ one-hot (0/1) —Ç—É–ª –∫–æ—Ä—Ä–µ–ª—è—Ü–∏ —Ç—É–π–ª—à—Ä–∞—Ö –º—ç—Ç —Ö–∞—Ä–∞–≥–¥–∞–∂ –±–æ–ª–Ω–æ.")
    df_encoded = df[columns].copy()
    for col in df_encoded.columns:
        if df_encoded[col].dtype == "object":
            df_encoded[col] = pd.Categorical(df_encoded[col]).codes
    corr_matrix = df_encoded.corr()
    corr_matrix = corr_matrix.iloc[::-1]
    fig, ax = plt.subplots(figsize=(max(8, 1.5*len(columns)), max(6, 1.2*len(columns))))
    sns.heatmap(corr_matrix, annot=True, cmap="coolwarm", vmin=-1, vmax=1, center=0, ax=ax, fmt=".3f")
    ax.set_title(title)
    plt.tight_layout()
    return fig

# -------------------------- ”®–≥”©–≥–¥”©–ª –∞—á–∞–∞–ª–∞–ª—Ç --------------------------
uploaded_file = st.sidebar.file_uploader("Excel —Ñ–∞–π–ª –æ—Ä—É—É–ª–∞—Ö (.xlsx)", type=["xlsx"])

@st.cache_data(show_spinner=True)
def load_data(file=None, default_path: str = "–∫–æ–¥–ª–æ–≥–¥—Å–æ–Ω.xlsx"):
    """Excel –¥–∞—Ç–∞ —É–Ω—à–∏–Ω–∞. –≠–Ω–¥ UI/stop —Ö–∏–π—Ö–≥“Ø–π, –∞–ª–¥–∞–∞–≥ raise —Ö–∏–π–Ω—ç."""
    if file is not None:
        df = pd.read_excel(file)
    else:
        local = Path(default_path)
        if not local.exists():
            raise FileNotFoundError(f"Excel —Ñ–∞–π–ª –æ–ª–¥—Å–æ–Ω–≥“Ø–π: {default_path}")
        df = pd.read_excel(local)

    # –ù—ç—Ä—à–∏–ª —Ü—ç–≤—ç—Ä–ª—ç–≥—ç—ç
    df.columns = [c.strip() if isinstance(c, str) else c for c in df.columns]

    # –û–≥–Ω–æ–æ –±–∞–≥–∞–Ω–∞ —Ö–∞–π—Ö
    recv_col = resolve_col(df, ["–•“Ø–ª—ç—ç–Ω –∞–≤—Å–∞–Ω", "–•“Ø–ª—ç—ç–Ω –∞–≤—Å–∞–Ω ", "–û–≥–Ω–æ–æ", "–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ",
                                "–û—Å–æ–ª –æ–≥–Ω–æ–æ", "–û—Å–ª—ã–Ω –æ–≥–Ω–æ–æ", "Date"])
    if recv_col is None:
        raise ValueError("–û–≥–Ω–æ–æ–Ω—ã –±–∞–≥–∞–Ω–∞ –æ–ª–¥—Å–æ–Ω–≥“Ø–π. –ñ–∏—à—ç—ç: '–•“Ø–ª—ç—ç–Ω –∞–≤—Å–∞–Ω'.")

    # '–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ' “Ø“Ø—Å–≥—ç—Ö
    df["–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ"] = pd.to_datetime(df[recv_col], errors="coerce")
    if df["–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ"].isna().all():
        raise ValueError("–û–≥–Ω–æ–æ–≥ parse —Ö–∏–π–∂ —á–∞–¥—Å–∞–Ω–≥“Ø–π. –û–≥–Ω–æ–æ–Ω—ã —Ñ–æ—Ä–º–∞—Ç —à–∞–ª–≥–∞–Ω–∞ —É—É.")

    df["Year"]  = df["–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ"].dt.year
    df["Month"] = df["–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ"].dt.month
    df["Day"]   = df["–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ"].dt.day_name()

    # –û–Ω –∂–∏–ª“Ø“Ø–¥–∏–π–Ω one-hot
    years = sorted(df["Year"].dropna().unique().tolist())
    for y in years:
        df[f"–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ –∂–∏–ª {int(y)}"] = (df["Year"] == int(y)).astype(int)
    if len(years) > 0:
        df["–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ –∂–∏–ª (min-max)"] = df["Year"].between(min(years), max(years)).astype(int)

    # –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç –Ω—ç—Ä—à–∏–ª
    lat_col = resolve_col(df, ["”®—Ä–≥”©—Ä”©–≥", "lat", "latitude"])
    lon_col = resolve_col(df, ["–£—Ä—Ç—Ä–∞–≥", "lon", "longitude"])

    # –ê–≤—Ç–æ–º–∞—Ç–∞–∞—Ä binary –±–∞–≥–∞–Ω—É—É–¥—ã–≥ –∏–ª—Ä“Ø“Ø–ª—ç—Ö
    exclude = {"–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ", "Year", "Month", "Day", "–¥/–¥", "–•–æ—Ä–æ–æ-–°—É–º", "–ê–π–º–∞–≥-–î“Ø“Ø—Ä—ç–≥"}
    if lat_col: exclude.add(lat_col)
    if lon_col: exclude.add(lon_col)
    binary_cols = [c for c in df.columns if c not in exclude and is_binary_series(df[c])]

    # –ù—ç–º—ç–ª—Ç —Ç–æ–æ–Ω candidate-—É—É–¥
    numeric_candidates = []
    if "–ê–≤—Ç–æ –∑–∞–º - –ó–æ—Ä—á–∏—Ö —Ö—ç—Å–≥–∏–π–Ω ”©—Ä–≥”©–Ω" in df.columns:
        numeric_candidates.append("–ê–≤—Ç–æ –∑–∞–º - –ó–æ—Ä—á–∏—Ö —Ö—ç—Å–≥–∏–π–Ω ”©—Ä–≥”©–Ω")

    # –î“Ø“Ø—Ä—ç–≥/–ê–π–º–∞–≥ fallback
    if "–î“Ø“Ø—Ä—ç–≥" not in df.columns:
        df["–î“Ø“Ø—Ä—ç–≥"] = 0
    if "–ê–π–º–∞–≥" not in df.columns:
        df["–ê–π–º–∞–≥"] = 0

    meta = {
        "lat_col": lat_col,
        "lon_col": lon_col,
        "binary_cols": binary_cols,
        "numeric_candidates": numeric_candidates,
        "years": years,
    }
    return df, meta

# === load ===
try:
    df, meta = load_data(uploaded_file)
except Exception as e:
    st.error(f"”®–≥”©–≥–¥”©–ª –∞—á–∞–∞–ª–∞—Ö–∞–¥ –∞–ª–¥–∞–∞: {e}")
    st.stop()

lat_col, lon_col = meta["lat_col"], meta["lon_col"]
binary_cols = meta["binary_cols"]
num_additional = meta["numeric_candidates"]
years = meta["years"]

# -------------------------- Target —Ç–æ—Ö–∏—Ä–≥–æ–æ --------------------------
st.sidebar.markdown("### üéØ –ó–æ—Ä–∏–ª—Ç–æ—Ç —Ç–æ–¥–æ—Ä—Ö–æ–π–ª–æ–ª—Ç (–û—Å–æ–ª)")
target_mode = st.sidebar.radio(
    "–û—Å–æ–ª –≥—ç–∂ —Ç–æ–æ—Ü–æ—Ö –∞–Ω–≥–∏–ª–ª—ã–≥ —Å–æ–Ω–≥–æ–Ω–æ —É—É:",
    ("–•–æ—ë—É–ª–∞–Ω–≥ 1 –≥—ç–∂ —Ç–æ–æ—Ü", "–ó”©–≤—Ö”©–Ω –ì—ç–º—Ç —Ö—ç—Ä—ç–≥", "–ó”©–≤—Ö”©–Ω –ó”©—Ä—á–ª–∏–π–Ω —Ö—ç—Ä—ç–≥"),
)

torol_col = resolve_col(df, ["–¢”©—Ä”©–ª"])
if torol_col is None:
    st.error("`–¢”©—Ä”©–ª` –±–∞–≥–∞–Ω–∞ –æ–ª–¥—Å–æ–Ω–≥“Ø–π. Target “Ø“Ø—Å–≥—ç—Ö –±–æ–ª–æ–º–∂–≥“Ø–π.")
    st.stop()

if target_mode == "–•–æ—ë—É–ª–∞–Ω–≥ 1 –≥—ç–∂ —Ç–æ–æ—Ü":
    df["–û—Å–æ–ª"] = df[torol_col].isin(["–ì—ç–º—Ç —Ö—ç—Ä—ç–≥", "–ó”©—Ä—á–ª–∏–π–Ω —Ö—ç—Ä—ç–≥"]).astype(int)
elif target_mode == "–ó”©–≤—Ö”©–Ω –ì—ç–º—Ç —Ö—ç—Ä—ç–≥":
    df["–û—Å–æ–ª"] = (df[torol_col] == "–ì—ç–º—Ç —Ö—ç—Ä—ç–≥").astype(int)
else:
    df["–û—Å–æ–ª"] = (df[torol_col] == "–ó”©—Ä—á–ª–∏–π–Ω —Ö—ç—Ä—ç–≥").astype(int)

# -------------------------- 5. –ò—Ä—ç—ç–¥“Ø–π–Ω –æ—Å–ª—ã–Ω —Ç–∞–∞–º–∞–≥–ª–∞–ª --------------------------
st.header("5. –ò—Ä—ç—ç–¥“Ø–π–Ω –æ—Å–ª—ã–Ω —Ç–∞–∞–º–∞–≥–ª–∞–ª (–û–ª–æ–Ω ML/DL –∑–∞–≥–≤–∞—Ä)")
st.caption("Binary (0/1) –±–∞–≥–∞–Ω—É—É–¥ –∞–≤—Ç–æ–º–∞—Ç–∞–∞—Ä –∏–ª—Ä“Ø“Ø–ª–∂ –∞—à–∏–≥–ª–∞–≥–¥–∞–Ω–∞. –ü—Ä–æ–≥–Ω–æ–∑ **—Å–∞—Ä—ã–Ω –∞–≥—Ä–µ–≥–∞—Ç** –¥—ç—ç—Ä —Ö–∏–π–≥–¥—ç–Ω—ç.")

# Feature pool
def nonleaky(col: str) -> bool:
    s = str(col)
    if s == "–û—Å–æ–ª": 
        return False
    if s.startswith("–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ –∂–∏–ª "):  # –±“Ø—Ö year dummies
        return False
    if "–¢”©—Ä”©–ª" in s:                       # —Ç”©—Ä–ª–∏–π–Ω one-hot, –Ω—ç—Ä—à–ª–∏–π–Ω —Ö—É–≤–∏–ª–±–∞—Ä—É—É–¥
        return False
    if s in {"Year", "Month", "Day"}:
        return False
    return True

feature_pool = [c for c in (binary_cols + num_additional) if nonleaky(c)]
if len(feature_pool) == 0:
    st.error("Leakage-–≥“Ø–π —à–∏–Ω–∂ –æ–ª–¥—Å–æ–Ω–≥“Ø–π. Metadata/one-hot “Ø“Ø—Å–≥—ç—Ö –¥“Ø—Ä–º—ç—ç —à–∞–ª–≥–∞–Ω–∞ —É—É.")
    st.stop()



# Target/Features (event-level ‚Üí monthly aggregate later)
y_all = pd.to_numeric(df["–û—Å–æ–ª"], errors="coerce").fillna(0).values
X_all = df[feature_pool].fillna(0.0).values

# Top features via RF + SHAP (guarded & sampled)
top_features = feature_pool[:min(14, len(feature_pool))]







# –°–∞—Ä –±“Ø—Ä–∏–π–Ω –∞–≥—Ä–µ–≥–∞—Ç
monthly_target = (
    df[df["–û—Å–æ–ª"] == 1]
    .groupby(["Year", "Month"])
    .agg(osol_count=("–û—Å–æ–ª", "sum"))
    .reset_index()
)
monthly_target["date"] = pd.to_datetime(monthly_target[["Year", "Month"]].assign(DAY=1))
monthly_features = (
    df.groupby(["Year","Month"])[feature_pool]  # –∞–Ω—Ö–Ω—ã pool (nonleaky)
      .sum()
      .reset_index()
      .sort_values(["Year","Month"])
)
# t —Å–∞—Ä–¥ –∑”©–≤—Ö”©–Ω (t-1) —Å–∞—Ä —Ö“Ø—Ä—Ç—ç–ª—Ö –º—ç–¥—ç—ç–ª—ç–ª –º—ç–¥—ç–≥–¥—ç–∂ –±–∞–π—Å–∞–Ω –±–∞–π—Ö —ë—Å—Ç–æ–π
for c in feature_pool:
    monthly_features[c] = monthly_features[c].shift(1)

grouped = (
    pd.merge(monthly_target, monthly_features, on=["Year","Month"], how="left")
      .sort_values(["Year","Month"]).reset_index(drop=True)
)

# Lag-—É—É–¥
# Lag-—É—É–¥
n_lag = st.sidebar.slider("–°–∞—Ä—ã–Ω –ª–∞–≥ —Ü–æ–Ω—Ö (n_lag)", min_value=6, max_value=18, value=12, step=1)
for i in range(1, n_lag + 1):
    grouped[f"osol_lag_{i}"] = grouped["osol_count"].shift(i)
grouped = grouped.dropna().reset_index(drop=True)

if grouped.empty or len(grouped) < 10:
    st.warning(f"–°—É—Ä–≥–∞–ª—Ç —Ö–∏–π—Ö—ç–¥ —Ö–∞–Ω–≥–∞–ª—Ç—Ç–∞–π —Å–∞—Ä —Ç—É—Ç–º—ã–Ω ”©–≥”©–≥–¥”©–ª –∞–ª–≥–∞ (lag={n_lag}). –û–Ω/—Å–∞—Ä–∞–∞ —à–∞–ª–≥–∞–Ω–∞ —É—É.")
    st.stop()

# --- Train/Test split —Ö—É–≤—å ---
split_ratio = st.sidebar.slider("Train ratio", 0.5, 0.9, 0.8, 0.05)

# --- –ù—ç—Ä—Å–∏–π–≥ —è–ª–≥–∞—Ö: –ª–∞–≥—É—É–¥ + —ç–∫–∑–æ–≥–µ–Ω ---
lag_cols  = [f"osol_lag_{i}" for i in range(1, n_lag + 1)]
exog_cols = feature_pool  # leakage-–≥“Ø–π–≥—ç—ç—Ä —Ü—ç–≤—ç—Ä–ª—ç—Å—ç–Ω pool

# --- Feature selection-–∏–π–≥ –ó”®–í–•”®–ù TRAIN –¥—ç—ç—Ä, –∑”©–≤ –∏–Ω–¥–µ–∫—Å—ç—ç—Ä ---
X_all_fs = grouped[lag_cols + exog_cols].fillna(0.0).values
y_all     = grouped["osol_count"].values.reshape(-1, 1)

train_size = int(len(X_all_fs) * split_ratio)
X_train_fs = X_all_fs[:train_size]
y_train_fs = y_all[:train_size].ravel()

try:
    rf_fs = RandomForestRegressor(n_estimators=300, random_state=42, n_jobs=-1)
    rf_fs.fit(X_train_fs, y_train_fs)

    imp_series = pd.Series(rf_fs.feature_importances_, index=lag_cols + exog_cols)
    k = min(14, len(exog_cols))
    exog_top = imp_series.loc[exog_cols].sort_values(ascending=False).head(k).index.tolist()

    st.caption("Train –¥—ç—ç—Ä —Ç–æ–¥–æ—Ä—Å–æ–Ω top exogenous features (leakage-–≥“Ø–π):")
    st.write(exog_top)
except Exception as e:
    st.warning(f"Feature selection train –¥—ç—ç—Ä –∞–∂–∏–ª–ª–∞—Ö–∞–¥ –∞–ª–¥–∞–∞: {e}")
    exog_top = exog_cols[:min(14, len(exog_cols))]

# --- –≠—Ü—Å–∏–π–Ω —à–∏–Ω–∂“Ø“Ø–¥: –ª–∞–≥ + —à–∏–ª–¥—ç–≥ —ç–∫–∑–æ–≥–µ–Ω ---
feature_cols = lag_cols + exog_top

# --- –≠—Ü—Å–∏–π–Ω X, y, split, scale (scaler-—É—É–¥—ã–≥ –¥–∞—Ö–∏–Ω fit) ---
X = grouped[feature_cols].fillna(0.0).values
y = grouped["osol_count"].values.reshape(-1, 1)

X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

scaler_X = MinMaxScaler()
X_train = scaler_X.fit_transform(X_train)
X_test  = scaler_X.transform(X_test)

scaler_y = MinMaxScaler()
y_train = scaler_y.fit_transform(y_train).ravel()
y_test  = scaler_y.transform(y_test).ravel()

# --- Leakage —à–∏–Ω–∂–∏–ª–≥—ç—ç (safety) ---
suspects = [c for c in exog_top if c.startswith("–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ –∂–∏–ª ")]
check = grouped[["date", "osol_count"] + feature_cols].copy()

identicals = [c for c in feature_cols
              if np.allclose(grouped[c].values, grouped["osol_count"].values, equal_nan=False)]
if identicals:
    st.error(f"IDENTICAL leakage “Ø–ª–¥–ª—ç—ç: {identicals}")
    st.stop()

corrs = check[feature_cols].corrwith(check["osol_count"]).sort_values(ascending=False)
st.write("Leakage —Å—ç–∂–∏–≥—Ç—ç–π (year dummies):", suspects)
st.write("Target-—Ç—ç–π –∫–æ—Ä—Ä–µ–ª—è—Ü–∏ (–¥—ç—ç–¥ 10):", corrs.head(10))
# Models
estimators = [
    ("rf", RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)),
    ("ridge", Ridge()),
    ("dt", DecisionTreeRegressor(random_state=42)),
]

MODEL_LIST = [
    ("LinearRegression", LinearRegression()),
    ("Ridge", Ridge()),
    ("Lasso", Lasso()),
    ("ElasticNet", ElasticNet()),
    ("DecisionTree", DecisionTreeRegressor(random_state=42)),
    ("RandomForest", RandomForestRegressor(random_state=42, n_jobs=-1)),
    ("ExtraTrees", ExtraTreesRegressor(random_state=42, n_jobs=-1)),
    ("GradientBoosting", GradientBoostingRegressor(random_state=42)),
    ("HistGB", HistGradientBoostingRegressor(random_state=42)),
    ("AdaBoost", AdaBoostRegressor(random_state=42)),
    ("KNeighbors", KNeighborsRegressor()),
    ("SVR", SVR()),
    ("MLPRegressor", MLPRegressor(hidden_layer_sizes=(64, 32), max_iter=800, random_state=42)),
    ("Stacking", StackingRegressor(estimators=estimators, final_estimator=LinearRegression(), cv=5)),
]
if XGBRegressor is not None:
    MODEL_LIST.append(("XGBRegressor", XGBRegressor(
        tree_method="hist", predictor="cpu_predictor", random_state=42, n_estimators=400)))
if CatBoostRegressor is not None:
    MODEL_LIST.append(("CatBoostRegressor", CatBoostRegressor(
        task_type="CPU", random_state=42, verbose=0)))
if LGBMRegressor is not None:
    MODEL_LIST.append(("LGBMRegressor", LGBMRegressor(
        device="cpu", random_state=42)))

# Voting/Stacking ensemble (–Ω—ç–º—ç–ª—Ç)
voting_estimators = []
if XGBRegressor is not None:
    voting_estimators.append(("xgb", XGBRegressor(tree_method="hist", predictor="cpu_predictor", random_state=42)))
if LGBMRegressor is not None:
    voting_estimators.append(("lgbm", LGBMRegressor(device="cpu", random_state=42)))
if CatBoostRegressor is not None:
    voting_estimators.append(("cat", CatBoostRegressor(task_type="CPU", random_state=42, verbose=0)))
voting_estimators += [
    ("rf", RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)),
    ("gb", GradientBoostingRegressor(random_state=42)),
]
if len(voting_estimators) > 1:
    MODEL_LIST.append(("VotingRegressor", VotingRegressor(estimators=voting_estimators)))

stacking_estimators = [("rf", RandomForestRegressor(random_state=42, n_jobs=-1))]
if XGBRegressor is not None:
    stacking_estimators.append(("xgb", XGBRegressor(tree_method="hist", predictor="cpu_predictor", random_state=42)))
if LGBMRegressor is not None:
    stacking_estimators.append(("lgbm", LGBMRegressor(device="cpu", random_state=42)))
if CatBoostRegressor is not None:
    stacking_estimators.append(("cat", CatBoostRegressor(task_type="CPU", verbose=0, random_state=42)))
MODEL_LIST.append(("StackingEnsemble", StackingRegressor(
    estimators=stacking_estimators, final_estimator=LinearRegression(), cv=5
)))

# Train all
progress_bar = st.progress(0, text="ML –º–æ–¥–µ–ª–∏–π–≥ —Å—É—Ä–≥–∞–∂ –±–∞–π–Ω–∞...")
results = []
y_preds = {}
for i, (name, model) in enumerate(MODEL_LIST):
    try:
        model.fit(X_train, y_train)
        y_pred = np.asarray(model.predict(X_test)).reshape(-1)
        y_preds[name] = y_pred
        mae = mean_absolute_error(y_test, y_pred)
        mse = mean_squared_error(y_test, y_pred)
        rmse = float(np.sqrt(mse))
        r2 = r2_score(y_test, y_pred)
        results.append({"Model": name, "MAE": mae, "RMSE": rmse, "R2": r2})
    except Exception as e:
        results.append({"Model": name, "MAE": np.nan, "RMSE": np.nan, "R2": np.nan, "Error": str(e)})
    progress = min(int((i + 1) / len(MODEL_LIST) * 100), 100)
    progress_bar.progress(progress, text=f"{name} –¥—É—É—Å–ª–∞–∞")
progress_bar.empty()
st.success("–ë“Ø—Ö ML –º–æ–¥–µ–ª —Å—É—Ä–≥–∞–≥–¥–ª–∞–∞!")


# --- Leakage —à–∏–Ω–∂–∏–ª–≥—ç—ç (—Å—ç–∂–∏–≥—Ç—ç–π –±–∞–≥–∞–Ω—É—É–¥—ã–≥ –∏–ª—Ä“Ø“Ø–ª—ç—Ö) ---
suspects = []
for c in top_features:
    if c.startswith("–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ –∂–∏–ª "):
        suspects.append(c)

# —Å–∞—Ä –±“Ø—Ä–∏–π–Ω —Ç“Ø–≤—à–∏–Ω–¥ —à–∞–ª–≥–∞—Ö
check = grouped[["date","osol_count"] + top_features].copy()

# 1) —è–≥ –∏–∂–∏–ª —ç—Å—ç—Ö
# identical —à–∞–ª–≥–∞–ª—Ç
identicals = []
for c in feature_pool:
    if np.allclose(grouped[c].values, grouped["osol_count"].values, equal_nan=False):
        identicals.append(c)
if identicals:
    st.error(f"IDENTICAL leakage “Ø–ª–¥–ª—ç—ç: {identicals}")
    st.stop()


# 2) –º–∞—à ”©–Ω–¥”©—Ä –∫–æ—Ä—Ä–µ–ª—è—Ü–∏
corrs = (
    check[top_features].corrwith(check["osol_count"])
    .sort_values(ascending=False)
)
st.write("Leakage —Å—ç–∂–∏–≥—Ç—ç–π (year dummies):", suspects)
st.write("–Ø–≥ —Ç—ç–Ω—Ü“Ø“Ø –≥–∞—Ä—á –±—É–π –±–∞–≥–∞–Ω—É—É–¥:", identicals)
st.write("Target-—Ç—ç–π –∫–æ—Ä—Ä–µ–ª—è—Ü–∏:", corrs.head(10))

results_df = pd.DataFrame(results).sort_values("RMSE", na_position="last")
st.dataframe(results_df, use_container_width=True)

# Excel —Ç–∞—Ç–∞—Ö (–º–µ—Ç—Ä–∏–∫)
with pd.ExcelWriter("model_metrics.xlsx", engine="xlsxwriter") as writer:
    results_df.to_excel(writer, index=False)
with open("model_metrics.xlsx", "rb") as f:
    st.download_button(
        "–ú–æ–¥–µ–ª–∏–π–Ω –º–µ—Ç—Ä–∏–∫ Excel —Ç–∞—Ç–∞—Ö",
        data=f,
        file_name="model_metrics.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
    )

# ---- –ü—Ä–æ–≥–Ω–æ–∑ helper (–ª–∞–≥-—Ü–æ–Ω—Ö—ã–≥ –ª —à–∏–Ω—ç—á–∏–ª–Ω—ç, —ç–∫–∑–æ–≥–µ–Ω —Ç–æ–≥—Ç–≤–æ—Ä—Ç–æ–π) ----
lag_count = len(lag_cols)

def forecast_next_monthly(model, last_raw_row, steps=12):
    """
    last_raw_row: grouped[feature_cols]-–∏–π–Ω –°“Æ“Æ–õ–ò–ô–ù –º”©—Ä (–∞–Ω—Å–∫–µ–π–ª–¥)
    –ë—É—Ü–∞–∞–ª—Ç: –æ—Å–ª—ã–Ω —Ç–æ–æ–Ω—ã –ø—Ä–æ–≥–Ω–æ–∑—É—É–¥ (–∞–Ω—Å–∫–µ–π–ª–¥, –±–æ–¥–∏—Ç –Ω—ç–≥–∂—ç—ç—Ä)
    """
    preds = []
    lag_vals  = last_raw_row[:lag_count].astype(float).copy()   # –∞–Ω—Å–∫–µ–π–ª–¥ –ª–∞–≥—É—É–¥
    exog_vals = last_raw_row[lag_count:].astype(float).copy()   # —Ç–æ–≥—Ç–º–æ–ª/—Å—Ü–µ–Ω–∞—Ä–∏ —ç–∫–∑–æ–≥–µ–Ω

    for _ in range(steps):
        seq_raw    = np.concatenate([lag_vals, exog_vals]).reshape(1, -1)
        seq_scaled = scaler_X.transform(seq_raw)
        p_scaled   = float(np.asarray(model.predict(seq_scaled)).ravel()[0])
        p          = float(scaler_y.inverse_transform(np.array([[p_scaled]])).ravel()[0])
        preds.append(p)

        # –ª–∞–≥ —Ü–æ–Ω—Ö—ã–≥ –±–∞—Ä—É—É–Ω —Ç–∏–π—à —à–∏–ª–∂“Ø“Ø–ª–∂, lag1-–¥ —à–∏–Ω—ç p-–≥ (–∞–Ω—Å–∫–µ–π–ª–¥) –±–∞–π—Ä–ª—É—É–ª–Ω–∞
        lag_vals = np.roll(lag_vals, 1)
        lag_vals[0] = p
    return np.array(preds)

# Forecasts by model
model_forecasts = {}
last_raw = grouped[feature_cols].iloc[-1].values  # –∞–Ω—Å–∫–µ–π–ª–¥, feature_cols-–∏–π–Ω –¥–∞—Ä–∞–∞–ª–ª–∞–∞—Ä

# –°–æ–Ω–≥–æ–ª—Ç—É—É–¥ ‚Äî —Å–∞—Ä—ã–Ω –∞–≥—Ä–µ–≥–∞—Ç—Ç–∞–π –Ω–∏–π—Ü“Ø“Ø–ª—ç—Ö (”©–¥—Ä–∏–π–Ω –Ω—ç—Ä—Ç—ç–π –±–æ–ª–æ–≤—á mapping –Ω—å —Å–∞—Ä)
h_map = {"7 —Ö–æ–Ω–æ–≥": 1, "14 —Ö–æ–Ω–æ–≥": 1, "30 —Ö–æ–Ω–æ–≥": 1, "90 —Ö–æ–Ω–æ–≥": 3, "180 —Ö–æ–Ω–æ–≥": 6, "365 —Ö–æ–Ω–æ–≥": 12}
for name, model in MODEL_LIST:
    # –¥–∞—Ä–∞–∞—Ö 'y_preds' –Ω—å test –¥—ç—ç—Ä—Ö scaled –ø—Ä–æ–≥–Ω–æ–∑ —Ç—É–ª —ç–Ω—ç dict-—Ç –±–∞–π—Ö–≥“Ø–π –±–∞–π–∂ –±–æ–ª–Ω–æ
    if name not in y_preds:
        continue
    preds_dict = {}
    for k, months in h_map.items():
        preds_dict[k] = forecast_next_monthly(model, last_raw, steps=months)  # –∞–Ω—Å–∫–µ–π–ª–¥ –±—É—Ü–∞–∞–Ω–∞
    model_forecasts[name] = preds_dict

# Test –¥—ç—ç—Ä—Ö –±–æ–¥–∏—Ç/—Ç–∞–∞–º–∞–≥ (—É–Ω—à–∏–≥–¥–∞—Ö—É–π—Ü –Ω—ç–≥–∂—ç—ç—Ä)
test_dates = grouped["date"].iloc[-len(X_test):].values
test_true  = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()
test_preds_df = pd.DataFrame({"date": test_dates, "real": test_true})
for name in model_forecasts.keys():
    ypi = scaler_y.inverse_transform(np.array(y_preds[name]).reshape(-1, 1)).flatten()
    test_preds_df[name] = ypi

# –ò—Ä—ç—ç–¥“Ø–π–Ω 12 —Å–∞—Ä—ã–Ω —Ç–∞–∞–º–∞–≥ (–∞–Ω—Å–∫–µ–π–ª–¥, –±–æ–¥–∏—Ç –Ω—ç–≥–∂—ç—ç—Ä)
future_dates = pd.date_range(start=grouped["date"].iloc[-1] + pd.offsets.MonthBegin(), periods=12, freq="MS")
future_preds_df = pd.DataFrame({"date": future_dates})
for name, model in MODEL_LIST:
    if name not in y_preds:
        continue
    future_preds_df[name] = forecast_next_monthly(model, last_raw, steps=12)


with pd.ExcelWriter("model_predictions.xlsx", engine="xlsxwriter") as writer:
    test_preds_df.to_excel(writer, index=False, sheet_name="Test_Predictions")
    future_preds_df.to_excel(writer, index=False, sheet_name="Future_Predictions")
with open("model_predictions.xlsx", "rb") as f:
    st.download_button(
        "Test/Forecast –±“Ø—Ö –º–æ–¥–µ–ª–∏–π–Ω —Ç–∞–∞–º–∞–≥–ª–∞–ª—É—É–¥—ã–≥ Excel-—Ä —Ç–∞—Ç–∞—Ö",
        data=f,
        file_name="model_predictions.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
    )

st.subheader("Test –¥–∞—Ç–∞–Ω –¥—ç—ç—Ä—Ö –±–æ–¥–∏—Ç/—Ç–∞–∞–º–∞–≥ (—Ç–æ–ª–≥–æ–π 10 –º”©—Ä)")
st.dataframe(test_preds_df.head(10), use_container_width=True)

st.subheader("–ò—Ä—ç—ç–¥“Ø–π–Ω 12 —Å–∞—Ä—ã–Ω –ø—Ä–æ–≥–Ω–æ–∑ (–º–æ–¥–µ–ª –±“Ø—Ä—ç—ç—Ä)")
st.dataframe(future_preds_df, use_container_width=True)

# –ì—Ä–∞—Ñ–∏–∫ —Ö–∞—Ä–∞—Ö UI
model_options = list(y_preds.keys())
selected_model = st.selectbox("–ú–æ–¥–µ–ª—å —Å–æ–Ω–≥–æ—Ö:", model_options)
selected_h = st.selectbox("–•–æ—Ä–∏–∑–æ–Ω—Ç:", list(h_map.keys()), index=2)

months = h_map[selected_h]
dates_future = pd.date_range(start=grouped["date"].iloc[-1] + pd.offsets.MonthBegin(), periods=months, freq="MS")
future_df = pd.DataFrame({"date": dates_future, "forecast": model_forecasts[selected_model][selected_h]})
fig = px.line(future_df, x="date", y="forecast", markers=True,
              title=f"{selected_model} ‚Äî {selected_h} (—Å–∞—Ä —Ä—É—É –∑—É—Ä–∞–≥–¥—Å–∞–Ω) –ø—Ä–æ–≥–Ω–æ–∑")
st.plotly_chart(fig, use_container_width=True)

# -------------------------- 1. –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–π–Ω —à–∏–Ω–∂–∏–ª–≥—ç—ç --------------------------
st.header("1. –û—Å–æ–ª–¥ –Ω”©–ª”©”©–ª”©—Ö —Ö“Ø—á–∏–Ω –∑“Ø–π–ª—Å–∏–π–Ω —Ç–∞—Ä—Ö–∞–ª—Ç/–∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π–Ω —à–∏–Ω–∂–∏–ª–≥—ç—ç")
st.write("–î–æ–æ—Ä—Ö multiselect-–æ–æ—Å –∏—Ö–¥—ç—ç 15 —Ö—É–≤—å—Å–∞–≥—á —Å–æ–Ω–≥–æ–∂ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π–Ω –º–∞—Ç—Ä–∏—Ü—ã–≥ “Ø–∑–Ω—ç “Ø“Ø.")

vars_for_corr = ["Year"]
vars_for_corr += [c for c in df.columns if c.startswith("–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ –∂–∏–ª ")][:10]
vars_for_corr += [c for c in (binary_cols + num_additional) if c in df.columns]
vars_for_corr = list(dict.fromkeys(vars_for_corr))  # remove dups

if len(vars_for_corr) > 1:
    Xx = df[vars_for_corr].fillna(0.0).values
    yy = pd.to_numeric(df["–û—Å–æ–ª"], errors="coerce").fillna(0).values
    try:
        rf_cor = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)
        rf_cor.fit(Xx, yy)
        importances_cor = rf_cor.feature_importances_
        indices_cor = np.argsort(importances_cor)[::-1]
        top_k_cor = min(15, len(vars_for_corr))
        default_cols = [vars_for_corr[i] for i in indices_cor[:top_k_cor]]
    except Exception:
        default_cols = vars_for_corr[: min(15, len(vars_for_corr))]
else:
    default_cols = vars_for_corr

selected_cols = st.multiselect(
    "–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–π–Ω –º–∞—Ç—Ä–∏—Ü–∞–¥ –æ—Ä—É—É–ª–∞—Ö —Ö—É–≤—å—Å–∞–≥—á–∏–¥:", vars_for_corr, default=default_cols, max_selections=15
)
if selected_cols:
    st.pyplot(plot_correlation_matrix(df, "Correlation Matrix", selected_cols))
else:
    st.info("–•—É–≤—å—Å–∞–≥—á —Å–æ–Ω–≥–æ–Ω–æ —É—É.")

# -------------------------- 2. –û—Å–ª—ã–Ω ”©—Å”©–ª—Ç–∏–π–Ω —Ç—Ä–µ–Ω–¥ --------------------------
st.header("2. –û—Å–ª—ã–Ω ”©—Å”©–ª—Ç–∏–π–Ω —Ç—Ä–µ–Ω–¥")
st.subheader("–ñ–∏–ª, —Å–∞—Ä –±“Ø—Ä–∏–π–Ω –æ—Å–ª—ã–Ω —Ç–æ–æ")
trend_data = (
    df[df["–û—Å–æ–ª"] == 1]
    .groupby(["Year", "Month"])
    .agg(osol_count=("–û—Å–æ–ª", "sum"))
    .reset_index()
)
trend_data["YearMonth"] = trend_data.apply(lambda x: f"{int(x['Year'])}-{int(x['Month']):02d}", axis=1)
available_years = sorted(trend_data["Year"].unique())
year_options = ["–ë“Ø–≥–¥"] + [str(y) for y in available_years]
selected_year = st.selectbox("–ñ–∏–ª —Å–æ–Ω–≥–æ—Ö:", year_options)
plot_df = trend_data if selected_year == "–ë“Ø–≥–¥" else trend_data[trend_data["Year"] == int(selected_year)].copy()
fig = px.line(plot_df, x="YearMonth", y="osol_count", markers=True,
              labels={"YearMonth": "–û–Ω-–°–∞—Ä", "osol_count": "–û—Å–ª—ã–Ω —Ç–æ–æ"}, title="")
fig.update_layout(xaxis_tickangle=45, hovermode="x unified", plot_bgcolor="white",
                  yaxis=dict(title="–û—Å–ª—ã–Ω —Ç–æ–æ", rangemode="tozero"), xaxis=dict(title="–û–Ω-–°–∞—Ä"))
fig.update_traces(line=dict(width=3))
st.plotly_chart(fig, use_container_width=True)

# -------------------------- 4. –ö–∞—Ç–µ–≥–æ—Ä–∏ —Ö–∞–º–∞–∞—Ä–∞–ª (Cram√©r‚Äôs V + Chi-square) --------------------------
st.header("4. –ö–∞—Ç–µ–≥–æ—Ä–∏ —Ö—É–≤—å—Å–∞–≥—á–¥—ã–Ω —Ö–æ–æ—Ä–æ–Ω–¥—ã–Ω —Ö–∞–º–∞–∞—Ä–∞–ª (Cram√©r‚Äôs V –±–æ–ª–æ–Ω Chi-square)")
low_card_cols = []
for c in df.columns:
    if c in ["–û—Å–æ–ª", "–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ", "Year", "Month", "Day"]:
        continue
    u = df[c].dropna().unique()
    if 2 <= len(u) <= 15:
        low_card_cols.append(c)

categorical_cols = sorted(list(set(binary_cols + low_card_cols)))
if len(categorical_cols) < 2:
    st.info("–ö–∞—Ç–µ–≥–æ—Ä–∏ –±–∞–≥–∞–Ω–∞ (2‚Äì15 —Ç“Ø–≤—à–∏–Ω) –æ–ª–¥—Å–æ–Ω–≥“Ø–π.")
else:
    var1 = st.selectbox("1-—Ä –∫–∞—Ç–µ–≥–æ—Ä–∏ —Ö—É–≤—å—Å–∞–≥—á:", categorical_cols)
    var2 = st.selectbox("2-—Ä –∫–∞—Ç–µ–≥–æ—Ä–∏ —Ö—É–≤—å—Å–∞–≥—á:", [c for c in categorical_cols if c != var1])

    table = pd.crosstab(df[var1], df[var2])
    chi2, p, dof, expected = chi2_contingency(table)
    n = table.values.sum()
    r, k = table.shape
    cramers_v = np.sqrt(chi2 / (n * (min(k, r) - 1))) if min(k, r) > 1 else np.nan

    st.subheader("1. Chi-square —Ç–µ—Å—Ç")
    st.write(f"**Chi-square statistic:** {chi2:.3f}")
    st.write(f"**p-value:** {p:.4f}")
    st.info("–¢–∞–π–ª–±–∞—Ä: p-value < 0.05 –±–æ–ª —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏–π–Ω —Ö—É–≤—å–¥ —Ö–∞–º–∞–∞—Ä–∞–ª—Ç–∞–π –≥—ç–∂ “Ø–∑–Ω—ç.")

    st.subheader("2. Cram√©r‚Äôs V")
    st.write(f"**Cram√©r‚Äôs V:** {cramers_v:.3f} (0=—Ö–∞–º–∞–∞—Ä–∞–ª–≥“Ø–π, 1=—Ö“Ø—á—Ç—ç–π —Ö–∞–º–∞–∞—Ä–∞–ª)")

    st.write("**Crosstab:**")
    st.dataframe(table, use_container_width=True)

# -------------------------- –£–ª–∏—Ä–ª—ã–Ω —è–ª–≥–∞–∞ --------------------------
def get_season(month: int) -> str:
    if month in [12, 1, 2]:
        return "”®–≤”©–ª"
    elif month in [3, 4, 5]:
        return "–•–∞–≤–∞—Ä"
    elif month in [6, 7, 8]:
        return "–ó—É–Ω"
    elif month in [9, 10, 11]:
        return "–ù–∞–º–∞—Ä"
    return "–¢–æ–¥–æ—Ä—Ö–æ–π–≥“Ø–π"

df["Season"] = df["–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ"].dt.month.apply(get_season)
table = pd.crosstab(df["Season"], df[torol_col])
chi2, p, dof, exp = chi2_contingency(table)
n = table.values.sum()
r, k = table.shape
cramers_v = np.sqrt(chi2 / (n*(min(k,r)-1)))
st.subheader("–£–ª–∏—Ä–ª—ã–Ω —è–ª–≥–∞–∞ (œá¬≤ –±–∞ Cram√©r‚Äôs V)")
st.write("**Chi-square statistic:**", round(chi2, 3))
st.write("**p-value:**", round(p, 4))
st.write("**Cram√©r‚Äôs V:**", round(cramers_v, 3))
st.dataframe(table, use_container_width=True)

# -------------------------- 6. Empirical Bayes (—Å–∞—Ä –±“Ø—Ä) --------------------------
st.header("6. Empirical Bayes before‚Äìafter —à–∏–Ω–∂–∏–ª–≥—ç—ç (—Å–∞—Ä –±“Ø—Ä)")

def empirical_bayes(obs, exp, prior_mean, prior_var):
    """EB –∂–∏–≥–Ω—ç–ª—Ç"""
    weight = prior_var / (prior_var + exp) if (prior_var + exp) > 0 else 0.0
    return weight * obs + (1 - weight) * prior_mean

monthly = (
    df[df["–û—Å–æ–ª"] == 1]
    .groupby(["Year", "Month"])
    .agg(osol_count=("–û—Å–æ–ª", "sum"))
    .reset_index()
)
monthly["date"] = pd.to_datetime(monthly[["Year", "Month"]].assign(DAY=1))
monthly["period"] = np.where(monthly["Year"] <= 2023, "before", "after")

expected = monthly[monthly["period"]=="before"]["osol_count"].mean()
prior_mean = float(expected) if pd.notna(expected) else 0.0
prior_var  = prior_mean / 2 if prior_mean > 0 else 1.0

monthly["EB"] = monthly.apply(
    lambda row: empirical_bayes(row["osol_count"], prior_mean, prior_mean, prior_var)
    if row["period"]=="after" else row["osol_count"],
    axis=1
)
fig = px.line(monthly, x="date", y=["osol_count","EB"], color="period", markers=True,
              labels={"value":"–û—Å–æ–ª (—Ç–æ–æ)", "date":"–û–Ω-–°–∞—Ä"},
              title="–û—Å–ª—ã–Ω —Å–∞—Ä –±“Ø—Ä–∏–π–Ω —Ç–æ–æ (EB –∂–∏–≥–Ω—ç–ª—Ç)")
st.plotly_chart(fig, use_container_width=True)

