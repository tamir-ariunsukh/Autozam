# -*- coding: utf-8 -*-
# ============================================================
# –ó–∞–º —Ç—ç—ç–≤—Ä–∏–π–Ω –æ—Å–æ–ª ‚Äî Auto ML & Hotspot Dashboard (Streamlit)
# –•—É–≤–∏–ª–±–∞—Ä: 2025-08-17b ‚Äî Leakage-free scaling, 200-row quick sample, bias-corrected Cram√©r's V
# –¢–∞–π–ª–±–∞—Ä:
#  - –•–∞–≤—Å–∞—Ä–≥–∞—Å–∞–Ω Excel ("–∫–æ–¥–ª–æ–≥–¥—Å–æ–Ω - Copy.xlsx")-—Ç–∞–π —à—É—É–¥ –∑–æ—Ö–∏—Ü–Ω–æ.
#  - Binary (0/1) –±“Ø—Ö –±–∞–≥–∞–Ω—ã–≥ –∞–≤—Ç–æ–º–∞—Ç–∞–∞—Ä –∏–ª—Ä“Ø“Ø–ª–∂, –º–æ–¥–µ–ª/–∫–æ—Ä—Ä–µ–ª—è—Ü/—Ö–æ—Ç—Å–ø–æ—Ç–æ–¥ –∞—à–∏–≥–ª–∞–Ω–∞.
#  - –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç –±–∞–≥–∞–Ω—É—É–¥ (”®—Ä–≥”©—Ä”©–≥/–£—Ä—Ç—Ä–∞–≥ —ç—Å–≤—ç–ª lat/lon) –±–∞–π–≤–∞–ª –≥–∞–∑—Ä—ã–Ω –∑—É—Ä–∞–≥ –∑—É—Ä–Ω–∞.
#  - –û–ª–æ–Ω ML –º–æ–¥–µ–ª —Å—É—Ä–≥–∞–ª—Ç, –º–µ—Ç—Ä–∏–∫/—Ç–∞–∞–º–∞–≥–ª–∞–ª—ã–≥ Excel –±–æ–ª–≥–æ–Ω —Ç–∞—Ç–∞—Ö –±–æ–ª–æ–º–∂—Ç–æ–π.
#  - "–û—Å–æ–ª" –±–∞–≥–∞–Ω–∞ –±–∞–π—Ö–≥“Ø–π —Ç–æ—Ö–∏–æ–ª–¥–æ–ª–¥ "–¢”©—Ä”©–ª"-”©”©—Å (–ì—ç–º—Ç —Ö—ç—Ä—ç–≥/–ó”©—Ä—á–ª–∏–π–Ω —Ö—ç—Ä—ç–≥) –∑–æ—Ä–∏–ª—Ç—ã–≥ “Ø“Ø—Å–≥—ç–Ω—ç.
#  - –®–∏–Ω—ç—á–ª—ç–ª—Ç“Ø“Ø–¥: (i) scaler-–∏–π–≥ train-–¥ –ª fit —Ö–∏–π–∂ data leakage –∞—Ä–∏–ª–≥–∞–≤, (ii) 200 –º”©—Ä–∏–π–Ω —Ö—É—Ä–¥–∞–Ω –∞–∂–∏–ª–ª—É—É–ª–∞—Ö —Å–æ–Ω–≥–æ–ª—Ç, (iii) Cram√©r's V-–∏–π–Ω bias-corrected —Ö—É–≤–∏–ª–±–∞—Ä –Ω—ç–º—ç–≤.
# –ì“Ø–π—Ü—ç—Ç–≥—ç—Ö: streamlit run osol_auto_streamlit.py
# ============================================================

from __future__ import annotations
import io
import numpy as np
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

from pathlib import Path

from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import (
    RandomForestRegressor,
    GradientBoostingRegressor,
    AdaBoostRegressor,
    ExtraTreesRegressor,
    StackingRegressor,
    HistGradientBoostingRegressor,
)
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.cluster import DBSCAN

# –ì—É—Ä–∞–≤–¥–∞–≥—á gradient boosting —Å–∞–Ω–≥—É—É–¥ (optional)
try:
    from xgboost import XGBRegressor  # type: ignore
except Exception:
    XGBRegressor = None  # pragma: no cover
try:
    from lightgbm import LGBMRegressor  # type: ignore
except Exception:
    LGBMRegressor = None  # pragma: no cover
try:
    from catboost import CatBoostRegressor  # type: ignore
except Exception:
    CatBoostRegressor = None  # pragma: no cover

from scipy.stats import chi2_contingency
import folium
from streamlit_folium import st_folium

# -------------------------- UI setup --------------------------
st.set_page_config(page_title="–û—Å–æ–ª ‚Äî Auto ML & Hotspot (auto-binary)", layout="wide")
st.title("–ó–∞–º —Ç—ç—ç–≤—Ä–∏–π–Ω –æ—Å–ª—ã–Ω –∞–Ω–∞–ª–∏–∑ ‚Äî Auto (Binary autodetect)")

# -------------------------- –¢—É—Å–ª–∞—Ö —Ñ—É–Ω–∫—Ü—É—É–¥ --------------------------

def _canon(s: str) -> str:
    return "".join(str(s).lower().split()) if isinstance(s, str) else str(s)

def resolve_col(df: pd.DataFrame, candidates) -> str | None:
    """–ù—ç—Ä—à–ª–∏–π–≥ robust –±–∞–π–¥–ª–∞–∞—Ä –æ–ª–Ω–æ (whitespace/—Ç–æ–º-–∂–∏–∂–∏–≥ “Ø–ª —Ç–æ–æ–Ω–æ)."""
    lut = {_canon(c): c for c in df.columns}
    for name in candidates:
        key = _canon(name)
        if key in lut:
            return lut[key]
    return None

def is_binary_series(s: pd.Series) -> bool:
    vals = pd.to_numeric(s.dropna(), errors="coerce").dropna().unique()
    if len(vals) == 0:
        return False
    return set(np.unique(vals)).issubset({0, 1})

def clean_road_width(width):
    """'–ê–≤—Ç–æ –∑–∞–º - –ó–æ—Ä—á–∏—Ö —Ö—ç—Å–≥–∏–π–Ω ”©—Ä–≥”©–Ω' —Ç–∞–ª–±–∞—Ä—ã–Ω —Ç–µ–∫—Å—Ç—ç–Ω —É—Ç–≥—É—É–¥—ã–≥ —Ç–æ–æ–Ω –±–æ–ª–≥–æ—Ö."""
    if pd.isna(width):
        return np.nan
    if isinstance(width, (int, float)):
        return float(width)
    if isinstance(width, str):
        w = (
            width.replace("–º", "")
            .replace("-—ç—ç—Å –¥—ç—ç—à", "")
            .replace("—Ö“Ø—Ä—Ç—ç–ª", "")
            .replace(",", ".")
            .strip()
        )
        if "-" in w:
            try:
                low, high = map(float, w.split("-"))
                return (low + high) / 2
            except Exception:
                return np.nan
        try:
            return float(w)
        except Exception:
            return np.nan
    return np.nan

def plot_correlation_matrix(df, title, columns):
    n_unique = df[columns].nunique()
    if all((n_unique == 2) | (n_unique == 1)):
        st.warning("–°–æ–Ω–≥–æ—Å–æ–Ω –±–∞–≥–∞–Ω—É—É–¥ –Ω—å one-hot (0/1) —Ö—ç–ª–±—ç—Ä—Ç—ç–π –±–∞–π–Ω–∞. –ò–π–º —Ç–æ—Ö–∏–æ–ª–¥–æ–ª–¥ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏ -1~1 —Ç—É–π–ª—Ä—É—É–≥–∞–∞ —Ö—ç–ª–±–∏–π–∂ —Ö–∞—Ä–∞–≥–¥–∞–∂ –±–æ–ª–Ω–æ.")
    df_encoded = df[columns].copy()
    for col in df_encoded.columns:
        if df_encoded[col].dtype == "object":
            df_encoded[col] = pd.Categorical(df_encoded[col]).codes
    corr_matrix = df_encoded.corr(numeric_only=True)
    corr_matrix = corr_matrix.iloc[::-1]
    fig, ax = plt.subplots(figsize=(max(8, 1.5*len(columns)), max(6, 1.2*len(columns))))
    sns.heatmap(corr_matrix, annot=True, cmap="coolwarm", vmin=-1, vmax=1, center=0, ax=ax, fmt=".3f")
    plt.title(title)
    plt.tight_layout()
    return fig

# Bias-corrected Cram√©r's V (Bergsma, 2013)

def cramers_v_bias_corrected(table: pd.DataFrame) -> float:
    chi2, _, _, _ = chi2_contingency(table)
    n = table.values.sum()
    phi2 = chi2 / max(n, 1)
    r, k = table.shape
    phi2_corr = max(0, phi2 - (k-1)*(r-1)/(max(n-1, 1)))
    r_corr = r - (r-1)**2 / max(n-1, 1)
    k_corr = k - (k-1)**2 / max(n-1, 1)
    denom = max(min(k_corr-1, r_corr-1), 1e-12)
    return float(np.sqrt(phi2_corr / denom))

# -------------------------- ”®–≥”©–≥–¥”©–ª –∞—á–∞–∞–ª–∞–ª—Ç --------------------------

@st.cache_data(show_spinner=True)
def load_data(default_path: str = "–∫–æ–¥–ª–æ–≥–¥—Å–æ–Ω - Copy.xlsx"):
    """
    - Sidebar –¥—ç—ç—Ä—ç—ç—Å .xlsx —Ñ–∞–π–ª–∞–∞—Ä upload —Ö–∏–π–∂ –±–æ–ª–Ω–æ.
    - –•—ç—Ä—ç–≤ –æ—Ä—É—É–ª–∞–∞–≥“Ø–π –±–æ–ª default_path-—ã–≥ —É–Ω—à–∏–Ω–∞.
    - –û–≥–Ω–æ–æ–Ω—ã –±–∞–≥–∞–Ω—ã–≥ robust –±–∞–π–¥–ª–∞–∞—Ä –æ–ª–Ω–æ, '–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ' “Ø“Ø—Å–≥—ç–Ω—ç.
    - –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç –±–∞ binary –±–∞–≥–∞–Ω—É—É–¥—ã–≥ –∞–≤—Ç–æ–º–∞—Ç–∞–∞—Ä –∏–ª—Ä“Ø“Ø–ª–Ω—ç.
    """
    up = st.sidebar.file_uploader("Excel —Ñ–∞–π–ª –æ—Ä—É—É–ª–∞—Ö (.xlsx)", type=["xlsx"])
    if up is not None:
        df = pd.read_excel(up)
    else:
        local = Path("/mnt/data/–∫–æ–¥–ª–æ–≥–¥—Å–æ–Ω - Copy.xlsx")
        if local.exists():
            df = pd.read_excel(local)
        else:
            df = pd.read_excel(default_path)

    # –ù—ç—Ä—à–∏–ª —Ü—ç–≤—ç—Ä–ª—ç–≥—ç—ç
    df.columns = [c.strip() if isinstance(c, str) else c for c in df.columns]

    # –û–≥–Ω–æ–æ –±–∞–≥–∞–Ω—ã–≥ robust –±–∞–π–¥–ª–∞–∞—Ä –æ–ª–æ—Ö
    recv_col = resolve_col(df, ["–•“Ø–ª—ç—ç–Ω –∞–≤—Å–∞–Ω", "–•“Ø–ª—ç—ç–Ω –∞–≤—Å–∞–Ω ", "–û–≥–Ω–æ–æ", "–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ", "–û—Å–æ–ª –æ–≥–Ω–æ–æ", "–û—Å–ª—ã–Ω –æ–≥–Ω–æ–æ", "Date"]) 
    if recv_col is None:
        st.error("–û–≥–Ω–æ–æ–Ω—ã –±–∞–≥–∞–Ω–∞ –æ–ª–¥—Å–æ–Ω–≥“Ø–π. –ñ–∏—à—ç—ç –Ω—å: '–•“Ø–ª—ç—ç–Ω –∞–≤—Å–∞–Ω'.")
        st.stop()

    # '–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ' “Ø“Ø—Å–≥—ç—Ö
    df["–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ"] = pd.to_datetime(df[recv_col], errors="coerce")
    df["Year"]  = df["–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ"].dt.year
    df["Month"] = df["–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ"].dt.month
    df["Day"]   = df["–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ"].dt.day_name()
    df["Date"] = df["–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ"].dt.normalize()

    # –û–Ω –∂–∏–ª“Ø“Ø–¥–∏–π–Ω one-hot
    years = sorted(df["Year"].dropna().unique().tolist())
    for y in years:
        df[f"–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ –∂–∏–ª {int(y)}"] = (df["Year"] == int(y)).astype(int)
    if len(years) > 0:
        df["–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ –∂–∏–ª (min-max)"] = df["Year"].between(min(years), max(years)).astype(int)

    # –ó–∞–º—ã–Ω ”©—Ä–≥”©–Ω —Ü—ç–≤—ç—Ä–ª—ç—Ö (–±–∞–π–≤–∞–ª)
    if "–ê–≤—Ç–æ –∑–∞–º - –ó–æ—Ä—á–∏—Ö —Ö—ç—Å–≥–∏–π–Ω ”©—Ä–≥”©–Ω" in df.columns:
        df["–ê–≤—Ç–æ –∑–∞–º - –ó–æ—Ä—á–∏—Ö —Ö—ç—Å–≥–∏–π–Ω ”©—Ä–≥”©–Ω"] = df["–ê–≤—Ç–æ –∑–∞–º - –ó–æ—Ä—á–∏—Ö —Ö—ç—Å–≥–∏–π–Ω ”©—Ä–≥”©–Ω"].apply(clean_road_width)

    # –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç –Ω—ç—Ä—à–∏–ª
    lat_col = resolve_col(df, ["”®—Ä–≥”©—Ä”©–≥", "lat", "latitude"])
    lon_col = resolve_col(df, ["–£—Ä—Ç—Ä–∞–≥", "lon", "longitude"])

    # –ê–≤—Ç–æ–º–∞—Ç–∞–∞—Ä binary –±–∞–≥–∞–Ω—É—É–¥—ã–≥ –∏–ª—Ä“Ø“Ø–ª—ç—Ö
    exclude = {"–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ", "Year", "Month", "Day", "–¥/–¥", "–•–æ—Ä–æ–æ-–°—É–º", "–ê–π–º–∞–≥-–î“Ø“Ø—Ä—ç–≥"}
    if lat_col: exclude.add(lat_col)
    if lon_col: exclude.add(lon_col)
    binary_cols = [c for c in df.columns if c not in exclude and is_binary_series(df[c])]

    # –ù—ç–º—ç–ª—Ç —Ç–æ–æ–Ω candidate-—É—É–¥
    numeric_candidates = []
    if "–ê–≤—Ç–æ –∑–∞–º - –ó–æ—Ä—á–∏—Ö —Ö—ç—Å–≥–∏–π–Ω ”©—Ä–≥”©–Ω" in df.columns:
        numeric_candidates.append("–ê–≤—Ç–æ –∑–∞–º - –ó–æ—Ä—á–∏—Ö —Ö—ç—Å–≥–∏–π–Ω ”©—Ä–≥”©–Ω")

    # –î“Ø“Ø—Ä—ç–≥/–ê–π–º–∞–≥ fallback
    if "–î“Ø“Ø—Ä—ç–≥" not in df.columns:
        df["–î“Ø“Ø—Ä—ç–≥"] = 0
    if "–ê–π–º–∞–≥" not in df.columns:
        df["–ê–π–º–∞–≥"] = 0

    meta = {"lat_col": lat_col, "lon_col": lon_col, "binary_cols": binary_cols, "numeric_candidates": numeric_candidates, "years": years}
    return df, meta

# -------------------------- –ê—á–∞–∞–ª–∂ —ç—Ö–ª—ç—Ö --------------------------

df, meta = load_data()
lat_col, lon_col = meta["lat_col"], meta["lon_col"]
binary_cols = meta["binary_cols"]
num_additional = meta["numeric_candidates"]
years = meta["years"]

# -------------------------- Sidebar: Sampling & Seed --------------------------

st.sidebar.markdown("### ‚öôÔ∏è –ê—à–∏–≥–ª–∞–ª—Ç—ã–Ω —Ç–æ—Ö–∏—Ä–≥–æ–æ")
seed = int(st.sidebar.number_input("Random seed", value=42, step=1))
quick_sample = st.sidebar.checkbox("‚ö° 200 –º”©—Ä”©”©—Ä —Ö—É—Ä–¥–∞–Ω –∞–∂–∏–ª–ª—É—É–ª–∞—Ö (—Å–∞–Ω–∞–º—Å–∞—Ä–≥“Ø–π)", value=False)
if quick_sample and len(df) > 200:
    df = df.sample(n=200, random_state=seed).sort_values("–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ")

# -------------------------- Target —Ç–æ—Ö–∏—Ä–≥–æ–æ --------------------------

st.sidebar.markdown("### üéØ –ó–æ—Ä–∏–ª—Ç–æ—Ç —Ç–æ–¥–æ—Ä—Ö–æ–π–ª–æ–ª—Ç (–û—Å–æ–ª)")
target_mode = st.sidebar.radio(
    "–û—Å–æ–ª –≥—ç–∂ —Ç–æ–æ—Ü–æ—Ö –∞–Ω–≥–∏–ª–ª—ã–≥ —Å–æ–Ω–≥–æ–Ω–æ —É—É:",
    ("–•–æ—ë—É–ª–∞–Ω–≥ 1 –≥—ç–∂ —Ç–æ–æ—Ü", "–ó”©–≤—Ö”©–Ω –ì—ç–º—Ç —Ö—ç—Ä—ç–≥", "–ó”©–≤—Ö”©–Ω –ó”©—Ä—á–ª–∏–π–Ω —Ö—ç—Ä—ç–≥"),
)

# '–û—Å–æ–ª' target “Ø“Ø—Å–≥—ç—Ö ('–¢”©—Ä”©–ª' –±–∞–≥–∞–Ω–∞–∞—Å)
torol_col = resolve_col(df, ["–¢”©—Ä”©–ª"])  # —Ç–∞–Ω—ã –¥–∞—Ç–∞–Ω–¥ –±–∏–π
if torol_col is None:
    st.error("`–¢”©—Ä”©–ª` –±–∞–≥–∞–Ω–∞ –æ–ª–¥—Å–æ–Ω–≥“Ø–π. Target “Ø“Ø—Å–≥—ç—Ö –±–æ–ª–æ–º–∂–≥“Ø–π –±–∞–π–Ω–∞.")
    st.stop()

if target_mode == "–•–æ—ë—É–ª–∞–Ω–≥ 1 –≥—ç–∂ —Ç–æ–æ—Ü":
    df["–û—Å–æ–ª"] = df[torol_col].isin(["–ì—ç–º—Ç —Ö—ç—Ä—ç–≥", "–ó”©—Ä—á–ª–∏–π–Ω —Ö—ç—Ä—ç–≥"]).astype(int)
elif target_mode == "–ó”©–≤—Ö”©–Ω –ì—ç–º—Ç —Ö—ç—Ä—ç–≥":
    df["–û—Å–æ–ª"] = (df[torol_col] == "–ì—ç–º—Ç —Ö—ç—Ä—ç–≥").astype(int)
else:  # –ó”©–≤—Ö”©–Ω –ó”©—Ä—á–ª–∏–π–Ω —Ö—ç—Ä—ç–≥
    df["–û—Å–æ–ª"] = (df[torol_col] == "–ó”©—Ä—á–ª–∏–π–Ω —Ö—ç—Ä—ç–≥").astype(int)

# -------------------------- 5. –ò—Ä—ç—ç–¥“Ø–π–Ω –æ—Å–ª—ã–Ω —Ç–∞–∞–º–∞–≥–ª–∞–ª --------------------------
# -------------------------- 5. –ò—Ä—ç—ç–¥“Ø–π–Ω –æ—Å–ª—ã–Ω –¢–ê–ú–ê–ì–õ–ê–õ ‚Äî ”®–î”®–† –ë“Æ–† --------------------------

st.header("5. –ò—Ä—ç—ç–¥“Ø–π–Ω –æ—Å–ª—ã–Ω –¢–ê–ú–ê–ì–õ–ê–õ ‚Äî ”®–î”®–† –ë“Æ–†")
st.caption("–ó–∞–≥–≤–∞—Ä –Ω—å –∑”©–≤—Ö”©–Ω –ª–∞–≥—É—É–¥ (”©–º–Ω”©—Ö ”©–¥—Ä“Ø“Ø–¥–∏–π–Ω –æ—Å–ª—ã–Ω —Ç–æ–æ) + –∫–∞–ª–µ–Ω–¥–∞—Ä—ã–Ω —à–∏–Ω–∂“Ø“Ø–¥–∏–π–≥ (7 —Ö–æ–Ω–æ–≥–∏–π–Ω ”©–¥”©—Ä) –∞—à–∏–≥–ª–∞–Ω–∞.")

# ”®–¥”©—Ä –±“Ø—Ä–∏–π–Ω —Ü—É–≤–∞–∞
series_day = (
    df[df["–û—Å–æ–ª"] == 1]
    .groupby("Date")["–û—Å–æ–ª"].sum()
)
if series_day.empty:
    st.error("”®–¥”©—Ä –±“Ø—Ä–∏–π–Ω –æ—Å–ª—ã–Ω —Ç–æ–æ “Ø“Ø—Å–≥—ç—Ö—ç–¥ ”©–≥”©–≥–¥”©–ª –∞–ª–≥–∞.")
    st.stop()

full_dates = pd.date_range(series_day.index.min(), series_day.index.max(), freq="D")
day_df = pd.DataFrame(index=full_dates)
day_df["osol_count"] = series_day.reindex(full_dates, fill_value=0).astype(float)

# –õ–∞–≥—É—É–¥ (”©–¥”©—Ä)
n_lag = st.sidebar.slider("”®–¥”©—Ä–∏–π–Ω –ª–∞–≥ —Ü–æ–Ω—Ö (n_lag)", min_value=7, max_value=90, value=30, step=1)
for i in range(1, n_lag + 1):
    day_df[f"lag_{i}"] = day_df["osol_count"].shift(i)

# –ö–∞–ª–µ–Ω–¥–∞—Ä—ã–Ω —à–∏–Ω–∂“Ø“Ø–¥ (–¥–æ–ª–æ–æ —Ö–æ–Ω–æ–≥–∏–π–Ω ”©–¥”©—Ä)
dow = day_df.index.dayofweek
for j in range(7):
    day_df[f"dow_{j}"] = (dow == j).astype(int)

model_df = day_df.dropna().copy()
feature_cols = [f"lag_{i}" for i in range(1, n_lag + 1)] + [f"dow_{j}" for j in range(7)]
X_all = model_df[feature_cols].values
y_all = model_df["osol_count"].values.reshape(-1, 1)

# Leakage-free scaling + time-based split
split_ratio = st.sidebar.slider("Train ratio (”©–¥”©—Ä)", 0.5, 0.95, 0.8, 0.05)
train_size = int(len(X_all) * split_ratio)
X_train_raw, X_test_raw = X_all[:train_size], X_all[train_size:]
y_train_raw, y_test_raw = y_all[:train_size], y_all[train_size:]

scaler_X = MinMaxScaler()
scaler_y = MinMaxScaler()
X_train = scaler_X.fit_transform(X_train_raw)
X_test  = scaler_X.transform(X_test_raw)
y_train = scaler_y.fit_transform(y_train_raw).flatten()
y_test  = scaler_y.transform(y_test_raw).flatten()

estimators = [
    ("rf", RandomForestRegressor(n_estimators=120, random_state=seed)),
    ("ridge", Ridge()),
    ("dt", DecisionTreeRegressor(random_state=seed)),
]

MODEL_LIST = [
    ("LinearRegression", LinearRegression()),
    ("Ridge", Ridge()),
    ("Lasso", Lasso()),
    ("DecisionTree", DecisionTreeRegressor(random_state=seed)),
    ("RandomForest", RandomForestRegressor(random_state=seed)),
    ("ExtraTrees", ExtraTreesRegressor(random_state=seed)),
    ("GradientBoosting", GradientBoostingRegressor(random_state=seed)),
    ("HistGB", HistGradientBoostingRegressor(random_state=seed)),
    ("AdaBoost", AdaBoostRegressor(random_state=seed)),
    ("KNeighbors", KNeighborsRegressor()),
    ("SVR", SVR()),
    ("MLPRegressor", MLPRegressor(hidden_layer_sizes=(64, 32), max_iter=800, random_state=seed)),
    ("ElasticNet", ElasticNet()),
    ("Stacking", StackingRegressor(estimators=estimators, final_estimator=LinearRegression(), cv=5)),
]
if XGBRegressor is not None:
    MODEL_LIST.append(("XGBRegressor", XGBRegressor(verbosity=0, random_state=seed)))
if CatBoostRegressor is not None:
    MODEL_LIST.append(("CatBoostRegressor", CatBoostRegressor(verbose=0, random_state=seed)))
if LGBMRegressor is not None:
    MODEL_LIST.append(("LGBMRegressor", LGBMRegressor(random_state=seed)))

progress_bar = st.progress(0, text="ML –º–æ–¥–µ–ª–∏–π–≥ —Å—É—Ä–≥–∞–∂ –±–∞–π–Ω–∞...")
results, y_preds = [], {}
for i, (name, model) in enumerate(MODEL_LIST):
    try:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        y_preds[name] = y_pred
        mae = mean_absolute_error(y_test, y_pred)
        rmse = float(np.sqrt(mean_squared_error(y_test, y_pred)))
        r2 = r2_score(y_test, y_pred)
        results.append({"Model": name, "MAE": mae, "RMSE": rmse, "R2": r2})
    except Exception as e:
        results.append({"Model": name, "MAE": np.nan, "RMSE": np.nan, "R2": np.nan, "Error": str(e)})
    progress_bar.progress(min(int((i + 1) / len(MODEL_LIST) * 100), 100), text=f"{name} –¥—É—É—Å–ª–∞–∞")
progress_bar.empty()
st.success("–ë“Ø—Ö ML –º–æ–¥–µ–ª —Å—É—Ä–≥–∞–≥–¥–ª–∞–∞!")
results_df = pd.DataFrame(results).sort_values("RMSE", na_position="last")
st.dataframe(results_df, use_container_width=True)

with pd.ExcelWriter("model_metrics_daily.xlsx", engine="xlsxwriter") as writer:
    results_df.to_excel(writer, index=False)
with open("model_metrics_daily.xlsx", "rb") as f:
    st.download_button("–ú–æ–¥–µ–ª–∏–π–Ω –º–µ—Ç—Ä–∏–∫ (”©–¥”©—Ä) Excel —Ç–∞—Ç–∞—Ö", data=f, file_name="model_metrics_daily.xlsx",
                       mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")

# --- –ò—Ä—ç—ç–¥“Ø–π–Ω ”©–¥”©—Ä —Ç—É—Ç–º—ã–Ω –ø—Ä–æ–≥–Ω–æ–∑ (—ç–∫–∑–æ–≥–µ–Ω –∫–∞–ª–µ–Ω–¥–∞—Ä –∞—à–∏–≥–ª–∞–Ω–∞) ---
def _dow_vec(d: int) -> np.ndarray:
    v = np.zeros(7); 
    if 0 <= d <= 6: v[d] = 1
    return v

def forecast_next_daily(model, last_lags_raw: np.ndarray, last_date: pd.Timestamp, steps: int) -> np.ndarray:
    seq = last_lags_raw.astype(float).copy()   # [lag_1, ..., lag_n]
    preds_raw, cur_date = [], last_date
    for _ in range(steps):
        next_date = cur_date + pd.Timedelta(days=1)
        x_raw = np.concatenate([seq, _dow_vec(next_date.dayofweek)])
        x_scaled = scaler_X.transform(x_raw.reshape(1, -1))
        yhat_scaled = model.predict(x_scaled)[0]
        yhat_raw = scaler_y.inverse_transform(np.array([[yhat_scaled]])).ravel()[0]
        preds_raw.append(yhat_raw)
        seq = np.concatenate([[yhat_raw], seq[:-1]])  # —à–∏–Ω—ç –ª–∞–≥_1 = ”©–Ω”©”©–¥—Ä–∏–π–Ω —Ç–∞–∞–º–∞–≥
        cur_date = next_date
    return np.array(preds_raw)

# Test –¥—ç—ç—Ä—Ö –±–æ–¥–∏—Ç/—Ç–∞–∞–º–∞–≥
idx_all = model_df.index
test_dates = idx_all[-len(X_test):]
true_test_raw = scaler_y.inverse_transform(y_test.reshape(-1, 1)).ravel()
test_preds_df = pd.DataFrame({"date": test_dates, "real": true_test_raw})
for name, yhat_scaled in y_preds.items():
    test_preds_df[name] = scaler_y.inverse_transform(np.array(yhat_scaled).reshape(-1, 1)).ravel()

# –ò—Ä—ç—ç–¥“Ø–π–Ω 30 —Ö–æ–Ω–æ–≥–∏–π–Ω —Ö“Ø—Å–Ω—ç–≥—Ç
last_date = model_df.index[-1]
last_lags_raw = model_df[[f"lag_{i}" for i in range(1, n_lag + 1)]].iloc[-1].values
future_dates_30 = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=30, freq="D")
future_preds_df = pd.DataFrame({"date": future_dates_30})
for name, model in MODEL_LIST:
    if name in y_preds:
        future_preds_df[name] = forecast_next_daily(model, last_lags_raw, last_date, steps=30)

with pd.ExcelWriter("model_predictions_daily.xlsx", engine="xlsxwriter") as writer:
    test_preds_df.to_excel(writer, index=False, sheet_name="Test_Predictions_Daily")
    future_preds_df.to_excel(writer, index=False, sheet_name="Future_30D_Predictions")
with open("model_predictions_daily.xlsx", "rb") as f:
    st.download_button("Test/Forecast (”©–¥”©—Ä) Excel —Ç–∞—Ç–∞—Ö", data=f, file_name="model_predictions_daily.xlsx",
                       mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")

st.subheader("Test –¥–∞—Ç–∞–Ω –¥—ç—ç—Ä—Ö –º–æ–¥–µ–ª –±“Ø—Ä–∏–π–Ω –±–æ–¥–∏—Ç –±–æ–ª–æ–Ω —Ç–∞–∞–º–∞–≥–ª–∞—Å–∞–Ω (”©–¥”©—Ä, —Ç–æ–ª–≥–æ–π 10 –º”©—Ä):")
st.dataframe(test_preds_df.head(10), use_container_width=True)

st.subheader("–•–æ—Ä–∏–∑–æ–Ω—Ç —Å–æ–Ω–≥–æ–∂ –≥—Ä–∞—Ñ–∏–∫–∞–∞—Ä —Ö–∞—Ä–∞—Ö:")
forecast_steps = {"7 —Ö–æ–Ω–æ–≥": 7, "14 —Ö–æ–Ω–æ–≥": 14, "30 —Ö–æ–Ω–æ–≥": 30, "90 —Ö–æ–Ω–æ–≥": 90,"180 —Ö–æ–Ω–æ–≥": 180,"365 —Ö–æ–Ω–æ–≥": 365}
selected_model = st.selectbox("–ú–æ–¥–µ–ª—å —Å–æ–Ω–≥–æ—Ö:", list(y_preds.keys()))
selected_h = st.selectbox("–•–æ—Ä–∏–∑–æ–Ω—Ç:", list(forecast_steps.keys()), index=2)
steps = forecast_steps[selected_h]
plot_future = forecast_next_daily(dict(MODEL_LIST)[selected_model], last_lags_raw, last_date, steps)
plot_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=steps, freq="D")
future_df = pd.DataFrame({"date": plot_dates, "forecast": plot_future})
fig = px.line(future_df, x="date", y="forecast", markers=True, title=f"{selected_model} ‚Äî –∏—Ä—ç—Ö {steps} —Ö–æ–Ω–æ–≥–∏–π–Ω –ø—Ä–æ–≥–Ω–æ–∑ (”©–¥”©—Ä)")
st.plotly_chart(fig, use_container_width=True)


# -------------------------- Hotspot (DBSCAN) --------------------------

st.subheader("–ê–Ω—Ö–∞–∞—Ä–∞—Ö –≥–∞–∑—Ä—ã–Ω –±–∞–π—Ä—à–∏–ª (DBSCAN –∫–ª–∞—Å—Ç–µ—Ä—á–∏–ª—Å–∞–Ω hotspot)")
if lat_col and lon_col:
    # –°“Ø“Ø–ª–∏–π–Ω 12 —Å–∞—Ä—ã–Ω –û–õ–û–ù–¢–û–ô (–û—Å–æ–ª==1) –º”©—Ä“Ø“Ø–¥—ç—ç—Ä –∫–ª–∞—Å—Ç–µ—Ä—á–∏–ª–Ω–∞
    recent_df = df[(df["–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ"] >= (df["–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ"].max() - pd.DateOffset(months=12))) & (df["–û—Å–æ–ª"] == 1)].copy()
    recent_df = recent_df.dropna(subset=[lat_col, lon_col]).copy()
    coords = recent_df[[lat_col, lon_col]].to_numpy()
    if len(coords) >= 3:
        kms_per_radian = 6371.0088
        epsilon = 0.1 / kms_per_radian  # ‚âà100–º
        try:
            db = DBSCAN(eps=epsilon, min_samples=3, algorithm="ball_tree", metric="haversine").fit(np.radians(coords))
            recent_df["cluster"] = db.labels_
        except Exception:
            # –ó–∞—Ä–∏–º sklearn —Ö—É–≤–∏–ª–±–∞—Ä—Ç metric="haversine" –∞—Å—É—É–¥–∞–ª –≥–∞—Ä–≤–∞–ª euclidean-–¥ —à–∏–ª–∂–∏–Ω—ç (”©—Ä–≥”©—Ä”©–≥/—É—Ä—Ç—Ä–∞–≥–∏–π–Ω —Ö—ç–º–∂—ç—ç–Ω–¥ –æ–π—Ä–æ–ª—Ü–æ–æ–ª–æ–ª)
            db = DBSCAN(eps=0.001, min_samples=3, metric="euclidean").fit(coords)
            recent_df["cluster"] = db.labels_
    else:
        recent_df["cluster"] = -1

    hotspots = (
        recent_df[recent_df["cluster"] != -1]
        .groupby("cluster")
        .agg(
            n_osol=("–û—Å–æ–ª", "sum"),
            lat=(lat_col, "mean"),
            lon=(lon_col, "mean"),
        )
        .sort_values("n_osol", ascending=False)
        .reset_index()
    )

    map_center_lat = hotspots["lat"].mean() if not hotspots.empty else (recent_df[lat_col].mean() if len(recent_df) else 47)
    map_center_lon = hotspots["lon"].mean() if not hotspots.empty else (recent_df[lon_col].mean() if len(recent_df) else 106)
    m = folium.Map(location=[map_center_lat, map_center_lon], zoom_start=12 if not hotspots.empty else 6)

    for _, row in hotspots.iterrows():
        folium.Circle(
            location=[row["lat"], row["lon"]],
            radius=120 + row["n_osol"] * 1,
            color="orange",
            fill=True,
            fill_opacity=0.55,
            popup=folium.Popup(f"<b>Hotspot (–∫–ª–∞—Å—Ç–µ—Ä)</b><br>–û—Å–ª—ã–Ω —Ç–æ–æ: <b>{int(row['n_osol'])}</b>", max_width=350),
        ).add_to(m)

    st_folium(m, width=1920, height=700)
else:
    st.info("–ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã–Ω –±–∞–≥–∞–Ω—É—É–¥ (”®—Ä–≥”©—Ä”©–≥/–£—Ä—Ç—Ä–∞–≥ —ç—Å—Ö“Ø–ª lat/lon) –±–∞–π—Ö–≥“Ø–π —Ç—É–ª –≥–∞–∑—Ä—ã–Ω –∑—É—Ä–≥–∏–π–≥ –∞–ª–≥–∞—Å–ª–∞–∞.")

# -------------------------- 1. –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–π–Ω —à–∏–Ω–∂–∏–ª–≥—ç—ç --------------------------

st.header("1. –û—Å–æ–ª–¥ –Ω”©–ª”©”©–ª”©—Ö —Ö“Ø—á–∏–Ω –∑“Ø–π–ª—Å–∏–π–Ω —Ç–∞—Ä—Ö–∞–ª—Ç/–∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π–Ω —à–∏–Ω–∂–∏–ª–≥—ç—ç")
st.write("–î–æ–æ—Ä—Ö multiselect –¥—ç—ç—Ä—ç—ç—Å –∏—Ö–¥—ç—ç 15 —Ö—É–≤—å—Å–∞–≥—á —Å–æ–Ω–≥–æ–∂ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π–Ω –º–∞—Ç—Ä–∏—Ü —Ö–∞—Ä–Ω–∞.")

vars_for_corr = ["Year"]
vars_for_corr += [c for c in df.columns if c.startswith("–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ –∂–∏–ª ")][:10]
vars_for_corr += [c for c in (binary_cols + num_additional) if c in df.columns]
# –î–∞–≤—Ö–∞—Ä–¥–∞–ª –∞—Ä–∏–ª–≥–∞—Ö
vars_for_corr = list(dict.fromkeys(vars_for_corr))

if len(vars_for_corr) > 1:
    Xx = df[vars_for_corr].fillna(0.0).values
    yy = pd.to_numeric(df["–û—Å–æ–ª"], errors="coerce").fillna(0).values
    try:
        rf_cor = RandomForestRegressor(n_estimators=200, random_state=seed)
        rf_cor.fit(Xx, yy)
        importances_cor = rf_cor.feature_importances_
        indices_cor = np.argsort(importances_cor)[::-1]
        top_k_cor = min(15, len(vars_for_corr))
        default_cols = [vars_for_corr[i] for i in indices_cor[:top_k_cor]]
    except Exception:
        default_cols = vars_for_corr[: min(15, len(vars_for_corr))]
else:
    default_cols = vars_for_corr

selected_cols = st.multiselect(
    "–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–π–Ω –º–∞—Ç—Ä–∏—Ü–∞–¥ –æ—Ä—É—É–ª–∞—Ö —Ö—É–≤—å—Å–∞–≥—á–∏–¥:", vars_for_corr, default=default_cols, max_selections=15
)
if selected_cols:
    st.pyplot(plot_correlation_matrix(df, "Correlation Matrix", selected_cols))
else:
    st.warning("–°–æ–Ω–≥–æ—Ö —Ö—É–≤—å—Å–∞–≥—á–∏–¥—ã–≥ –æ—Ä—É—É–ª–Ω–∞ —É—É!")

# -------------------------- 2. –û—Å–ª—ã–Ω ”©—Å”©–ª—Ç–∏–π–Ω —Ç—Ä–µ–Ω–¥ --------------------------
st.header("2. –û—Å–ª—ã–Ω ”©—Å”©–ª—Ç–∏–π–Ω —Ç—Ä–µ–Ω–¥")
mode_trend = st.radio("–î—ç–ª–≥—ç—Ü–ª—ç—Ö –æ–≥–Ω–æ–æ–Ω—ã –Ω—è–≥—Ç—Ä–∞–ª:", ["”®–¥”©—Ä", "–°–∞—Ä"], index=0, horizontal=True)

if mode_trend == "”®–¥”©—Ä":
    daily_tr = df[df["–û—Å–æ–ª"] == 1].groupby("Date").agg(osol_count=("–û—Å–æ–ª", "sum")).reset_index()
    fig = px.line(daily_tr, x="Date", y="osol_count", markers=True, labels={"Date":"–û–≥–Ω–æ–æ","osol_count":"–û—Å–ª—ã–Ω —Ç–æ–æ"})
else:
    monthly_tr = df[df["–û—Å–æ–ª"] == 1].groupby(["Year","Month"]).agg(osol_count=("–û—Å–æ–ª","sum")).reset_index()
    monthly_tr["YearMonth"] = monthly_tr.apply(lambda x: f"{int(x['Year'])}-{int(x['Month']):02d}", axis=1)
    fig = px.line(monthly_tr, x="YearMonth", y="osol_count", markers=True, labels={"YearMonth":"–û–Ω-–°–∞—Ä","osol_count":"–û—Å–ª—ã–Ω —Ç–æ–æ"})
st.plotly_chart(fig, use_container_width=True)


# -------------------------- 4. –ö–∞—Ç–µ–≥–æ—Ä–∏ —Ö–∞–º–∞–∞—Ä–∞–ª (Cram√©r‚Äôs V + Chi-square) --------------------------

st.header("4. –ö–∞—Ç–µ–≥–æ—Ä–∏ —Ö—É–≤—å—Å–∞–≥—á–¥—ã–Ω —Ö–æ–æ—Ä–æ–Ω–¥—ã–Ω —Ö–∞–º–∞–∞—Ä–∞–ª (Cram√©r‚Äôs V –±–æ–ª–æ–Ω Chi-square)")

low_card_cols = []
for c in df.columns:
    if c in ["–û—Å–æ–ª", "–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ", "Year", "Month", "Day"]:
        continue
    u = df[c].dropna().unique()
    if 2 <= len(u) <= 15:
        low_card_cols.append(c)

categorical_cols = sorted(list(set(binary_cols + low_card_cols)))
if len(categorical_cols) < 2:
    st.info("–ö–∞—Ç–µ–≥–æ—Ä–∏ –±–∞–≥–∞–Ω–∞ (2-–æ–æ—Å 15 —è–ª–≥–∞–∞—Ç–∞–π —É—Ç–≥–∞—Ç–∞–π) –æ–ª–¥—Å–æ–Ω–≥“Ø–π.")
else:
    var1 = st.selectbox("1-—Ä –∫–∞—Ç–µ–≥–æ—Ä–∏ —Ö—É–≤—å—Å–∞–≥—á:", categorical_cols)
    var2 = st.selectbox("2-—Ä –∫–∞—Ç–µ–≥–æ—Ä–∏ —Ö—É–≤—å—Å–∞–≥—á:", [c for c in categorical_cols if c != var1])

    table = pd.crosstab(df[var1], df[var2])
    chi2, p, dof, expected = chi2_contingency(table)

    # –•–æ—ë—Ä —Ö—É–≤–∏–ª–±–∞—Ä—ã–Ω V
    n = table.values.sum()
    r, k = table.shape
    cramers_v_naive = np.sqrt(chi2 / (n * (min(k, r) - 1))) if min(k, r) > 1 else np.nan
    cramers_v_bc = cramers_v_bias_corrected(table)

    st.subheader("1. Chi-square —Ç–µ—Å—Ç")
    st.write("p-value < 0.05 –±–æ–ª —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏–π–Ω —Ö—É–≤—å–¥ —Ö–∞–º–∞–∞—Ä–∞–ª—Ç–∞–π –≥—ç–∂ “Ø–∑–Ω—ç.")
    st.write(f"**Chi-square statistic:** {chi2:.3f}")
    st.write(f"**p-value:** {p:.4f}")
    if p < 0.05:
        st.success("p < 0.05 ‚Üí –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏–π–Ω —Ö—É–≤—å–¥ —Ö–∞–º–∞–∞—Ä–∞–ª—Ç–∞–π!")
    else:
        st.info("p ‚â• 0.05 ‚Üí –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏–π–Ω —Ö—É–≤—å–¥ —Ö–∞–º–∞–∞—Ä–∞–ª–≥“Ø–π.")

    use_bc = st.checkbox("Bias-corrected Cram√©r‚Äôs V (—Å–∞–Ω–∞–ª –±–æ–ª–≥–æ–∂ –±–∞–π–Ω–∞)", value=True)
    v_to_show = cramers_v_bc if use_bc else cramers_v_naive

    st.subheader("2. Cram√©r‚Äôs V")
    st.write("0-–¥ –æ–π—Ä—Ö–æ–Ω –±–æ–ª –±–∞—Ä–∞–≥ —Ö–∞–º–∞–∞—Ä–∞–ª–≥“Ø–π, 1-–¥ –æ–π—Ä –±–æ–ª —Ö“Ø—á—Ç—ç–π —Ö–∞–º–∞–∞—Ä–∞–ª—Ç–∞–π.")
    st.write(f"**Cram√©r‚Äôs V:** {v_to_show:.3f} (0=—Ö–∞–º–∞–∞—Ä–∞–ª–≥“Ø–π, 1=—Ö“Ø—á—Ç—ç–π —Ö–∞–º–∞–∞—Ä–∞–ª)")

    st.write("**Crosstab:**")
    st.dataframe(table, use_container_width=True)

# -------------------------- –¢”©—Å–ª–∏–π–Ω —Ç”©–≥—Å–≥”©–ª --------------------------

st.markdown(
    """
    ---
    **–¢–∞–π–ª–±–∞—Ä**  
    ‚Ä¢ –ó–æ—Ä–∏–ª—Ç–æ—Ç —Ö—É–≤—å—Å–∞–≥—á–∏–π–≥ Sidebar –¥—ç—ç—Ä—ç—ç—Å —Å–æ–Ω–≥–æ—Ö –±–æ–ª–æ–º–∂—Ç–æ–π (–ì—ç–º—Ç —Ö—ç—Ä—ç–≥/–ó”©—Ä—á–ª–∏–π–Ω —Ö—ç—Ä—ç–≥/–•–æ—Å–æ–ª—Å–æ–Ω).  
    ‚Ä¢ "Leakage-free" scaling: scaler-—É—É–¥—ã–≥ –∑”©–≤—Ö”©–Ω train –¥—ç—ç—Ä fit —Ö–∏–π–¥—ç–≥ —Ç—É–ª “Ø–Ω—ç–ª–≥—ç—ç –∏–ª“Ø“Ø –Ω–∞–π–¥–≤–∞—Ä—Ç–∞–π.  
    ‚Ä¢ ‚ö° –•—É—Ä–¥–∞–Ω –∞–∂–∏–ª–ª—É—É–ª–∞—Ö–∞–¥ 200 –º”©—Ä–∏–π–Ω —Å–∞–Ω–∞–º—Å–∞—Ä–≥“Ø–π –¥—ç–¥ –¥—ç—ç–∂ –∞–≤—á —Ç—É—Ä—à–∏–Ω–∞.  
    ‚Ä¢ –•—ç—Ä—ç–≤ XGBoost/LightGBM/CatBoost —Å—É—É–ª–≥–∞–≥–¥–∞–∞–≥“Ø–π –±–æ–ª —Å—É—É–ª–≥–∞–ª–≥“Ø–π–≥—ç—ç—Ä –±—É—Å–∞–¥ –º–æ–¥–µ–ª“Ø“Ø–¥ –∞–∂–∏–ª–ª–∞–Ω–∞.  
    """
)