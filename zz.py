# -*- coding: utf-8 -*-
# ============================================================
# –ó–∞–º —Ç—ç—ç–≤—Ä–∏–π–Ω –æ—Å–æ–ª ‚Äî Auto ML & Hotspot Dashboard (Streamlit)
# –•—É–≤–∏–ª–±–∞—Ä: 2025-08-17b ‚Äî Leakage-free scaling, 200-row quick sample, bias-corrected Cram√©r's V
# –¢–∞–π–ª–±–∞—Ä:
#  - –•–∞–≤—Å–∞—Ä–≥–∞—Å–∞–Ω Excel ("–∫–æ–¥–ª–æ–≥–¥—Å–æ–Ω - Copy.xlsx")-—Ç–∞–π —à—É—É–¥ –∑–æ—Ö–∏—Ü–Ω–æ.
#  - Binary (0/1) –±“Ø—Ö –±–∞–≥–∞–Ω—ã–≥ –∞–≤—Ç–æ–º–∞—Ç–∞–∞—Ä –∏–ª—Ä“Ø“Ø–ª–∂, –º–æ–¥–µ–ª/–∫–æ—Ä—Ä–µ–ª—è—Ü/—Ö–æ—Ç—Å–ø–æ—Ç–æ–¥ –∞—à–∏–≥–ª–∞–Ω–∞.
#  - –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç –±–∞–≥–∞–Ω—É—É–¥ (”®—Ä–≥”©—Ä”©–≥/–£—Ä—Ç—Ä–∞–≥ —ç—Å–≤—ç–ª lat/lon) –±–∞–π–≤–∞–ª –≥–∞–∑—Ä—ã–Ω –∑—É—Ä–∞–≥ –∑—É—Ä–Ω–∞.
#  - –û–ª–æ–Ω ML –º–æ–¥–µ–ª —Å—É—Ä–≥–∞–ª—Ç, –º–µ—Ç—Ä–∏–∫/—Ç–∞–∞–º–∞–≥–ª–∞–ª—ã–≥ Excel –±–æ–ª–≥–æ–Ω —Ç–∞—Ç–∞—Ö –±–æ–ª–æ–º–∂—Ç–æ–π.
#  - "–û—Å–æ–ª" –±–∞–≥–∞–Ω–∞ –±–∞–π—Ö–≥“Ø–π —Ç–æ—Ö–∏–æ–ª–¥–æ–ª–¥ "–¢”©—Ä”©–ª"-”©”©—Å (–ì—ç–º—Ç —Ö—ç—Ä—ç–≥/–ó”©—Ä—á–ª–∏–π–Ω —Ö—ç—Ä—ç–≥) –∑–æ—Ä–∏–ª—Ç—ã–≥ “Ø“Ø—Å–≥—ç–Ω—ç.
#  - –®–∏–Ω—ç—á–ª—ç–ª—Ç“Ø“Ø–¥: (i) scaler-–∏–π–≥ train-–¥ –ª fit —Ö–∏–π–∂ data leakage –∞—Ä–∏–ª–≥–∞–≤, (ii) 200 –º”©—Ä–∏–π–Ω —Ö—É—Ä–¥–∞–Ω –∞–∂–∏–ª–ª—É—É–ª–∞—Ö —Å–æ–Ω–≥–æ–ª—Ç, (iii) Cram√©r's V-–∏–π–Ω bias-corrected —Ö—É–≤–∏–ª–±–∞—Ä –Ω—ç–º—ç–≤.
# –ì“Ø–π—Ü—ç—Ç–≥—ç—Ö: streamlit run osol_auto_streamlit.py
# ============================================================

from __future__ import annotations
import io
import numpy as np
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

from pathlib import Path

from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import (
    RandomForestRegressor,
    GradientBoostingRegressor,
    AdaBoostRegressor,
    ExtraTreesRegressor,
    StackingRegressor,
    HistGradientBoostingRegressor,
)
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.cluster import DBSCAN

# –ì—É—Ä–∞–≤–¥–∞–≥—á gradient boosting —Å–∞–Ω–≥—É—É–¥ (optional)
try:
    from xgboost import XGBRegressor  # type: ignore
except Exception:
    XGBRegressor = None  # pragma: no cover
try:
    from lightgbm import LGBMRegressor  # type: ignore
except Exception:
    LGBMRegressor = None  # pragma: no cover
try:
    from catboost import CatBoostRegressor  # type: ignore
except Exception:
    CatBoostRegressor = None  # pragma: no cover

from scipy.stats import chi2_contingency
import folium
from streamlit_folium import st_folium

# -------------------------- UI setup --------------------------
st.set_page_config(page_title="–û—Å–æ–ª ‚Äî Auto ML & Hotspot (auto-binary)", layout="wide")
st.title("–ó–∞–º —Ç—ç—ç–≤—Ä–∏–π–Ω –æ—Å–ª—ã–Ω –∞–Ω–∞–ª–∏–∑ ‚Äî Auto (Binary autodetect)")

# -------------------------- –¢—É—Å–ª–∞—Ö —Ñ—É–Ω–∫—Ü—É—É–¥ --------------------------

def _canon(s: str) -> str:
    return "".join(str(s).lower().split()) if isinstance(s, str) else str(s)

def resolve_col(df: pd.DataFrame, candidates) -> str | None:
    """–ù—ç—Ä—à–ª–∏–π–≥ robust –±–∞–π–¥–ª–∞–∞—Ä –æ–ª–Ω–æ (whitespace/—Ç–æ–º-–∂–∏–∂–∏–≥ “Ø–ª —Ç–æ–æ–Ω–æ)."""
    lut = {_canon(c): c for c in df.columns}
    for name in candidates:
        key = _canon(name)
        if key in lut:
            return lut[key]
    return None

def is_binary_series(s: pd.Series) -> bool:
    vals = pd.to_numeric(s.dropna(), errors="coerce").dropna().unique()
    if len(vals) == 0:
        return False
    return set(np.unique(vals)).issubset({0, 1})

def clean_road_width(width):
    """'–ê–≤—Ç–æ –∑–∞–º - –ó–æ—Ä—á–∏—Ö —Ö—ç—Å–≥–∏–π–Ω ”©—Ä–≥”©–Ω' —Ç–∞–ª–±–∞—Ä—ã–Ω —Ç–µ–∫—Å—Ç—ç–Ω —É—Ç–≥—É—É–¥—ã–≥ —Ç–æ–æ–Ω –±–æ–ª–≥–æ—Ö."""
    if pd.isna(width):
        return np.nan
    if isinstance(width, (int, float)):
        return float(width)
    if isinstance(width, str):
        w = (
            width.replace("–º", "")
            .replace("-—ç—ç—Å –¥—ç—ç—à", "")
            .replace("—Ö“Ø—Ä—Ç—ç–ª", "")
            .replace(",", ".")
            .strip()
        )
        if "-" in w:
            try:
                low, high = map(float, w.split("-"))
                return (low + high) / 2
            except Exception:
                return np.nan
        try:
            return float(w)
        except Exception:
            return np.nan
    return np.nan

def plot_correlation_matrix(df, title, columns):
    n_unique = df[columns].nunique()
    if all((n_unique == 2) | (n_unique == 1)):
        st.warning("–°–æ–Ω–≥–æ—Å–æ–Ω –±–∞–≥–∞–Ω—É—É–¥ –Ω—å one-hot (0/1) —Ö—ç–ª–±—ç—Ä—Ç—ç–π –±–∞–π–Ω–∞. –ò–π–º —Ç–æ—Ö–∏–æ–ª–¥–æ–ª–¥ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏ -1~1 —Ç—É–π–ª—Ä—É—É–≥–∞–∞ —Ö—ç–ª–±–∏–π–∂ —Ö–∞—Ä–∞–≥–¥–∞–∂ –±–æ–ª–Ω–æ.")
    df_encoded = df[columns].copy()
    for col in df_encoded.columns:
        if df_encoded[col].dtype == "object":
            df_encoded[col] = pd.Categorical(df_encoded[col]).codes
    corr_matrix = df_encoded.corr(numeric_only=True)
    corr_matrix = corr_matrix.iloc[::-1]
    fig, ax = plt.subplots(figsize=(max(8, 1.5*len(columns)), max(6, 1.2*len(columns))))
    sns.heatmap(corr_matrix, annot=True, cmap="coolwarm", vmin=-1, vmax=1, center=0, ax=ax, fmt=".3f")
    plt.title(title)
    plt.tight_layout()
    return fig

# Bias-corrected Cram√©r's V (Bergsma, 2013)

def cramers_v_bias_corrected(table: pd.DataFrame) -> float:
    chi2, _, _, _ = chi2_contingency(table)
    n = table.values.sum()
    phi2 = chi2 / max(n, 1)
    r, k = table.shape
    phi2_corr = max(0, phi2 - (k-1)*(r-1)/(max(n-1, 1)))
    r_corr = r - (r-1)**2 / max(n-1, 1)
    k_corr = k - (k-1)**2 / max(n-1, 1)
    denom = max(min(k_corr-1, r_corr-1), 1e-12)
    return float(np.sqrt(phi2_corr / denom))

# -------------------------- ”®–≥”©–≥–¥”©–ª –∞—á–∞–∞–ª–∞–ª—Ç --------------------------

@st.cache_data(show_spinner=True)
def load_data(default_path: str = "–∫–æ–¥–ª–æ–≥–¥—Å–æ–Ω.xlsx"):
    """
    - Sidebar –¥—ç—ç—Ä—ç—ç—Å .xlsx —Ñ–∞–π–ª–∞–∞—Ä upload —Ö–∏–π–∂ –±–æ–ª–Ω–æ.
    - –•—ç—Ä—ç–≤ –æ—Ä—É—É–ª–∞–∞–≥“Ø–π –±–æ–ª default_path-—ã–≥ —É–Ω—à–∏–Ω–∞.
    - –û–≥–Ω–æ–æ–Ω—ã –±–∞–≥–∞–Ω—ã–≥ robust –±–∞–π–¥–ª–∞–∞—Ä –æ–ª–Ω–æ, '–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ' “Ø“Ø—Å–≥—ç–Ω—ç.
    - –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç –±–∞ binary –±–∞–≥–∞–Ω—É—É–¥—ã–≥ –∞–≤—Ç–æ–º–∞—Ç–∞–∞—Ä –∏–ª—Ä“Ø“Ø–ª–Ω—ç.
    """
    up = st.sidebar.file_uploader("Excel —Ñ–∞–π–ª –æ—Ä—É—É–ª–∞—Ö (.xlsx)", type=["xlsx"])
    if up is not None:
        df = pd.read_excel(up)
    else:
        local = Path("/mnt/data/–∫–æ–¥–ª–æ–≥–¥—Å–æ–Ω - Copy.xlsx")
        if local.exists():
            df = pd.read_excel(local)
        else:
            df = pd.read_excel(default_path)

    # –ù—ç—Ä—à–∏–ª —Ü—ç–≤—ç—Ä–ª—ç–≥—ç—ç
    df.columns = [c.strip() if isinstance(c, str) else c for c in df.columns]

    # –û–≥–Ω–æ–æ –±–∞–≥–∞–Ω—ã–≥ robust –±–∞–π–¥–ª–∞–∞—Ä –æ–ª–æ—Ö
    recv_col = resolve_col(df, ["–•“Ø–ª—ç—ç–Ω –∞–≤—Å–∞–Ω", "–•“Ø–ª—ç—ç–Ω –∞–≤—Å–∞–Ω ", "–û–≥–Ω–æ–æ", "–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ", "–û—Å–æ–ª –æ–≥–Ω–æ–æ", "–û—Å–ª—ã–Ω –æ–≥–Ω–æ–æ", "Date"]) 
    if recv_col is None:
        st.error("–û–≥–Ω–æ–æ–Ω—ã –±–∞–≥–∞–Ω–∞ –æ–ª–¥—Å–æ–Ω–≥“Ø–π. –ñ–∏—à—ç—ç –Ω—å: '–•“Ø–ª—ç—ç–Ω –∞–≤—Å–∞–Ω'.")
        st.stop()

    # '–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ' “Ø“Ø—Å–≥—ç—Ö
    df["–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ"] = pd.to_datetime(df[recv_col], errors="coerce")
    df["Year"]  = df["–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ"].dt.year
    df["Month"] = df["–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ"].dt.month
    df["Day"]   = df["–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ"].dt.day_name()

    # –û–Ω –∂–∏–ª“Ø“Ø–¥–∏–π–Ω one-hot
    years = sorted(df["Year"].dropna().unique().tolist())
    for y in years:
        df[f"–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ –∂–∏–ª {int(y)}"] = (df["Year"] == int(y)).astype(int)
    if len(years) > 0:
        df["–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ –∂–∏–ª (min-max)"] = df["Year"].between(min(years), max(years)).astype(int)

    # –ó–∞–º—ã–Ω ”©—Ä–≥”©–Ω —Ü—ç–≤—ç—Ä–ª—ç—Ö (–±–∞–π–≤–∞–ª)
    if "–ê–≤—Ç–æ –∑–∞–º - –ó–æ—Ä—á–∏—Ö —Ö—ç—Å–≥–∏–π–Ω ”©—Ä–≥”©–Ω" in df.columns:
        df["–ê–≤—Ç–æ –∑–∞–º - –ó–æ—Ä—á–∏—Ö —Ö—ç—Å–≥–∏–π–Ω ”©—Ä–≥”©–Ω"] = df["–ê–≤—Ç–æ –∑–∞–º - –ó–æ—Ä—á–∏—Ö —Ö—ç—Å–≥–∏–π–Ω ”©—Ä–≥”©–Ω"].apply(clean_road_width)

    # –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç –Ω—ç—Ä—à–∏–ª
    lat_col = resolve_col(df, ["”®—Ä–≥”©—Ä”©–≥", "lat", "latitude"])
    lon_col = resolve_col(df, ["–£—Ä—Ç—Ä–∞–≥", "lon", "longitude"])

    # –ê–≤—Ç–æ–º–∞—Ç–∞–∞—Ä binary –±–∞–≥–∞–Ω—É—É–¥—ã–≥ –∏–ª—Ä“Ø“Ø–ª—ç—Ö
    exclude = {"–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ", "Year", "Month", "Day", "–¥/–¥", "–•–æ—Ä–æ–æ-–°—É–º", "–ê–π–º–∞–≥-–î“Ø“Ø—Ä—ç–≥"}
    if lat_col: exclude.add(lat_col)
    if lon_col: exclude.add(lon_col)
    binary_cols = [c for c in df.columns if c not in exclude and is_binary_series(df[c])]

    # –ù—ç–º—ç–ª—Ç —Ç–æ–æ–Ω candidate-—É—É–¥
    numeric_candidates = []
    if "–ê–≤—Ç–æ –∑–∞–º - –ó–æ—Ä—á–∏—Ö —Ö—ç—Å–≥–∏–π–Ω ”©—Ä–≥”©–Ω" in df.columns:
        numeric_candidates.append("–ê–≤—Ç–æ –∑–∞–º - –ó–æ—Ä—á–∏—Ö —Ö—ç—Å–≥–∏–π–Ω ”©—Ä–≥”©–Ω")

    # –î“Ø“Ø—Ä—ç–≥/–ê–π–º–∞–≥ fallback
    if "–î“Ø“Ø—Ä—ç–≥" not in df.columns:
        df["–î“Ø“Ø—Ä—ç–≥"] = 0
    if "–ê–π–º–∞–≥" not in df.columns:
        df["–ê–π–º–∞–≥"] = 0

    meta = {"lat_col": lat_col, "lon_col": lon_col, "binary_cols": binary_cols, "numeric_candidates": numeric_candidates, "years": years}
    return df, meta

# -------------------------- –ê—á–∞–∞–ª–∂ —ç—Ö–ª—ç—Ö --------------------------

df, meta = load_data()
lat_col, lon_col = meta["lat_col"], meta["lon_col"]
binary_cols = meta["binary_cols"]
num_additional = meta["numeric_candidates"]
years = meta["years"]

# -------------------------- Sidebar: Sampling & Seed --------------------------

st.sidebar.markdown("### ‚öôÔ∏è –ê—à–∏–≥–ª–∞–ª—Ç—ã–Ω —Ç–æ—Ö–∏—Ä–≥–æ–æ")
seed = int(st.sidebar.number_input("Random seed", value=42, step=1))
quick_sample = st.sidebar.checkbox("‚ö° 200 –º”©—Ä”©”©—Ä —Ö—É—Ä–¥–∞–Ω –∞–∂–∏–ª–ª—É—É–ª–∞—Ö (—Å–∞–Ω–∞–º—Å–∞—Ä–≥“Ø–π)", value=False)
if quick_sample and len(df) > 200:
    df = df.sample(n=200, random_state=seed).sort_values("–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ")

# -------------------------- Target —Ç–æ—Ö–∏—Ä–≥–æ–æ --------------------------

st.sidebar.markdown("### üéØ –ó–æ—Ä–∏–ª—Ç–æ—Ç —Ç–æ–¥–æ—Ä—Ö–æ–π–ª–æ–ª—Ç (–û—Å–æ–ª)")
target_mode = st.sidebar.radio(
    "–û—Å–æ–ª –≥—ç–∂ —Ç–æ–æ—Ü–æ—Ö –∞–Ω–≥–∏–ª–ª—ã–≥ —Å–æ–Ω–≥–æ–Ω–æ —É—É:",
    ("–•–æ—ë—É–ª–∞–Ω–≥ 1 –≥—ç–∂ —Ç–æ–æ—Ü", "–ó”©–≤—Ö”©–Ω –ì—ç–º—Ç —Ö—ç—Ä—ç–≥", "–ó”©–≤—Ö”©–Ω –ó”©—Ä—á–ª–∏–π–Ω —Ö—ç—Ä—ç–≥"),
)

# '–û—Å–æ–ª' target “Ø“Ø—Å–≥—ç—Ö ('–¢”©—Ä”©–ª' –±–∞–≥–∞–Ω–∞–∞—Å)
torol_col = resolve_col(df, ["–¢”©—Ä”©–ª"])  # —Ç–∞–Ω—ã –¥–∞—Ç–∞–Ω–¥ –±–∏–π
if torol_col is None:
    st.error("`–¢”©—Ä”©–ª` –±–∞–≥–∞–Ω–∞ –æ–ª–¥—Å–æ–Ω–≥“Ø–π. Target “Ø“Ø—Å–≥—ç—Ö –±–æ–ª–æ–º–∂–≥“Ø–π –±–∞–π–Ω–∞.")
    st.stop()

if target_mode == "–•–æ—ë—É–ª–∞–Ω–≥ 1 –≥—ç–∂ —Ç–æ–æ—Ü":
    df["–û—Å–æ–ª"] = df[torol_col].isin(["–ì—ç–º—Ç —Ö—ç—Ä—ç–≥", "–ó”©—Ä—á–ª–∏–π–Ω —Ö—ç—Ä—ç–≥"]).astype(int)
elif target_mode == "–ó”©–≤—Ö”©–Ω –ì—ç–º—Ç —Ö—ç—Ä—ç–≥":
    df["–û—Å–æ–ª"] = (df[torol_col] == "–ì—ç–º—Ç —Ö—ç—Ä—ç–≥").astype(int)
else:  # –ó”©–≤—Ö”©–Ω –ó”©—Ä—á–ª–∏–π–Ω —Ö—ç—Ä—ç–≥
    df["–û—Å–æ–ª"] = (df[torol_col] == "–ó”©—Ä—á–ª–∏–π–Ω —Ö—ç—Ä—ç–≥").astype(int)

# -------------------------- 5. –ò—Ä—ç—ç–¥“Ø–π–Ω –æ—Å–ª—ã–Ω —Ç–∞–∞–º–∞–≥–ª–∞–ª --------------------------

st.header("5. –ò—Ä—ç—ç–¥“Ø–π–Ω –æ—Å–ª—ã–Ω —Ç–∞–∞–º–∞–≥–ª–∞–ª (–û–ª–æ–Ω ML/DL –∑–∞–≥–≤–∞—Ä)")
st.caption("Binary (0/1) –±–∞–≥–∞–Ω—É—É–¥—ã–≥ –∞–≤—Ç–æ–º–∞—Ç–∞–∞—Ä –∏–ª—Ä“Ø“Ø–ª–∂, –∑–∞–≥–≤–∞—Ä—Ç –∞—à–∏–≥–ª–∞—Å–∞–Ω. Leakage-free scaling —Ö—ç—Ä—ç–≥–ª—ç—Å—ç–Ω.")

# Feature pool: '–û—Å–æ–ª'-–æ–æ—Å –±—É—Å–∞–¥ binary + –Ω—ç–º—ç–ª—Ç —Ç–æ–æ–Ω
feature_pool = [c for c in (binary_cols + num_additional) if c != "–û—Å–æ–ª"]
if len(feature_pool) == 0:
    st.error("Binary (0/1) —Ö—ç–ª–±—ç—Ä–∏–π–Ω –±–∞–≥–∞–Ω–∞ –æ–ª–¥—Å–æ–Ω–≥“Ø–π. Excel-—ç—ç —à–∞–ª–≥–∞–Ω–∞ —É—É.")
    st.stop()

# Target/Features
y_all = pd.to_numeric(df["–û—Å–æ–ª"], errors="coerce").fillna(0).values
X_all = df[feature_pool].fillna(0.0).values

# Top features via RandomForest
try:
    rf_global = RandomForestRegressor(n_estimators=300, random_state=seed)
    rf_global.fit(X_all, y_all)
    importances = rf_global.feature_importances_
    indices = np.argsort(importances)[::-1]
    top_k = min(14, len(feature_pool))
    top_features = [feature_pool[i] for i in indices[:top_k]]
    st.caption("RandomForest-–∞–∞—Ä —Å–æ–Ω–≥–æ—Å–æ–Ω –Ω”©–ª”©”© –∏—Ö—Ç—ç–π —à–∏–Ω–∂“Ø“Ø–¥:")
    st.write(top_features)
except Exception as e:
    st.warning(f"Top features —Ç–æ–æ—Ü–æ—Ö–æ–¥ –∞–ª–¥–∞–∞ –≥–∞—Ä–ª–∞–∞: {e}")
    top_features = feature_pool[:min(14, len(feature_pool))]

# –°–∞—Ä –±“Ø—Ä–∏–π–Ω –∞–≥—Ä–µ–≥–∞—Ç (target=–û—Å–æ–ª==1 –¥–∞–≤—Ç–∞–º–∂)
monthly_target = (
    df[df["–û—Å–æ–ª"] == 1]
    .groupby(["Year", "Month"])
    .agg(osol_count=("–û—Å–æ–ª", "sum"))
    .reset_index()
)
monthly_target["date"] = pd.to_datetime(monthly_target[["Year", "Month"]].assign(DAY=1))
monthly_features = df.groupby(["Year", "Month"])[top_features].sum().reset_index()

grouped = pd.merge(monthly_target, monthly_features, on=["Year", "Month"], how="left").sort_values("date").reset_index(drop=True)

# Lag “Ø“Ø—Å–≥—ç—Ö
n_lag = st.sidebar.slider("–°–∞—Ä–Ω—ã –ª–∞–≥ —Ü–æ–Ω—Ö (n_lag)", min_value=6, max_value=18, value=12, step=1)
for i in range(1, n_lag + 1):
    grouped[f"osol_lag_{i}"] = grouped["osol_count"].shift(i)

grouped = grouped.dropna().reset_index(drop=True)

if grouped.empty or len(grouped) < 10:
    st.warning(f"–°—É—Ä–≥–∞–ª—Ç —Ö–∏–π—Ö—ç–¥ —Ö–∞–Ω–≥–∞–ª—Ç—Ç–∞–π —Å–∞—Ä —Ç—É—Ç–º—ã–Ω ”©–≥”©–≥–¥”©–ª –∞–ª–≥–∞ (lag={n_lag}). –û–Ω/—Å–∞—Ä–∞–∞ —à–∞–ª–≥–∞–Ω–∞ —É—É.")
else:
    feature_cols = [f"osol_lag_{i}" for i in range(1, n_lag + 1)] + top_features
    X = grouped[feature_cols].fillna(0.0).values
    y = grouped["osol_count"].astype(float).values.reshape(-1, 1)

    # ---------------- Leakage-free scaling ----------------
    split_ratio = st.sidebar.slider("Train ratio", 0.5, 0.9, 0.8, 0.05)
    train_size = int(len(X) * split_ratio)

    X_train_raw, X_test_raw = X[:train_size], X[train_size:]
    y_train_raw, y_test_raw = y[:train_size], y[train_size:]

    scaler_X = MinMaxScaler()
    scaler_y = MinMaxScaler()
    X_train = scaler_X.fit_transform(X_train_raw)
    X_test  = scaler_X.transform(X_test_raw)
    y_train = scaler_y.fit_transform(y_train_raw).flatten()
    y_test  = scaler_y.transform(y_test_raw).flatten()

    estimators = [
        ("rf", RandomForestRegressor(n_estimators=120, random_state=seed)),
        ("ridge", Ridge()),
        ("dt", DecisionTreeRegressor(random_state=seed)),
    ]

    MODEL_LIST = [
        ("LinearRegression", LinearRegression()),
        ("Ridge", Ridge()),
        ("Lasso", Lasso()),
        ("DecisionTree", DecisionTreeRegressor(random_state=seed)),
        ("RandomForest", RandomForestRegressor(random_state=seed)),
        ("ExtraTrees", ExtraTreesRegressor(random_state=seed)),
        ("GradientBoosting", GradientBoostingRegressor(random_state=seed)),
        ("HistGB", HistGradientBoostingRegressor(random_state=seed)),
        ("AdaBoost", AdaBoostRegressor(random_state=seed)),
        ("KNeighbors", KNeighborsRegressor()),
        ("SVR", SVR()),
        ("MLPRegressor", MLPRegressor(hidden_layer_sizes=(64, 32), max_iter=800, random_state=seed)),
        ("ElasticNet", ElasticNet()),
        ("Stacking", StackingRegressor(estimators=estimators, final_estimator=LinearRegression(), cv=5)),
    ]
    if XGBRegressor is not None:
        MODEL_LIST.append(("XGBRegressor", XGBRegressor(verbosity=0, random_state=seed)))
    if CatBoostRegressor is not None:
        MODEL_LIST.append(("CatBoostRegressor", CatBoostRegressor(verbose=0, random_state=seed)))
    if LGBMRegressor is not None:
        MODEL_LIST.append(("LGBMRegressor", LGBMRegressor(random_state=seed)))

    progress_bar = st.progress(0, text="ML –º–æ–¥–µ–ª–∏–π–≥ —Å—É—Ä–≥–∞–∂ –±–∞–π–Ω–∞...")
    results = []
    y_preds = {}

    for i, (name, model) in enumerate(MODEL_LIST):
        try:
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            y_preds[name] = y_pred
            mae = mean_absolute_error(y_test, y_pred)
            mse = mean_squared_error(y_test, y_pred)
            rmse = float(np.sqrt(mse))
            r2 = r2_score(y_test, y_pred)
            results.append({"Model": name, "MAE": mae, "RMSE": rmse, "R2": r2})
        except Exception as e:
            results.append({"Model": name, "MAE": np.nan, "RMSE": np.nan, "R2": np.nan, "Error": str(e)})
        progress = min(int((i + 1) / len(MODEL_LIST) * 100), 100)
        progress_bar.progress(progress, text=f"{name} –¥—É—É—Å–ª–∞–∞")

    progress_bar.empty()
    st.success("–ë“Ø—Ö ML –º–æ–¥–µ–ª —Å—É—Ä–≥–∞–≥–¥–ª–∞–∞!")

    results_df = pd.DataFrame(results).sort_values("RMSE", na_position="last")
    st.dataframe(results_df, use_container_width=True)

    # Excel —Ç–∞—Ç–∞—Ö (–º–µ—Ç—Ä–∏–∫)
    with pd.ExcelWriter("model_metrics.xlsx", engine="xlsxwriter") as writer:
        results_df.to_excel(writer, index=False)
    with open("model_metrics.xlsx", "rb") as f:
        st.download_button(
            "–ú–æ–¥–µ–ª–∏–π–Ω –º–µ—Ç—Ä–∏–∫ Excel —Ç–∞—Ç–∞—Ö",
            data=f,
            file_name="model_metrics.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        )

    # –ò—Ä—ç—ç–¥“Ø–π–Ω –ø—Ä–æ–≥–Ω–æ–∑ helper
    def forecast_next(model, last_values, steps=12):
        preds = []
        seq = last_values.copy()
        for _ in range(steps):
            pred = model.predict([seq])[0]
            preds.append(pred)
            seq = np.roll(seq, -1)
            seq[-1] = pred
        return np.array(preds)

    forecast_steps = {"30 —Ö–æ–Ω–æ–≥": 1, "90 —Ö–æ–Ω–æ–≥": 3, "180 —Ö–æ–Ω–æ–≥": 6, "365 —Ö–æ–Ω–æ–≥": 12}
    model_forecasts = {}
    last_seq = scaler_X.transform(X[-1].reshape(1, -1)).flatten()

    for name, model in MODEL_LIST:
        if name not in y_preds:
            continue
        preds_dict = {}
        for k, s in forecast_steps.items():
            scaled_preds = forecast_next(model, last_seq, steps=s)
            inv_preds = scaler_y.inverse_transform(scaled_preds.reshape(-1, 1)).flatten()
            preds_dict[k] = inv_preds
        model_forecasts[name] = preds_dict

    # Test –¥—ç—ç—Ä—Ö –±–æ–¥–∏—Ç/—Ç–∞–∞–º–∞–≥
    test_dates = grouped["date"].iloc[-len(X_test):].values
    test_true = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()
    test_preds_df = pd.DataFrame({"date": test_dates, "real": test_true})
    for name in model_forecasts.keys():
        ypi = scaler_y.inverse_transform(np.array(y_preds[name]).reshape(-1, 1)).flatten()
        test_preds_df[name] = ypi

    # –ò—Ä—ç—ç–¥“Ø–π–Ω 12 —Å–∞—Ä—ã–Ω —Ç–∞–∞–º–∞–≥ (–º–æ–¥–µ–ª –±“Ø—Ä—ç—ç—Ä)
    future_dates = pd.date_range(start=grouped["date"].iloc[-1] + pd.offsets.MonthBegin(), periods=12, freq="MS")
    future_preds_df = pd.DataFrame({"date": future_dates})
    for name, model in MODEL_LIST:
        if name not in y_preds:
            continue
        scaled_preds = forecast_next(model, last_seq, steps=12)
        inv_preds = scaler_y.inverse_transform(scaled_preds.reshape(-1, 1)).flatten()
        future_preds_df[name] = inv_preds

    with pd.ExcelWriter("model_predictions.xlsx", engine="xlsxwriter") as writer:
        test_preds_df.to_excel(writer, index=False, sheet_name="Test_Predictions")
        future_preds_df.to_excel(writer, index=False, sheet_name="Future_Predictions")
    with open("model_predictions.xlsx", "rb") as f:
        st.download_button(
            "Test/Forecast –±“Ø—Ö –º–æ–¥–µ–ª–∏–π–Ω —Ç–∞–∞–º–∞–≥–ª–∞–ª—É—É–¥—ã–≥ Excel-—Ä —Ç–∞—Ç–∞—Ö",
            data=f,
            file_name="model_predictions.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        )

    st.subheader("Test –¥–∞—Ç–∞–Ω –¥—ç—ç—Ä—Ö –º–æ–¥–µ–ª –±“Ø—Ä–∏–π–Ω –±–æ–¥–∏—Ç –±–æ–ª–æ–Ω —Ç–∞–∞–º–∞–≥–ª–∞—Å–∞–Ω —É—Ç–≥—É—É–¥ (—Ç–æ–ª–≥–æ–π 10 –º”©—Ä):")
    st.dataframe(test_preds_df.head(10), use_container_width=True)

    st.subheader("–ò—Ä—ç—ç–¥“Ø–π–Ω 12 —Å–∞—Ä—ã–Ω –ø—Ä–æ–≥–Ω–æ–∑ (–º–æ–¥–µ–ª –±“Ø—Ä—ç—ç—Ä):")
    st.dataframe(future_preds_df, use_container_width=True)

    st.subheader("1 –∂–∏–ª–∏–π–Ω –ø—Ä–æ–≥–Ω–æ–∑ –≥—Ä–∞—Ñ–∏–∫ (–º–æ–¥–µ–ª —Å–æ–Ω–≥–æ–æ–¥ —Ö–∞—Ä–Ω–∞):")
    selected_model = st.selectbox("–ú–æ–¥–µ–ª—å —Å–æ–Ω–≥–æ—Ö:", list(model_forecasts.keys()))
    future = model_forecasts[selected_model]["365 —Ö–æ–Ω–æ–≥"]
    dates_future = pd.date_range(start=grouped["date"].iloc[-1] + pd.offsets.MonthBegin(), periods=12, freq="MS")
    future_df = pd.DataFrame({"date": dates_future, "forecast": future})
    fig = px.line(future_df, x="date", y="forecast", markers=True, title=f"{selected_model}-–∏–π–Ω –∏—Ä—ç—Ö 12 —Å–∞—Ä—ã–Ω –ø—Ä–æ–≥–Ω–æ–∑")
    st.plotly_chart(fig, use_container_width=True)

# -------------------------- Hotspot (DBSCAN) --------------------------

st.subheader("–ê–Ω—Ö–∞–∞—Ä–∞—Ö –≥–∞–∑—Ä—ã–Ω –±–∞–π—Ä—à–∏–ª (DBSCAN –∫–ª–∞—Å—Ç–µ—Ä—á–∏–ª—Å–∞–Ω hotspot)")
if lat_col and lon_col:
    # –°“Ø“Ø–ª–∏–π–Ω 12 —Å–∞—Ä—ã–Ω –û–õ–û–ù–¢–û–ô (–û—Å–æ–ª==1) –º”©—Ä“Ø“Ø–¥—ç—ç—Ä –∫–ª–∞—Å—Ç–µ—Ä—á–∏–ª–Ω–∞
    recent_df = df[(df["–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ"] >= (df["–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ"].max() - pd.DateOffset(months=12))) & (df["–û—Å–æ–ª"] == 1)].copy()
    recent_df = recent_df.dropna(subset=[lat_col, lon_col]).copy()
    coords = recent_df[[lat_col, lon_col]].to_numpy()
    if len(coords) >= 3:
        kms_per_radian = 6371.0088
        epsilon = 0.1 / kms_per_radian  # ‚âà100–º
        try:
            db = DBSCAN(eps=epsilon, min_samples=3, algorithm="ball_tree", metric="haversine").fit(np.radians(coords))
            recent_df["cluster"] = db.labels_
        except Exception:
            # –ó–∞—Ä–∏–º sklearn —Ö—É–≤–∏–ª–±–∞—Ä—Ç metric="haversine" –∞—Å—É—É–¥–∞–ª –≥–∞—Ä–≤–∞–ª euclidean-–¥ —à–∏–ª–∂–∏–Ω—ç (”©—Ä–≥”©—Ä”©–≥/—É—Ä—Ç—Ä–∞–≥–∏–π–Ω —Ö—ç–º–∂—ç—ç–Ω–¥ –æ–π—Ä–æ–ª—Ü–æ–æ–ª–æ–ª)
            db = DBSCAN(eps=0.001, min_samples=3, metric="euclidean").fit(coords)
            recent_df["cluster"] = db.labels_
    else:
        recent_df["cluster"] = -1

    hotspots = (
        recent_df[recent_df["cluster"] != -1]
        .groupby("cluster")
        .agg(
            n_osol=("–û—Å–æ–ª", "sum"),
            lat=(lat_col, "mean"),
            lon=(lon_col, "mean"),
        )
        .sort_values("n_osol", ascending=False)
        .reset_index()
    )

    map_center_lat = hotspots["lat"].mean() if not hotspots.empty else (recent_df[lat_col].mean() if len(recent_df) else 47)
    map_center_lon = hotspots["lon"].mean() if not hotspots.empty else (recent_df[lon_col].mean() if len(recent_df) else 106)
    m = folium.Map(location=[map_center_lat, map_center_lon], zoom_start=12 if not hotspots.empty else 6)

    for _, row in hotspots.iterrows():
        folium.Circle(
            location=[row["lat"], row["lon"]],
            radius=120 + row["n_osol"] * 1,
            color="orange",
            fill=True,
            fill_opacity=0.55,
            popup=folium.Popup(f"<b>Hotspot (–∫–ª–∞—Å—Ç–µ—Ä)</b><br>–û—Å–ª—ã–Ω —Ç–æ–æ: <b>{int(row['n_osol'])}</b>", max_width=350),
        ).add_to(m)

    st_folium(m, width=1920, height=700)
else:
    st.info("–ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã–Ω –±–∞–≥–∞–Ω—É—É–¥ (”®—Ä–≥”©—Ä”©–≥/–£—Ä—Ç—Ä–∞–≥ —ç—Å—Ö“Ø–ª lat/lon) –±–∞–π—Ö–≥“Ø–π —Ç—É–ª –≥–∞–∑—Ä—ã–Ω –∑—É—Ä–≥–∏–π–≥ –∞–ª–≥–∞—Å–ª–∞–∞.")

# -------------------------- 1. –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–π–Ω —à–∏–Ω–∂–∏–ª–≥—ç—ç --------------------------

st.header("1. –û—Å–æ–ª–¥ –Ω”©–ª”©”©–ª”©—Ö —Ö“Ø—á–∏–Ω –∑“Ø–π–ª—Å–∏–π–Ω —Ç–∞—Ä—Ö–∞–ª—Ç/–∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π–Ω —à–∏–Ω–∂–∏–ª–≥—ç—ç")
st.write("–î–æ–æ—Ä—Ö multiselect –¥—ç—ç—Ä—ç—ç—Å –∏—Ö–¥—ç—ç 15 —Ö—É–≤—å—Å–∞–≥—á —Å–æ–Ω–≥–æ–∂ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π–Ω –º–∞—Ç—Ä–∏—Ü —Ö–∞—Ä–Ω–∞.")

vars_for_corr = ["Year"]
vars_for_corr += [c for c in df.columns if c.startswith("–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ –∂–∏–ª ")][:10]
vars_for_corr += [c for c in (binary_cols + num_additional) if c in df.columns]
# –î–∞–≤—Ö–∞—Ä–¥–∞–ª –∞—Ä–∏–ª–≥–∞—Ö
vars_for_corr = list(dict.fromkeys(vars_for_corr))

if len(vars_for_corr) > 1:
    Xx = df[vars_for_corr].fillna(0.0).values
    yy = pd.to_numeric(df["–û—Å–æ–ª"], errors="coerce").fillna(0).values
    try:
        rf_cor = RandomForestRegressor(n_estimators=200, random_state=seed)
        rf_cor.fit(Xx, yy)
        importances_cor = rf_cor.feature_importances_
        indices_cor = np.argsort(importances_cor)[::-1]
        top_k_cor = min(15, len(vars_for_corr))
        default_cols = [vars_for_corr[i] for i in indices_cor[:top_k_cor]]
    except Exception:
        default_cols = vars_for_corr[: min(15, len(vars_for_corr))]
else:
    default_cols = vars_for_corr

selected_cols = st.multiselect(
    "–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–π–Ω –º–∞—Ç—Ä–∏—Ü–∞–¥ –æ—Ä—É—É–ª–∞—Ö —Ö—É–≤—å—Å–∞–≥—á–∏–¥:", vars_for_corr, default=default_cols, max_selections=15
)
if selected_cols:
    st.pyplot(plot_correlation_matrix(df, "Correlation Matrix", selected_cols))
else:
    st.warning("–°–æ–Ω–≥–æ—Ö —Ö—É–≤—å—Å–∞–≥—á–∏–¥—ã–≥ –æ—Ä—É—É–ª–Ω–∞ —É—É!")

# -------------------------- 2. –û—Å–ª—ã–Ω ”©—Å”©–ª—Ç–∏–π–Ω —Ç—Ä–µ–Ω–¥ --------------------------

st.header("2. –û—Å–ª—ã–Ω ”©—Å”©–ª—Ç–∏–π–Ω —Ç—Ä–µ–Ω–¥")
st.subheader("–ñ–∏–ª, —Å–∞—Ä –±“Ø—Ä—ç—ç—Ä –æ—Å–ª—ã–Ω —Ç–æ–æ–Ω—ã —Ç—Ä–µ–Ω–¥")

trend_data = (
    df[df["–û—Å–æ–ª"] == 1]
    .groupby(["Year", "Month"])
    .agg(osol_count=("–û—Å–æ–ª", "sum"))
    .reset_index()
)
trend_data["YearMonth"] = trend_data.apply(lambda x: f"{int(x['Year'])}-{int(x['Month']):02d}", axis=1)

available_years = sorted(trend_data["Year"].unique())
year_options = ["–ë“Ø–≥–¥"] + [str(y) for y in available_years]
selected_year = st.selectbox("–ñ–∏–ª —Å–æ–Ω–≥–æ—Ö:", year_options)

plot_df = trend_data if selected_year == "–ë“Ø–≥–¥" else trend_data[trend_data["Year"] == int(selected_year)].copy()
fig = px.line(plot_df, x="YearMonth", y="osol_count", markers=True, labels={"YearMonth": "–û–Ω-–°–∞—Ä", "osol_count": "–û—Å–ª—ã–Ω —Ç–æ–æ"}, title="")
fig.update_layout(
    xaxis_tickangle=45,
    hovermode="x unified",
    plot_bgcolor="white",
    yaxis=dict(title="–û—Å–ª—ã–Ω —Ç–æ–æ", rangemode="tozero"),
    xaxis=dict(title="–û–Ω-–°–∞—Ä"),
)
fig.update_traces(line=dict(width=3))
st.write("–î–æ–æ—Ä—Ö –≥—Ä–∞—Ñ–∏–∫—Ç –æ—Å–ª—ã–Ω —Ç–æ–æ–Ω—ã ”©”©—Ä—á–ª”©–ª—Ç–∏–π–≥ —Ö–∞—Ä—É—É–ª–∞–≤.")
st.plotly_chart(fig, use_container_width=True)

# -------------------------- 4. –ö–∞—Ç–µ–≥–æ—Ä–∏ —Ö–∞–º–∞–∞—Ä–∞–ª (Cram√©r‚Äôs V + Chi-square) --------------------------

st.header("4. –ö–∞—Ç–µ–≥–æ—Ä–∏ —Ö—É–≤—å—Å–∞–≥—á–¥—ã–Ω —Ö–æ–æ—Ä–æ–Ω–¥—ã–Ω —Ö–∞–º–∞–∞—Ä–∞–ª (Cram√©r‚Äôs V –±–æ–ª–æ–Ω Chi-square)")

low_card_cols = []
for c in df.columns:
    if c in ["–û—Å–æ–ª", "–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ", "Year", "Month", "Day"]:
        continue
    u = df[c].dropna().unique()
    if 2 <= len(u) <= 15:
        low_card_cols.append(c)

categorical_cols = sorted(list(set(binary_cols + low_card_cols)))
if len(categorical_cols) < 2:
    st.info("–ö–∞—Ç–µ–≥–æ—Ä–∏ –±–∞–≥–∞–Ω–∞ (2-–æ–æ—Å 15 —è–ª–≥–∞–∞—Ç–∞–π —É—Ç–≥–∞—Ç–∞–π) –æ–ª–¥—Å–æ–Ω–≥“Ø–π.")
else:
    var1 = st.selectbox("1-—Ä –∫–∞—Ç–µ–≥–æ—Ä–∏ —Ö—É–≤—å—Å–∞–≥—á:", categorical_cols)
    var2 = st.selectbox("2-—Ä –∫–∞—Ç–µ–≥–æ—Ä–∏ —Ö—É–≤—å—Å–∞–≥—á:", [c for c in categorical_cols if c != var1])

    table = pd.crosstab(df[var1], df[var2])
    chi2, p, dof, expected = chi2_contingency(table)

    # –•–æ—ë—Ä —Ö—É–≤–∏–ª–±–∞—Ä—ã–Ω V
    n = table.values.sum()
    r, k = table.shape
    cramers_v_naive = np.sqrt(chi2 / (n * (min(k, r) - 1))) if min(k, r) > 1 else np.nan
    cramers_v_bc = cramers_v_bias_corrected(table)

    st.subheader("1. Chi-square —Ç–µ—Å—Ç")
    st.write("p-value < 0.05 –±–æ–ª —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏–π–Ω —Ö—É–≤—å–¥ —Ö–∞–º–∞–∞—Ä–∞–ª—Ç–∞–π –≥—ç–∂ “Ø–∑–Ω—ç.")
    st.write(f"**Chi-square statistic:** {chi2:.3f}")
    st.write(f"**p-value:** {p:.4f}")
    if p < 0.05:
        st.success("p < 0.05 ‚Üí –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏–π–Ω —Ö—É–≤—å–¥ —Ö–∞–º–∞–∞—Ä–∞–ª—Ç–∞–π!")
    else:
        st.info("p ‚â• 0.05 ‚Üí –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏–π–Ω —Ö—É–≤—å–¥ —Ö–∞–º–∞–∞—Ä–∞–ª–≥“Ø–π.")

    use_bc = st.checkbox("Bias-corrected Cram√©r‚Äôs V (—Å–∞–Ω–∞–ª –±–æ–ª–≥–æ–∂ –±–∞–π–Ω–∞)", value=True)
    v_to_show = cramers_v_bc if use_bc else cramers_v_naive

    st.subheader("2. Cram√©r‚Äôs V")
    st.write("0-–¥ –æ–π—Ä—Ö–æ–Ω –±–æ–ª –±–∞—Ä–∞–≥ —Ö–∞–º–∞–∞—Ä–∞–ª–≥“Ø–π, 1-–¥ –æ–π—Ä –±–æ–ª —Ö“Ø—á—Ç—ç–π —Ö–∞–º–∞–∞—Ä–∞–ª—Ç–∞–π.")
    st.write(f"**Cram√©r‚Äôs V:** {v_to_show:.3f} (0=—Ö–∞–º–∞–∞—Ä–∞–ª–≥“Ø–π, 1=—Ö“Ø—á—Ç—ç–π —Ö–∞–º–∞–∞—Ä–∞–ª)")

    st.write("**Crosstab:**")
    st.dataframe(table, use_container_width=True)

# -------------------------- –¢”©—Å–ª–∏–π–Ω —Ç”©–≥—Å–≥”©–ª --------------------------

st.markdown(
    """
    ---
    **–¢–∞–π–ª–±–∞—Ä**  
    ‚Ä¢ –ó–æ—Ä–∏–ª—Ç–æ—Ç —Ö—É–≤—å—Å–∞–≥—á–∏–π–≥ Sidebar –¥—ç—ç—Ä—ç—ç—Å —Å–æ–Ω–≥–æ—Ö –±–æ–ª–æ–º–∂—Ç–æ–π (–ì—ç–º—Ç —Ö—ç—Ä—ç–≥/–ó”©—Ä—á–ª–∏–π–Ω —Ö—ç—Ä—ç–≥/–•–æ—Å–æ–ª—Å–æ–Ω).  
    ‚Ä¢ "Leakage-free" scaling: scaler-—É—É–¥—ã–≥ –∑”©–≤—Ö”©–Ω train –¥—ç—ç—Ä fit —Ö–∏–π–¥—ç–≥ —Ç—É–ª “Ø–Ω—ç–ª–≥—ç—ç –∏–ª“Ø“Ø –Ω–∞–π–¥–≤–∞—Ä—Ç–∞–π.  
    ‚Ä¢ ‚ö° –•—É—Ä–¥–∞–Ω –∞–∂–∏–ª–ª—É—É–ª–∞—Ö–∞–¥ 200 –º”©—Ä–∏–π–Ω —Å–∞–Ω–∞–º—Å–∞—Ä–≥“Ø–π –¥—ç–¥ –¥—ç—ç–∂ –∞–≤—á —Ç—É—Ä—à–∏–Ω–∞.  
    ‚Ä¢ –•—ç—Ä—ç–≤ XGBoost/LightGBM/CatBoost —Å—É—É–ª–≥–∞–≥–¥–∞–∞–≥“Ø–π –±–æ–ª —Å—É—É–ª–≥–∞–ª–≥“Ø–π–≥—ç—ç—Ä –±—É—Å–∞–¥ –º–æ–¥–µ–ª“Ø“Ø–¥ –∞–∂–∏–ª–ª–∞–Ω–∞.  
    """
)