# -*- coding: utf-8 -*-
# ============================================================
# –ó–∞–º —Ç—ç—ç–≤—Ä–∏–π–Ω –æ—Å–æ–ª ‚Äî Auto ML & Hotspot Dashboard (Streamlit)
# –•—É–≤–∏–ª–±–∞—Ä: 2025-08-17 ‚Äî Binary autodetect + robust column resolver + target UI
# –¢–∞–π–ª–±–∞—Ä:
#  - –•–∞–≤—Å–∞—Ä–≥–∞—Å–∞–Ω Excel ("–∫–æ–¥–ª–æ–≥–¥—Å–æ–Ω - Copy.xlsx")-—Ç–∞–π —à—É—É–¥ –∑–æ—Ö–∏—Ü–Ω–æ.
#  - Binary (0/1) –±“Ø—Ö –±–∞–≥–∞–Ω—ã–≥ –∞–≤—Ç–æ–º–∞—Ç–∞–∞—Ä –∏–ª—Ä“Ø“Ø–ª–∂, –º–æ–¥–µ–ª/–∫–æ—Ä—Ä–µ–ª—è—Ü/—Ö–æ—Ç—Å–ø–æ—Ç–æ–¥ –∞—à–∏–≥–ª–∞–Ω–∞.
#  - –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç –±–∞–≥–∞–Ω—É—É–¥ (”®—Ä–≥”©—Ä”©–≥/–£—Ä—Ç—Ä–∞–≥ —ç—Å–≤—ç–ª lat/lon) –±–∞–π–≤–∞–ª –≥–∞–∑—Ä—ã–Ω –∑—É—Ä–∞–≥ –∑—É—Ä–Ω–∞.
#  - –û–ª–æ–Ω ML –º–æ–¥–µ–ª —Å—É—Ä–≥–∞–ª—Ç, –º–µ—Ç—Ä–∏–∫/—Ç–∞–∞–º–∞–≥–ª–∞–ª—ã–≥ Excel –±–æ–ª–≥–æ–Ω —Ç–∞—Ç–∞—Ö –±–æ–ª–æ–º–∂—Ç–æ–π.
#  - "–û—Å–æ–ª" –±–∞–≥–∞–Ω–∞ –±–∞–π—Ö–≥“Ø–π —Ç–æ—Ö–∏–æ–ª–¥–æ–ª–¥ "–¢”©—Ä”©–ª"-”©”©—Å (–ì—ç–º—Ç —Ö—ç—Ä—ç–≥/–ó”©—Ä—á–ª–∏–π–Ω —Ö—ç—Ä—ç–≥) –∑–æ—Ä–∏–ª—Ç—ã–≥ “Ø“Ø—Å–≥—ç–Ω—ç.
# –ì“Ø–π—Ü—ç—Ç–≥—ç—Ö: streamlit run osol_auto_streamlit.py
# ============================================================

from __future__ import annotations
import io
import numpy as np
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

from pathlib import Path

from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet


from sklearn.ensemble import (
    RandomForestRegressor,
    GradientBoostingRegressor,
    AdaBoostRegressor,
    ExtraTreesRegressor,
    StackingRegressor,
    HistGradientBoostingRegressor,
)
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.cluster import DBSCAN

# –ì—É—Ä–∞–≤–¥–∞–≥—á gradient boosting —Å–∞–Ω–≥—É—É–¥ (optional)
try:
    from xgboost import XGBRegressor  # type: ignore
except Exception:
    XGBRegressor = None  # pragma: no cover
try:
    from lightgbm import LGBMRegressor  # type: ignore
except Exception:
    LGBMRegressor = None  # pragma: no cover
try:
    from catboost import CatBoostRegressor  # type: ignore
except Exception:
    CatBoostRegressor = None  # pragma: no cover

from scipy.stats import chi2_contingency
import folium
from streamlit_folium import st_folium

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

def build_lstm_model(input_shape, units=64):
    model = Sequential()
    model.add(LSTM(units, activation="tanh", input_shape=input_shape))
    model.add(Dense(1))
    model.compile(optimizer="adam", loss="mse")
    return model
# -------------------------- UI setup --------------------------
st.set_page_config(page_title="–û—Å–æ–ª ‚Äî Auto ML & Hotspot (auto-binary)", layout="wide")

st.title=("–°.–¶–æ–ª–º–æ–Ω, –ê.–¢–∞–º–∏—Ä –Ω–∞—Ä—ã–Ω —Ö–∞—Ä —Ü—ç–≥–∏–π–Ω —Å—É–¥–∞–ª–≥–∞–∞ 2025-08-18")

# -------------------------- –¢—É—Å–ª–∞—Ö —Ñ—É–Ω–∫—Ü—É—É–¥ --------------------------

def _canon(s: str) -> str:
    return "".join(str(s).lower().split()) if isinstance(s, str) else str(s)

def resolve_col(df: pd.DataFrame, candidates) -> str | None:
    """–ù—ç—Ä—à–ª–∏–π–≥ robust –±–∞–π–¥–ª–∞–∞—Ä –æ–ª–Ω–æ (whitespace/—Ç–æ–º-–∂–∏–∂–∏–≥ “Ø–ª —Ç–æ–æ–Ω–æ)."""
    lut = {_canon(c): c for c in df.columns}
    for name in candidates:
        key = _canon(name)
        if key in lut:
            return lut[key]
    return None

def is_binary_series(s: pd.Series) -> bool:
    vals = pd.to_numeric(s.dropna(), errors="coerce").dropna().unique()
    if len(vals) == 0:
        return False
    return set(np.unique(vals)).issubset({0, 1})

def plot_correlation_matrix(df, title, columns):
    n_unique = df[columns].nunique()
    if all((n_unique == 2) | (n_unique == 1)):
        st.warning("–°–æ–Ω–≥–æ—Å–æ–Ω –±–∞–≥–∞–Ω—É—É–¥ –Ω—å one-hot (0/1) —Ö—ç–ª–±—ç—Ä—Ç—ç–π –±–∞–π–Ω–∞. –ò–π–º —Ç–æ—Ö–∏–æ–ª–¥–æ–ª–¥ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏ -1~1 —Ç—É–π–ª—Ä—É—É–≥–∞–∞ —Ö—ç–ª–±–∏–π–∂ —Ö–∞—Ä–∞–≥–¥–∞–∂ –±–æ–ª–Ω–æ.")
    df_encoded = df[columns].copy()
    for col in df_encoded.columns:
        if df_encoded[col].dtype == "object":
            df_encoded[col] = pd.Categorical(df_encoded[col]).codes
    corr_matrix = df_encoded.corr()
    corr_matrix = corr_matrix.iloc[::-1]
    fig, ax = plt.subplots(figsize=(max(8, 1.5*len(columns)), max(6, 1.2*len(columns))))
    sns.heatmap(corr_matrix, annot=True, cmap="coolwarm", vmin=-1, vmax=1, center=0, ax=ax, fmt=".3f")
    plt.title(title)
    plt.tight_layout()
    return fig

# -------------------------- ”®–≥”©–≥–¥”©–ª –∞—á–∞–∞–ª–∞–ª—Ç --------------------------
from pathlib import Path
import pandas as pd
import streamlit as st

# 1. –≠–Ω–¥ widget-—ç—ç –≥–∞–¥–Ω–∞ –≥–∞—Ä–≥–∞–∂ ”©–≥–Ω”©
uploaded_file = st.sidebar.file_uploader("Excel —Ñ–∞–π–ª –æ—Ä—É—É–ª–∞—Ö (.xlsx)", type=["xlsx"])

# 2. –ö—ç—à—Ç—ç–π –∑”©–≤—Ö”©–Ω –¥–∞—Ç–∞ —É–Ω—à–∏—Ö —Ñ—É–Ω–∫—Ü
@st.cache_data(show_spinner=True)
def load_data(file=None, default_path: str = "–∫–æ–¥–ª–æ–≥–¥—Å–æ–Ω.xlsx"):
    """
    Excel –¥–∞—Ç–∞ —É–Ω—à–∏—Ö —Ñ—É–Ω–∫—Ü (widget –¥–æ—Ç–æ—Ä –±–∏—à).
    """
    if file is not None:
        df = pd.read_excel(file)
    else:
        local = Path("–∫–æ–¥–ª–æ–≥–¥—Å–æ–Ω.xlsx")
        if local.exists():
            df = pd.read_excel(local)
        else:
            df = pd.read_excel(default_path)

    # –ù—ç—Ä—à–∏–ª —Ü—ç–≤—ç—Ä–ª—ç–≥—ç—ç
    df.columns = [c.strip() if isinstance(c, str) else c for c in df.columns]

    # –û–≥–Ω–æ–æ –±–∞–≥–∞–Ω–∞ —Ö–∞–π—Ö
    recv_col = resolve_col(df, ["–•“Ø–ª—ç—ç–Ω –∞–≤—Å–∞–Ω", "–•“Ø–ª—ç—ç–Ω –∞–≤—Å–∞–Ω ", "–û–≥–Ω–æ–æ", "–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ", 
                                "–û—Å–æ–ª –æ–≥–Ω–æ–æ", "–û—Å–ª—ã–Ω –æ–≥–Ω–æ–æ", "Date"]) 
    if recv_col is None:
        st.error("–û–≥–Ω–æ–æ–Ω—ã –±–∞–≥–∞–Ω–∞ –æ–ª–¥—Å–æ–Ω–≥“Ø–π. –ñ–∏—à—ç—ç –Ω—å: '–•“Ø–ª—ç—ç–Ω –∞–≤—Å–∞–Ω'.")
        st.stop()

    # '–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ' “Ø“Ø—Å–≥—ç—Ö
    df["–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ"] = pd.to_datetime(df[recv_col], errors="coerce")
    df["Year"]  = df["–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ"].dt.year
    df["Month"] = df["–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ"].dt.month
    df["Day"]   = df["–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ"].dt.day_name()


    # –û–Ω –∂–∏–ª“Ø“Ø–¥–∏–π–Ω one-hot
    years = sorted(df["Year"].dropna().unique().tolist())
    for y in years:
        df[f"–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ –∂–∏–ª {int(y)}"] = (df["Year"] == int(y)).astype(int)
    if len(years) > 0:
        df["–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ –∂–∏–ª (min-max)"] = df["Year"].between(min(years), max(years)).astype(int)



    # –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç –Ω—ç—Ä—à–∏–ª
    lat_col = resolve_col(df, ["”®—Ä–≥”©—Ä”©–≥", "lat", "latitude"])
    lon_col = resolve_col(df, ["–£—Ä—Ç—Ä–∞–≥", "lon", "longitude"])

    # –ê–≤—Ç–æ–º–∞—Ç–∞–∞—Ä binary –±–∞–≥–∞–Ω—É—É–¥—ã–≥ –∏–ª—Ä“Ø“Ø–ª—ç—Ö
    exclude = {"–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ", "Year", "Month", "Day", "–¥/–¥", "–•–æ—Ä–æ–æ-–°—É–º", "–ê–π–º–∞–≥-–î“Ø“Ø—Ä—ç–≥"}
    if lat_col: exclude.add(lat_col)
    if lon_col: exclude.add(lon_col)
    binary_cols = [c for c in df.columns if c not in exclude and is_binary_series(df[c])]

    # –ù—ç–º—ç–ª—Ç —Ç–æ–æ–Ω candidate-—É—É–¥
    numeric_candidates = []
    if "–ê–≤—Ç–æ –∑–∞–º - –ó–æ—Ä—á–∏—Ö —Ö—ç—Å–≥–∏–π–Ω ”©—Ä–≥”©–Ω" in df.columns:
        numeric_candidates.append("–ê–≤—Ç–æ –∑–∞–º - –ó–æ—Ä—á–∏—Ö —Ö—ç—Å–≥–∏–π–Ω ”©—Ä–≥”©–Ω")

    # –î“Ø“Ø—Ä—ç–≥/–ê–π–º–∞–≥ fallback
    if "–î“Ø“Ø—Ä—ç–≥" not in df.columns:
        df["–î“Ø“Ø—Ä—ç–≥"] = 0
    if "–ê–π–º–∞–≥" not in df.columns:
        df["–ê–π–º–∞–≥"] = 0

    meta = {
        "lat_col": lat_col,
        "lon_col": lon_col,
        "binary_cols": binary_cols,
        "numeric_candidates": numeric_candidates,
        "years": years
    }
    return df, meta

# -------------------------- –ê—á–∞–∞–ª–∂ —ç—Ö–ª—ç—Ö --------------------------

df, meta = load_data(uploaded_file)
lat_col, lon_col = meta["lat_col"], meta["lon_col"]
binary_cols = meta["binary_cols"]
num_additional = meta["numeric_candidates"]
years = meta["years"]

# -------------------------- Target —Ç–æ—Ö–∏—Ä–≥–æ–æ --------------------------

st.sidebar.markdown("### üéØ –ó–æ—Ä–∏–ª—Ç–æ—Ç —Ç–æ–¥–æ—Ä—Ö–æ–π–ª–æ–ª—Ç (–û—Å–æ–ª)")
target_mode = st.sidebar.radio(
    "–û—Å–æ–ª –≥—ç–∂ —Ç–æ–æ—Ü–æ—Ö –∞–Ω–≥–∏–ª–ª—ã–≥ —Å–æ–Ω–≥–æ–Ω–æ —É—É:",
    ("–•–æ—ë—É–ª–∞–Ω–≥ 1 –≥—ç–∂ —Ç–æ–æ—Ü", "–ó”©–≤—Ö”©–Ω –ì—ç–º—Ç —Ö—ç—Ä—ç–≥", "–ó”©–≤—Ö”©–Ω –ó”©—Ä—á–ª–∏–π–Ω —Ö—ç—Ä—ç–≥"),
)

# '–û—Å–æ–ª' target “Ø“Ø—Å–≥—ç—Ö ('–¢”©—Ä”©–ª' –±–∞–≥–∞–Ω–∞–∞—Å)
torol_col = resolve_col(df, ["–¢”©—Ä”©–ª"])  # —Ç–∞–Ω—ã –¥–∞—Ç–∞–Ω–¥ –±–∏–π
if torol_col is None:
    st.error("`–¢”©—Ä”©–ª` –±–∞–≥–∞–Ω–∞ –æ–ª–¥—Å–æ–Ω–≥“Ø–π. Target “Ø“Ø—Å–≥—ç—Ö –±–æ–ª–æ–º–∂–≥“Ø–π –±–∞–π–Ω–∞.")
    st.stop()

if target_mode == "–•–æ—ë—É–ª–∞–Ω–≥ 1 –≥—ç–∂ —Ç–æ–æ—Ü":
    df["–û—Å–æ–ª"] = df[torol_col].isin(["–ì—ç–º—Ç —Ö—ç—Ä—ç–≥", "–ó”©—Ä—á–ª–∏–π–Ω —Ö—ç—Ä—ç–≥"]).astype(int)
elif target_mode == "–ó”©–≤—Ö”©–Ω –ì—ç–º—Ç —Ö—ç—Ä—ç–≥":
    df["–û—Å–æ–ª"] = (df[torol_col] == "–ì—ç–º—Ç —Ö—ç—Ä—ç–≥").astype(int)
else:  # –ó”©–≤—Ö”©–Ω –ó”©—Ä—á–ª–∏–π–Ω —Ö—ç—Ä—ç–≥
    df["–û—Å–æ–ª"] = (df[torol_col] == "–ó”©—Ä—á–ª–∏–π–Ω —Ö—ç—Ä—ç–≥").astype(int)

# -------------------------- 5. –ò—Ä—ç—ç–¥“Ø–π–Ω –æ—Å–ª—ã–Ω —Ç–∞–∞–º–∞–≥–ª–∞–ª --------------------------

st.header("5. –ò—Ä—ç—ç–¥“Ø–π–Ω –æ—Å–ª—ã–Ω —Ç–∞–∞–º–∞–≥–ª–∞–ª (–û–ª–æ–Ω ML/DL –∑–∞–≥–≤–∞—Ä)")
st.caption("Binary (0/1) –±–∞–≥–∞–Ω—É—É–¥—ã–≥ –∞–≤—Ç–æ–º–∞—Ç–∞–∞—Ä –∏–ª—Ä“Ø“Ø–ª–∂, –∑–∞–≥–≤–∞—Ä—Ç –∞—à–∏–≥–ª–∞—Å–∞–Ω.")

# Feature pool: '–û—Å–æ–ª'-–æ–æ—Å –±—É—Å–∞–¥ binary + –Ω—ç–º—ç–ª—Ç —Ç–æ–æ–Ω
feature_pool = [c for c in (binary_cols + num_additional) if c != "–û—Å–æ–ª"]
if len(feature_pool) == 0:
    st.error("Binary (0/1) —Ö—ç–ª–±—ç—Ä–∏–π–Ω –±–∞–≥–∞–Ω–∞ –æ–ª–¥—Å–æ–Ω–≥“Ø–π. Excel-—ç—ç —à–∞–ª–≥–∞–Ω–∞ —É—É.")
    st.stop()

# Target/Features
y_all = pd.to_numeric(df["–û—Å–æ–ª"], errors="coerce").fillna(0).values
X_all = df[feature_pool].fillna(0.0).values

# Top features via RandomForest
# Top features via RandomForest + SHAP
# Top features via RandomForest + SHAP
try:
    import shap
    rf_global = RandomForestRegressor(n_estimators=300, random_state=42)
    rf_global.fit(X_all, y_all)

    importances = rf_global.feature_importances_
    indices = np.argsort(importances)[::-1]
    top_k = min(14, len(feature_pool))
    top_features = [feature_pool[i] for i in indices[:top_k]]

    st.caption("RandomForest-–∞–∞—Ä —Å–æ–Ω–≥–æ—Å–æ–Ω –Ω”©–ª”©”© –∏—Ö—Ç—ç–π —à–∏–Ω–∂“Ø“Ø–¥ (top importance):")
    st.write(top_features)

    # SHAP plot
    explainer = shap.TreeExplainer(rf_global)
    shap_values = explainer.shap_values(X_all)
    st.subheader("üîé SHAP value —à–∏–Ω–∂–∏–ª–≥—ç—ç (global importance)")
    shap.summary_plot(shap_values, X_all, feature_names=feature_pool, plot_type="bar", show=False)
    st.pyplot(plt.gcf())  # ‚Üê –∂–∏–Ω—Ö—ç–Ω—ç SHAP –≥—Ä–∞—Ñ–∏–∫–∏–π–≥ —Ö–∞—Ä—É—É–ª–Ω–∞


    # Rare feature filter
    rare_threshold = 0.01  # <1% –º”©—Ä”©–Ω–¥ –ª 1 –≥—ç—Å—ç–Ω —É—Ç–≥–∞—Ç–∞–π –±–æ–ª 'rare'
    rare_features = []
    for col in feature_pool:
        freq = df[col].mean() if col in df else 0
        if freq < rare_threshold:
            rare_features.append(col)

    if rare_features:
        st.warning(f"‚ö†Ô∏è –î–æ–æ—Ä—Ö –±–∞–≥–∞–Ω—É—É–¥ –º–∞—à —Ü”©”©–Ω —Ç–æ—Ö–∏–æ–ª–¥–æ–ª—Ç–æ–π —Ç—É–ª importance —Ö—ç—Ç ”©–Ω–¥”©—Ä –≥–∞—Ä—á –º–∞–≥–∞–¥–≥“Ø–π: {rare_features}")

except Exception as e:
    st.warning(f"Top features/SHAP —Ç–æ–æ—Ü–æ—Ö–æ–¥ –∞–ª–¥–∞–∞ –≥–∞—Ä–ª–∞–∞: {e}")
    top_features = feature_pool[:min(14, len(feature_pool))]

# –°–∞—Ä –±“Ø—Ä–∏–π–Ω –∞–≥—Ä–µ–≥–∞—Ç (target=–û—Å–æ–ª==1 –¥–∞–≤—Ç–∞–º–∂)
monthly_target = (
    df[df["–û—Å–æ–ª"] == 1]
    .groupby(["Year", "Month"])
    .agg(osol_count=("–û—Å–æ–ª", "sum"))
    .reset_index()
)
monthly_target["date"] = pd.to_datetime(monthly_target[["Year", "Month"]].assign(DAY=1))
monthly_features = df.groupby(["Year", "Month"])[top_features].sum().reset_index()

grouped = pd.merge(monthly_target, monthly_features, on=["Year", "Month"], how="left").sort_values("date").reset_index(drop=True)

# Lag “Ø“Ø—Å–≥—ç—Ö
n_lag = st.sidebar.slider("–°–∞—Ä–Ω—ã –ª–∞–≥ —Ü–æ–Ω—Ö (n_lag)", min_value=6, max_value=18, value=12, step=1)
for i in range(1, n_lag + 1):
    grouped[f"osol_lag_{i}"] = grouped["osol_count"].shift(i)

grouped = grouped.dropna().reset_index(drop=True)

if grouped.empty or len(grouped) < 10:
    st.warning(f"–°—É—Ä–≥–∞–ª—Ç —Ö–∏–π—Ö—ç–¥ —Ö–∞–Ω–≥–∞–ª—Ç—Ç–∞–π —Å–∞—Ä —Ç—É—Ç–º—ã–Ω ”©–≥”©–≥–¥”©–ª –∞–ª–≥–∞ (lag={n_lag}). –û–Ω/—Å–∞—Ä–∞–∞ —à–∞–ª–≥–∞–Ω–∞ —É—É.")
else:
    feature_cols = [f"osol_lag_{i}" for i in range(1, n_lag + 1)] + top_features
    X = grouped[feature_cols].fillna(0.0).values
    y = grouped["osol_count"].values.reshape(-1, 1)

    # Scale
    split_ratio = st.sidebar.slider("Train ratio", 0.5, 0.9, 0.8, 0.05)
    train_size = int(len(X) * split_ratio)

    X_train, y_train = X[:train_size], y[:train_size].reshape(-1, 1)
    X_test, y_test = X[train_size:], y[train_size:].reshape(-1, 1)

    # Scale –∑”©–≤ –¥–∞—Ä–∞–∞–ª–ª–∞–∞—Ä
    scaler_X = MinMaxScaler()
    X_train = scaler_X.fit_transform(X_train)
    X_test = scaler_X.transform(X_test)

    scaler_y = MinMaxScaler()
    y_train = scaler_y.fit_transform(y_train).flatten()
    y_test = scaler_y.transform(y_test).flatten()

    estimators = [
        ("rf", RandomForestRegressor(n_estimators=120, random_state=42)),
        ("ridge", Ridge()),
        ("dt", DecisionTreeRegressor(random_state=42)),
    ]

    MODEL_LIST = [
        ("LinearRegression", LinearRegression()),
        ("Ridge", Ridge()),
        ("Lasso", Lasso()),
        ("DecisionTree", DecisionTreeRegressor(random_state=42)),
        ("RandomForest", RandomForestRegressor(random_state=42)),
        ("ExtraTrees", ExtraTreesRegressor(random_state=42)),
        ("GradientBoosting", GradientBoostingRegressor(random_state=42)),
        ("HistGB", HistGradientBoostingRegressor(random_state=42)),
        ("AdaBoost", AdaBoostRegressor(random_state=42)),
        ("KNeighbors", KNeighborsRegressor()),
        ("SVR", SVR()),
        ("MLPRegressor", MLPRegressor(hidden_layer_sizes=(64, 32), max_iter=800, random_state=42)),
        ("ElasticNet", ElasticNet()),
        ("Stacking", StackingRegressor(estimators=estimators, final_estimator=LinearRegression(), cv=5)),
        ("LSTM", build_lstm_model)
    ]
    if XGBRegressor is not None:
        MODEL_LIST.append(("XGBRegressor",
            XGBRegressor(tree_method="hist", predictor="cpu_predictor", random_state=42)))
    if CatBoostRegressor is not None:
        MODEL_LIST.append(("CatBoostRegressor",
            CatBoostRegressor(task_type="CPU", random_state=42, verbose=0)))
    if LGBMRegressor is not None:
        MODEL_LIST.append(("LGBMRegressor",
            LGBMRegressor(device="cpu", random_state=42)))


    # ====================================================
    # üÜï VotingRegressor + StackingEnsemble –Ω—ç–º—Å—ç–Ω —Ö—ç—Å—ç–≥
    # ====================================================
    from sklearn.ensemble import VotingRegressor, StackingRegressor

    voting_estimators = []
    if XGBRegressor is not None:
        voting_estimators.append(("xgb", XGBRegressor(tree_method="hist", predictor="cpu_predictor", random_state=42)))
    if LGBMRegressor is not None:
        voting_estimators.append(("lgbm", LGBMRegressor(device="cpu", random_state=42)))
    if CatBoostRegressor is not None:
        voting_estimators.append(("cat", CatBoostRegressor(task_type="CPU", random_state=42, verbose=0)))
    voting_estimators.append(("rf", RandomForestRegressor(n_estimators=200, random_state=42)))
    voting_estimators.append(("gb", GradientBoostingRegressor(random_state=42)))

    if len(voting_estimators) > 1:
        MODEL_LIST.append(("VotingRegressor", VotingRegressor(estimators=voting_estimators)))

    stacking_estimators = [("rf", RandomForestRegressor(random_state=42))]
    if XGBRegressor is not None:
        stacking_estimators.append(("xgb", XGBRegressor(tree_method="hist", predictor="cpu_predictor", random_state=42)))
    if LGBMRegressor is not None:
        stacking_estimators.append(("lgbm", LGBMRegressor(device="cpu", random_state=42)))
    if CatBoostRegressor is not None:
        stacking_estimators.append(("cat", CatBoostRegressor(task_type="CPU", verbose=0, random_state=42)))


    MODEL_LIST.append((
        "StackingEnsemble",
        StackingRegressor(estimators=stacking_estimators, final_estimator=LinearRegression(), cv=5)
    ))


    progress_bar = st.progress(0, text="ML –º–æ–¥–µ–ª–∏–π–≥ —Å—É—Ä–≥–∞–∂ –±–∞–π–Ω–∞...")
    results = []
    y_preds = {}

    for i, (name, model) in enumerate(MODEL_LIST):
        try:
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            y_preds[name] = y_pred
            mae = mean_absolute_error(y_test, y_pred)
            mse = mean_squared_error(y_test, y_pred)
            rmse = float(np.sqrt(mse))
            r2 = r2_score(y_test, y_pred)
            results.append({"Model": name, "MAE": mae, "RMSE": rmse, "R2": r2})
        except Exception as e:
            results.append({"Model": name, "MAE": np.nan, "RMSE": np.nan, "R2": np.nan, "Error": str(e)})
        progress = min(int((i + 1) / len(MODEL_LIST) * 100), 100)
        progress_bar.progress(progress, text=f"{name} –¥—É—É—Å–ª–∞–∞")

    progress_bar.empty()
    st.success("–ë“Ø—Ö ML –º–æ–¥–µ–ª —Å—É—Ä–≥–∞–≥–¥–ª–∞–∞!")

    results_df = pd.DataFrame(results).sort_values("RMSE", na_position="last")
    st.dataframe(results_df, use_container_width=True)

    # Excel —Ç–∞—Ç–∞—Ö (–º–µ—Ç—Ä–∏–∫)
    with pd.ExcelWriter("model_metrics.xlsx", engine="xlsxwriter") as writer:
        results_df.to_excel(writer, index=False)
    with open("model_metrics.xlsx", "rb") as f:
        st.download_button(
            "–ú–æ–¥–µ–ª–∏–π–Ω –º–µ—Ç—Ä–∏–∫ Excel —Ç–∞—Ç–∞—Ö",
            data=f,
            file_name="model_metrics.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        )

    # –ò—Ä—ç—ç–¥“Ø–π–Ω –ø—Ä–æ–≥–Ω–æ–∑ helper
    # –ò—Ä—ç—ç–¥“Ø–π–Ω –ø—Ä–æ–≥–Ω–æ–∑ helper
    def forecast_next(model, last_values, steps=12):
        preds = []
        seq = last_values.copy()
        for _ in range(steps):
            pred = model.predict([seq])[0]
            preds.append(pred)
            seq = np.roll(seq, -1)
            seq[-1] = pred
        return np.array(preds)
    
    def forecast_next_daily(model, last_values, steps=30):
        preds = []
        seq = np.array(last_values).reshape(1, -1)  # ML model –æ—Ä–æ–ª—Ç –∑”©–≤ —Ö—ç–ª–±—ç—Ä—Ç—ç–π –±–æ–ª–≥–æ–Ω–æ
        for _ in range(steps):
            pred = model.predict(seq)[0]
            preds.append(pred)
            seq = np.roll(seq, -1, axis=1)
            seq[0, -1] = pred
        return np.array(preds)
    def forecast_next_daily_lstm(model, last_values, steps=30, window=12):
        preds = []
        seq = np.array(last_values[-window:]).reshape(1, window, 1)
        for _ in range(steps):
            pred = model.predict(seq, verbose=0)[0][0]
            preds.append(pred)
            # –¥–∞—Ä–∞–∞–≥–∏–π–Ω –∞–ª—Ö–∞–º–¥ input-–≥ update —Ö–∏–π–Ω—ç
            seq = np.roll(seq, -1, axis=1)
            seq[0, -1, 0] = pred
        return np.array(preds)

    model_forecasts = {}
    # üõ† –ê–ª–¥–∞–∞–≥ –∑–∞—Å—Å–∞–Ω: X_scaled –±–∏—à X_test –∞—à–∏–≥–ª–∞–≤
    last_seq = X_test[-1]
    forecast_steps = {"7 —Ö–æ–Ω–æ–≥": 7, "14 —Ö–æ–Ω–æ–≥": 14, "30 —Ö–æ–Ω–æ–≥": 30, "90 —Ö–æ–Ω–æ–≥": 90, "180 —Ö–æ–Ω–æ–≥": 180, "365 —Ö–æ–Ω–æ–≥": 365}
    for name, model in MODEL_LIST:
        if name not in y_preds:
            continue
        preds_dict = {}
        for k, s in forecast_steps.items():
            if name == "LSTM":
                scaled_preds = forecast_next_daily_lstm(model, last_seq, steps=s, window=n_lag)
            else:
                scaled_preds = forecast_next(model, last_seq, steps=s)
            inv_preds = scaler_y.inverse_transform(scaled_preds.reshape(-1, 1)).flatten()
            preds_dict[k] = inv_preds
        model_forecasts[name] = preds_dict

    # Test –¥—ç—ç—Ä—Ö –±–æ–¥–∏—Ç/—Ç–∞–∞–º–∞–≥
    test_dates = grouped["date"].iloc[-len(X_test):].values
    test_true = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()
    test_preds_df = pd.DataFrame({"date": test_dates, "real": test_true})
    for name in model_forecasts.keys():
        ypi = scaler_y.inverse_transform(np.array(y_preds[name]).reshape(-1, 1)).flatten()
        test_preds_df[name] = ypi

    # –ò—Ä—ç—ç–¥“Ø–π–Ω 12 —Å–∞—Ä—ã–Ω —Ç–∞–∞–º–∞–≥ (–º–æ–¥–µ–ª –±“Ø—Ä—ç—ç—Ä)
    future_dates = pd.date_range(start=grouped["date"].iloc[-1] + pd.offsets.MonthBegin(), periods=12, freq="MS")
    future_preds_df = pd.DataFrame({"date": future_dates})
    for name, model in MODEL_LIST:
        if name not in y_preds:
            continue
        scaled_preds = forecast_next(model, last_seq, steps=12)
        inv_preds = scaler_y.inverse_transform(scaled_preds.reshape(-1, 1)).flatten()
        future_preds_df[name] = inv_preds

    with pd.ExcelWriter("model_predictions.xlsx", engine="xlsxwriter") as writer:
        test_preds_df.to_excel(writer, index=False, sheet_name="Test_Predictions")
        future_preds_df.to_excel(writer, index=False, sheet_name="Future_Predictions")
    with open("model_predictions.xlsx", "rb") as f:
        st.download_button(
            "Test/Forecast –±“Ø—Ö –º–æ–¥–µ–ª–∏–π–Ω —Ç–∞–∞–º–∞–≥–ª–∞–ª—É—É–¥—ã–≥ Excel-—Ä —Ç–∞—Ç–∞—Ö",
            data=f,
            file_name="model_predictions.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        )

    st.subheader("Test –¥–∞—Ç–∞–Ω –¥—ç—ç—Ä—Ö –º–æ–¥–µ–ª –±“Ø—Ä–∏–π–Ω –±–æ–¥–∏—Ç –±–æ–ª–æ–Ω —Ç–∞–∞–º–∞–≥–ª–∞—Å–∞–Ω —É—Ç–≥—É—É–¥ (—Ç–æ–ª–≥–æ–π 10 –º”©—Ä):")
    st.dataframe(test_preds_df.head(10), use_container_width=True)

    st.subheader("–ò—Ä—ç—ç–¥“Ø–π–Ω 12 —Å–∞—Ä—ã–Ω –ø—Ä–æ–≥–Ω–æ–∑ (–º–æ–¥–µ–ª –±“Ø—Ä—ç—ç—Ä):")
    st.dataframe(future_preds_df, use_container_width=True)

    st.subheader("–•–æ—Ä–∏–∑–æ–Ω—Ç —Å–æ–Ω–≥–æ–∂ –≥—Ä–∞—Ñ–∏–∫–∞–∞—Ä —Ö–∞—Ä–∞—Ö:")


# –ú–æ–¥–µ–ª—å —Å–æ–Ω–≥–æ–ª—Ç (”©–¥”©—Ä–∏–π–Ω pipeline –∞–∂–∏–ª–ª–∞–∂ –±–∞–π–≥–∞–∞ —ç—Å—ç—Ö—ç—ç—Å —à–∞–ª—Ç–≥–∞–∞–ª–∞–Ω)
model_options = list(y_preds.keys()) if 'y_preds' in locals() and len(y_preds) > 0 else list(model_forecasts.keys())
selected_model = st.selectbox("–ú–æ–¥–µ–ª—å —Å–æ–Ω–≥–æ—Ö:", model_options)
selected_h = st.selectbox("–•–æ—Ä–∏–∑–æ–Ω—Ç:", list(forecast_steps.keys()), index=2)

# –û–≥–Ω–æ–æ–Ω—ã –Ω—è–≥—Ç—Ä–∞–ª —à–∏–ª–∂–ª“Ø“Ø–ª—Ç
gran = st.radio("–î—ç–ª–≥—ç—Ü–ª—ç—Ö –æ–≥–Ω–æ–æ–Ω—ã –Ω—è–≥—Ç—Ä–∞–ª ‚Ññ1:", ["”®–¥”©—Ä", "–°–∞—Ä"], index=0, horizontal=True)
last_date = grouped["date"].iloc[-1]
last_seq = X_test[-1]

last_lags_raw = grouped[feature_cols].iloc[-1].values

steps = forecast_steps[selected_h]
if 'forecast_next_daily' in globals():
    # ”®–¥”©—Ä —Ç—É—Ç–º—ã–Ω —Ç–∞–∞–º–∞–≥–ª–∞–ª—Ç–∞–π –≥–æ—Ä–∏–º
    plot_future = forecast_next_daily(dict(MODEL_LIST)[selected_model], last_seq, steps)
    plot_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=steps, freq="D")
    future_df = pd.DataFrame({"date": plot_dates, "forecast": plot_future})
    if gran == "–°–∞—Ä":
        future_df = future_df.set_index("date").resample("MS").sum().reset_index()
    title = f"{selected_model} ‚Äî –∏—Ä—ç—Ö {steps} —Ö–æ–Ω–æ–≥–∏–π–Ω –ø—Ä–æ–≥–Ω–æ–∑ ({'”©–¥”©—Ä' if gran=='”®–¥”©—Ä' else '—Å–∞—Ä'})"
else:
    # –°–∞—Ä—ã–Ω fallback –≥–æ—Ä–∏–º
    preds = model_forecasts[selected_model].get(selected_h)
    months = len(preds) if preds is not None else 0
    dates_future = pd.date_range(start=grouped["date"].iloc[-1] + pd.offsets.MonthBegin(), periods=months, freq="MS")
    future_df = pd.DataFrame({"date": dates_future, "forecast": preds})
    gran = "–°–∞—Ä"
    title = f"{selected_model} ‚Äî {selected_h} –ø—Ä–æ–≥–Ω–æ–∑ (—Å–∞—Ä)"


fig = px.line(future_df, x="date", y="forecast", markers=True, title=title)
st.plotly_chart(fig, use_container_width=True)

# -------------------------- 1. –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–π–Ω —à–∏–Ω–∂–∏–ª–≥—ç—ç --------------------------

st.header("1. –û—Å–æ–ª–¥ –Ω”©–ª”©”©–ª”©—Ö —Ö“Ø—á–∏–Ω –∑“Ø–π–ª—Å–∏–π–Ω —Ç–∞—Ä—Ö–∞–ª—Ç/–∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π–Ω —à–∏–Ω–∂–∏–ª–≥—ç—ç")
st.write("–î–æ–æ—Ä—Ö multiselect –¥—ç—ç—Ä—ç—ç—Å –∏—Ö–¥—ç—ç 15 —Ö—É–≤—å—Å–∞–≥—á —Å–æ–Ω–≥–æ–∂ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π–Ω –º–∞—Ç—Ä–∏—Ü —Ö–∞—Ä–Ω–∞.")

vars_for_corr = ["Year"]
vars_for_corr += [c for c in df.columns if c.startswith("–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ –∂–∏–ª ")][:10]
vars_for_corr += [c for c in (binary_cols + num_additional) if c in df.columns]
# –î–∞–≤—Ö–∞—Ä–¥–∞–ª –∞—Ä–∏–ª–≥–∞—Ö
vars_for_corr = list(dict.fromkeys(vars_for_corr))

if len(vars_for_corr) > 1:
    Xx = df[vars_for_corr].fillna(0.0).values
    yy = pd.to_numeric(df["–û—Å–æ–ª"], errors="coerce").fillna(0).values
    try:
        rf_cor = RandomForestRegressor(n_estimators=200, random_state=42)
        rf_cor.fit(Xx, yy)
        importances_cor = rf_cor.feature_importances_
        indices_cor = np.argsort(importances_cor)[::-1]
        top_k_cor = min(15, len(vars_for_corr))
        default_cols = [vars_for_corr[i] for i in indices_cor[:top_k_cor]]
    except Exception:
        default_cols = vars_for_corr[: min(15, len(vars_for_corr))]
else:
    default_cols = vars_for_corr

selected_cols = st.multiselect(
    "–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–π–Ω –º–∞—Ç—Ä–∏—Ü–∞–¥ –æ—Ä—É—É–ª–∞—Ö —Ö—É–≤—å—Å–∞–≥—á–∏–¥:", vars_for_corr, default=default_cols, max_selections=15
)
if selected_cols:
    st.pyplot(plot_correlation_matrix(df, "Correlation Matrix", selected_cols))
else:
    st.warning("–°–æ–Ω–≥–æ—Ö —Ö—É–≤—å—Å–∞–≥—á–∏–¥—ã–≥ –æ—Ä—É—É–ª–Ω–∞ —É—É!")

# -------------------------- 2. –û—Å–ª—ã–Ω ”©—Å”©–ª—Ç–∏–π–Ω —Ç—Ä–µ–Ω–¥ --------------------------
# -------------------------- 2. –û—Å–ª—ã–Ω ”©—Å”©–ª—Ç–∏–π–Ω —Ç—Ä–µ–Ω–¥ --------------------------
st.header("2. –û—Å–ª—ã–Ω ”©—Å”©–ª—Ç–∏–π–Ω —Ç—Ä–µ–Ω–¥") 
st.subheader("–ñ–∏–ª, —Å–∞—Ä –±“Ø—Ä—ç—ç—Ä –æ—Å–ª—ã–Ω —Ç–æ–æ–Ω—ã —Ç—Ä–µ–Ω–¥") 
trend_data = ( df[df["–û—Å–æ–ª"] == 1] .groupby(["Year", "Month"]) .agg(osol_count=("–û—Å–æ–ª", "sum")) .reset_index() ) 
trend_data["YearMonth"] = trend_data.apply(lambda x: f"{int(x['Year'])}-{int(x['Month']):02d}", axis=1) 
available_years = sorted(trend_data["Year"].unique()) 
year_options = ["–ë“Ø–≥–¥"] + [str(y) for y in available_years] 
selected_year = st.selectbox("–ñ–∏–ª —Å–æ–Ω–≥–æ—Ö:", year_options) 
plot_df = trend_data if selected_year == "–ë“Ø–≥–¥" else trend_data[trend_data["Year"] == int(selected_year)].copy() 
fig = px.line(plot_df, x="YearMonth", y="osol_count", markers=True, labels={"YearMonth": "–û–Ω-–°–∞—Ä", "osol_count": "–û—Å–ª—ã–Ω —Ç–æ–æ"}, title="") 
fig.update_layout( xaxis_tickangle=45, hovermode="x unified", plot_bgcolor="white", yaxis=dict(title="–û—Å–ª—ã–Ω —Ç–æ–æ", rangemode="tozero"), xaxis=dict(title="–û–Ω-–°–∞—Ä"), ) 
fig.update_traces(line=dict(width=3)) 
st.write("–î–æ–æ—Ä—Ö –≥—Ä–∞—Ñ–∏–∫—Ç –æ—Å–ª—ã–Ω —Ç–æ–æ–Ω—ã ”©”©—Ä—á–ª”©–ª—Ç–∏–π–≥ —Ö–∞—Ä—É—É–ª–∞–≤.") 
st.plotly_chart(fig, use_container_width=True)



# -------------------------- 4. –ö–∞—Ç–µ–≥–æ—Ä–∏ —Ö–∞–º–∞–∞—Ä–∞–ª (Cram√©r‚Äôs V + Chi-square) --------------------------

st.header("4. –ö–∞—Ç–µ–≥–æ—Ä–∏ —Ö—É–≤—å—Å–∞–≥—á–¥—ã–Ω —Ö–æ–æ—Ä–æ–Ω–¥—ã–Ω —Ö–∞–º–∞–∞—Ä–∞–ª (Cram√©r‚Äôs V –±–æ–ª–æ–Ω Chi-square)")

low_card_cols = []
for c in df.columns:
    if c in ["–û—Å–æ–ª", "–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ", "Year", "Month", "Day"]:
        continue
    u = df[c].dropna().unique()
    if 2 <= len(u) <= 15:
        low_card_cols.append(c)

categorical_cols = sorted(list(set(binary_cols + low_card_cols)))
if len(categorical_cols) < 2:
    st.info("–ö–∞—Ç–µ–≥–æ—Ä–∏ –±–∞–≥–∞–Ω–∞ (2-–æ–æ—Å 15 —è–ª–≥–∞–∞—Ç–∞–π —É—Ç–≥–∞—Ç–∞–π) –æ–ª–¥—Å–æ–Ω–≥“Ø–π.")
else:
    var1 = st.selectbox("1-—Ä –∫–∞—Ç–µ–≥–æ—Ä–∏ —Ö—É–≤—å—Å–∞–≥—á:", categorical_cols)
    var2 = st.selectbox("2-—Ä –∫–∞—Ç–µ–≥–æ—Ä–∏ —Ö—É–≤—å—Å–∞–≥—á:", [c for c in categorical_cols if c != var1])

    table = pd.crosstab(df[var1], df[var2])
    chi2, p, dof, expected = chi2_contingency(table)
    n = table.values.sum()
    r, k = table.shape
    cramers_v = np.sqrt(chi2 / (n * (min(k, r) - 1))) if min(k, r) > 1 else np.nan

    st.subheader("1. Chi-square —Ç–µ—Å—Ç")
    st.write("p-value < 0.05 –±–æ–ª —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏–π–Ω —Ö—É–≤—å–¥ —Ö–∞–º–∞–∞—Ä–∞–ª—Ç–∞–π –≥—ç–∂ “Ø–∑–Ω—ç.")
    st.write(f"**Chi-square statistic:** {chi2:.3f}")
    st.write(f"**p-value:** {p:.4f}")
    if p < 0.05:
        st.success("p < 0.05 ‚Üí –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏–π–Ω —Ö—É–≤—å–¥ —Ö–∞–º–∞–∞—Ä–∞–ª—Ç–∞–π!")
    else:
        st.info("p ‚â• 0.05 ‚Üí –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏–π–Ω —Ö—É–≤—å–¥ —Ö–∞–º–∞–∞—Ä–∞–ª–≥“Ø–π.")

    st.subheader("2. Cram√©r‚Äôs V")
    st.write("0-–¥ –æ–π—Ä—Ö–æ–Ω –±–æ–ª –±–∞—Ä–∞–≥ —Ö–∞–º–∞–∞—Ä–∞–ª–≥“Ø–π, 1-–¥ –æ–π—Ä –±–æ–ª —Ö“Ø—á—Ç—ç–π —Ö–∞–º–∞–∞—Ä–∞–ª—Ç–∞–π.")
    st.write(f"**Cram√©r‚Äôs V:** {cramers_v:.3f} (0=—Ö–∞–º–∞–∞—Ä–∞–ª–≥“Ø–π, 1=—Ö“Ø—á—Ç—ç–π —Ö–∞–º–∞–∞—Ä–∞–ª)")

    st.write("**Crosstab:**")
    st.dataframe(table, use_container_width=True)

# -------------------------- –¢”©—Å–ª–∏–π–Ω —Ç”©–≥—Å–≥”©–ª --------------------------
def get_season(month: int) -> str:
    if month in [12, 1, 2]:
        return "”®–≤”©–ª"
    elif month in [3, 4, 5]:
        return "–•–∞–≤–∞—Ä"
    elif month in [6, 7, 8]:
        return "–ó—É–Ω"
    elif month in [9, 10, 11]:
        return "–ù–∞–º–∞—Ä"
    return "–¢–æ–¥–æ—Ä—Ö–æ–π–≥“Ø–π"

df["Season"] = df["–ó”©—Ä—á–∏–ª –æ–≥–Ω–æ–æ"].dt.month.apply(get_season)

# Crosstab + œá¬≤ + Cram√©r‚Äôs V
table = pd.crosstab(df["Season"], df["–¢”©—Ä”©–ª"])
chi2, p, dof, exp = chi2_contingency(table)
n = table.values.sum()
r, k = table.shape
cramers_v = np.sqrt(chi2 / (n*(min(k,r)-1)))

st.subheader("–£–ª–∏—Ä–ª—ã–Ω —è–ª–≥–∞–∞ (œá¬≤ –±–∞ Cram√©r‚Äôs V)")
st.write("**Chi-square statistic:**", round(chi2, 3))
st.write("**p-value:**", round(p, 4))
st.write("**Cram√©r‚Äôs V:**", round(cramers_v, 3))
st.dataframe(table, use_container_width=True)






# -------------------------- 6. Empirical Bayes —à–∏–Ω–∂–∏–ª–≥—ç—ç --------------------------

st.header("6. Empirical Bayes before‚Äìafter —à–∏–Ω–∂–∏–ª–≥—ç—ç (—Å–∞—Ä –±“Ø—Ä)")


def empirical_bayes(obs, exp, prior_mean, prior_var):
    """EB —Ö“Ø–ª—ç—ç–≥–¥—ç–∂ –±—É–π vs –∞–∂–∏–≥–ª–∞–≥–¥—Å–∞–Ω —Ç–æ–æ—Ü–æ–æ–ª–æ–ª"""
    weight = prior_var / (prior_var + exp)
    return weight * obs + (1 - weight) * prior_mean

# –°–∞—Ä –±“Ø—Ä–∏–π–Ω –∞–≥—Ä–µ–≥–∞—Ç (–∞–ª—å —Ö—ç–¥–∏–π–Ω trend_data –¥—ç—ç—Ä –±–∞–π–≥–∞–∞)
monthly = (
    df[df["–û—Å–æ–ª"] == 1]
    .groupby(["Year", "Month"])
    .agg(osol_count=("–û—Å–æ–ª", "sum"))
    .reset_index()
)
monthly["date"] = pd.to_datetime(monthly[["Year", "Month"]].assign(DAY=1))

# Before/After —Ö—É–≤–∞–∞–ª—Ç: 2020‚Äì2022 before, 2023‚Äì2024 after
monthly["period"] = np.where(monthly["Year"] <= 2023, "before", "after")

# –•“Ø–ª—ç—ç–≥–¥—ç–∂ –±—É–π —É—Ç–≥–∞ = before “Ø–µ–∏–π–Ω –¥—É–Ω–¥–∞–∂
expected = monthly[monthly["period"]=="before"]["osol_count"].mean()
prior_mean = expected
prior_var = expected / 2

# EB-–≥ –∑”©–≤—Ö”©–Ω after –¥—ç—ç—Ä —Ç–æ–æ—Ü–Ω–æ
monthly["EB"] = monthly.apply(
    lambda row: empirical_bayes(
        row["osol_count"], expected, prior_mean, prior_var
    ) if row["period"]=="after" else row["osol_count"],
    axis=1
)

# st.write("EB “Ø—Ä –¥“Ø–Ω (—Å–∞—Ä –±“Ø—Ä–∏–π–Ω —Ç“Ø–≤—à–∏–Ω–¥):")
# st.dataframe(monthly.head(24))

# –ì—Ä–∞—Ñ–∏–∫–∞–∞—Ä —Ö–∞—Ä—É—É–ª–∞—Ö
fig = px.line(
    monthly, x="date", y=["osol_count","EB"], 
    color="period", markers=True,
    labels={"value":"–û—Å–æ–ª (—Ç–æ–æ)", "date":"–û–Ω-–°–∞—Ä"},
    title="–û—Å–ª—ã–Ω —Å–∞—Ä –±“Ø—Ä–∏–π–Ω —Ç–æ–æ (EB –∂–∏–Ω–ª—ç–ª–∏–π–Ω –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç)"
)
st.plotly_chart(fig, use_container_width=True)

# -------------------------- 7. Empirical Bayes —à–∏–Ω–∂–∏–ª–≥—ç—ç (–±–∞–π—Ä—à–ª–∞–∞—Ä, —Å–∞—Ä –±“Ø—Ä) --------------------------
st.header("7. Empirical Bayes before‚Äìafter —à–∏–Ω–∂–∏–ª–≥—ç—ç (–±–∞–π—Ä—à–ª–∞–∞—Ä, —Å–∞—Ä –±“Ø—Ä)")

st.markdown("""
EB –∂–∏–Ω–ª—ç–ª–∏–π–Ω –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –≥—ç–∂ —é—É –≤—ç?
Empirical Bayes (EB) –∞—Ä–≥–∞ –Ω—å –±–æ–¥–∏—Ç –∞–∂–∏–≥–ª–∞–≥–¥—Å–∞–Ω (Observed) –±–æ–ª–æ–Ω —Ö“Ø–ª—ç—ç–≥–¥—ç–∂ –±—É–π (Expected) —É—Ç–≥—ã–≥ 
–∂–∏–Ω–ª—ç–∂ –Ω—ç–≥—Ç–≥—ç–¥—ç–≥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫ –∞—Ä–≥–∞—á–ª–∞–ª —é–º. –≠–Ω—ç –Ω—å —Å–∞–Ω–∞–º—Å–∞—Ä–≥“Ø–π —Ö—ç–ª–±—ç–ª–∑–ª—ç—ç—Å —à–∞–ª—Ç–≥–∞–∞–ª—Å–∞–Ω –≥–∞–∂—É—É–¥–ª—ã–≥ 
–±–∞–≥–∞—Å–≥–∞—Ö –∑–æ—Ä–∏–ª–≥–æ—Ç–æ–π.

–ú–∞—Ç–µ–º–∞—Ç–∏–∫ –∑–∞–≥–≤–∞—Ä—á–ª–∞–ª:

EB = w * Observed + (1 - w) * Expected  
w = PriorVar / (PriorVar + Expected)

- **Observed** = —Ç—É—Ö–∞–π–Ω —Å–∞—Ä/–±–∞–π—Ä—à–ª—ã–Ω –æ—Å–ª—ã–Ω —Ç–æ–æ  
- **Expected** = ”©–º–Ω”©—Ö —Ö—É–≥–∞—Ü–∞–∞–Ω—ã –¥—É–Ω–¥–∞–∂ –æ—Å–ª—ã–Ω —Ç–æ–æ  
- **PriorVar** = —Å—É—É—Ä—å —Ö—ç–ª–±—ç–ª–∑—ç–ª (–¥—É–Ω–¥–∞–∂–∏–π–Ω —Ç–∞–ª—ã–≥ –∞—à–∏–≥–ª–∞—Å–∞–Ω)

""")


def empirical_bayes(obs, exp, prior_mean, prior_var):
    """EB –∂–∏–≥–Ω—ç–ª—Ç: –∞–∂–∏–≥–ª–∞–≥–¥—Å–∞–Ω –±–∞ —Ö“Ø–ª—ç—ç–≥–¥—ç–∂ –±—É–π–≥ –Ω—ç–≥—Ç–≥—ç—Ö"""
    weight = prior_var / (prior_var + exp)
    return weight * obs + (1 - weight) * prior_mean

# -------------------------- –ë–∞–π—Ä—à–ª—ã–Ω –±–∞–≥–∞–Ω—ã–≥ —Ç–∞–Ω–∏—Ö --------------------------
loc_col = resolve_col(df, ["–ó–∞–º—ã–Ω –±–∞–π—Ä—à–∏–ª ", "–ë–∞–π—Ä—à–∏–ª", "location", "Road Location"])
if loc_col is None:
    st.error("‚ö†Ô∏è –ó–∞–º—ã–Ω –±–∞–π—Ä—à–ª—ã–Ω –±–∞–≥–∞–Ω–∞ –æ–ª–¥—Å–æ–Ω–≥“Ø–π. Excel –¥—ç—ç—Ä '–ó–∞–º—ã–Ω –±–∞–π—Ä—à–∏–ª' –≥—ç—Ö –º—ç—Ç –±–∞–≥–∞–Ω–∞ –±–∞–π–≥–∞–∞ —ç—Å—ç—Ö–∏–π–≥ —à–∞–ª–≥–∞–Ω–∞ —É—É.")
    st.stop()

# -------------------------- –°–∞—Ä –±“Ø—Ä–∏–π–Ω –∞–≥—Ä–µ–≥–∞—Ç --------------------------
monthly_loc = (
    df[df["–û—Å–æ–ª"] == 1]
    .groupby([loc_col, "Year", "Month"])
    .agg(osol_count=("–û—Å–æ–ª", "sum"))
    .reset_index()
)
monthly_loc["date"] = pd.to_datetime(monthly_loc[["Year", "Month"]].assign(DAY=1))

# -------------------------- Before/After —Ö—É–≥–∞—Ü–∞–∞ —Å–æ–Ω–≥–æ—Ö (–∂–∏–ª—ç—ç—Ä) --------------------------
years = sorted(df["Year"].unique())
col1, col2, col3, col4 = st.columns(4)
with col1:
    before_start = st.selectbox("Before —ç—Ö–ª—ç—Ö –∂–∏–ª", years, index=0)
with col2:
    before_end = st.selectbox("Before –¥—É—É—Å–∞—Ö –∂–∏–ª", years, index=len(years)//2 - 1)
with col3:
    after_start = st.selectbox("After —ç—Ö–ª—ç—Ö –∂–∏–ª", years, index=len(years)//2)
with col4:
    after_end = st.selectbox("After –¥—É—É—Å–∞—Ö –∂–∏–ª", years, index=len(years)-1)

before_range = (pd.to_datetime(f"{before_start}-01-01"), pd.to_datetime(f"{before_end}-12-31"))
after_range = (pd.to_datetime(f"{after_start}-01-01"), pd.to_datetime(f"{after_end}-12-31"))

# -------------------------- Period –æ–Ω–æ–æ—Ö --------------------------
monthly_loc["period"] = np.where(
    (monthly_loc["date"] >= before_range[0]) & (monthly_loc["date"] <= before_range[1]),
    "before",
    np.where(
        (monthly_loc["date"] >= after_range[0]) & (monthly_loc["date"] <= after_range[1]),
        "after",
        "outside"
    )
)

# –∑”©–≤—Ö”©–Ω before/after “Ø–ª–¥—ç—ç–Ω—ç
monthly_loc = monthly_loc[monthly_loc["period"].isin(["before","after"])]

# -------------------------- EB —Ç–æ–æ—Ü–æ–æ–ª–æ–ª --------------------------
results = []
for loc, grp in monthly_loc.groupby(loc_col):
    expected = grp[grp["period"] == "before"]["osol_count"].mean()
    if pd.isna(expected):  # before —Ö–æ–æ—Å–æ–Ω –±–æ–ª –∞–ª–≥–∞—Å–Ω–∞
        continue
    prior_mean = expected
    prior_var = expected / 2

    grp["EB"] = grp.apply(
        lambda row: empirical_bayes(
            row["osol_count"], expected, prior_mean, prior_var
        ) if row["period"] == "after" else row["osol_count"],
        axis=1
    )
    results.append(grp)

if results:
    monthly_loc = pd.concat(results)
else:
    st.warning("‚ö†Ô∏è –°–æ–Ω–≥–æ—Å–æ–Ω —Ö—É–≥–∞—Ü–∞–∞–Ω–¥ EB —Ç–æ–æ—Ü–æ–æ–ª–æ—Ö ”©–≥”©–≥–¥”©–ª –æ–ª–¥—Å–æ–Ω–≥“Ø–π.")

# -------------------------- OUTPUT --------------------------
st.write("EB “Ø—Ä –¥“Ø–Ω (–±–∞–π—Ä—à–ª–∞–∞—Ä, —Å–∞—Ä –±“Ø—Ä):")
st.dataframe(monthly_loc.head(500))

# –ì—Ä–∞—Ñ–∏–∫
if not monthly_loc.empty:
    fig = px.line(
        monthly_loc, x="date", y="EB",
        color=loc_col, line_dash="period", markers=True,
        labels={"EB":"EB-–∂–∏–≥–Ω—ç—Å—ç–Ω –æ—Å–ª—ã–Ω —Ç–æ–æ", "date":"–û–Ω-–°–∞—Ä", loc_col:"–ë–∞–π—Ä—à–∏–ª"},
        title="–û—Å–ª—ã–Ω EB-–∂–∏–≥–Ω—ç–ª—Ç—Ç—ç–π —Ç–æ–æ (–±–∞–π—Ä—à–ª–∞–∞—Ä, —Å–∞—Ä –±“Ø—Ä)"
    )
    st.plotly_chart(fig, use_container_width=True)
# -------------------------- 8. –ë–∞–π—Ä—à–ª—ã–Ω ”©”©—Ä—á–ª”©–ª—Ç”©”©—Ä —ç—Ä—ç–º–±—ç–ª—ç—Ö --------------------------
st.header("8. –•–∞–º–≥–∏–π–Ω —Å–∞–π–Ω / –º—É—É 100 –±–∞–π—Ä—à–ª—ã–Ω –∂–∞–≥—Å–∞–∞–ª—Ç")

# Before / After –¥—É–Ω–¥–∞–∂ EB-–≥ —Ç–æ–æ—Ü–æ–æ–ª–æ—Ö
summary = (
    monthly_loc.groupby([loc_col, "period"])["EB"]
    .mean()
    .reset_index()
    .pivot(index=loc_col, columns="period", values="EB")
    .reset_index()
)

# Before –±–∞ After —è–ª–≥–∞–≤–∞—Ä
summary["Œî"] = summary["after"] - summary["before"]

# –•–∞–º–≥–∏–π–Ω —Å–∞–π–Ω 100 (Œî —Ö–∞–º–≥–∏–π–Ω –±–∞–≥–∞)
best_100 = summary.nsmallest(100, "Œî")

# –•–∞–º–≥–∏–π–Ω –º—É—É 100 (Œî —Ö–∞–º–≥–∏–π–Ω –∏—Ö)
worst_100 = summary.nlargest(100, "Œî")

# ==================== OUTPUT ====================
st.subheader("‚úÖ –•–∞–º–≥–∏–π–Ω —Å–∞–π–Ω 100 –±–∞–π—Ä—à–∏–ª (EB –±—É—É—Ä—Å–∞–Ω)")
st.dataframe(best_100)

st.subheader("‚ùå –•–∞–º–≥–∏–π–Ω –º—É—É 100 –±–∞–π—Ä—à–∏–ª (EB –Ω—ç–º—ç–≥–¥—Å—ç–Ω)")
st.dataframe(worst_100)

# –•“Ø—Å–≤—ç–ª Excel –±–æ–ª–≥–æ–∂ —Ç–∞—Ç–∞–∂ –∞–≤–∞—Ö
def convert_df(df):
    return df.to_csv(index=False).encode("utf-8-sig")

st.download_button(
    "üì• Best 100 (CSV)", convert_df(best_100), "best_100.csv", "text/csv"
)

st.download_button(
    "üì• Worst 100 (CSV)", convert_df(worst_100), "worst_100.csv", "text/csv"
)


st.markdown(
    """
    ---
    **–¢–∞–π–ª–±–∞—Ä**  
    ‚Ä¢ –ó–æ—Ä–∏–ª—Ç–æ—Ç —Ö—É–≤—å—Å–∞–≥—á–∏–π–≥ Sidebar –¥—ç—ç—Ä—ç—ç—Å —Å–æ–Ω–≥–æ—Ö –±–æ–ª–æ–º–∂—Ç–æ–π (–ì—ç–º—Ç —Ö—ç—Ä—ç–≥/–ó”©—Ä—á–ª–∏–π–Ω —Ö—ç—Ä—ç–≥/–•–æ—Å–æ–ª—Å–æ–Ω).  
    ‚Ä¢ –¢–æ–º —Ö—ç–º–∂—ç—ç—Ç—ç–π —Ñ–∞–π–ª—É—É–¥–∞–¥ `@st.cache_data` –∞—á–∞–∞–ª–ª–∞–∞–≥ –±—É—É—Ä—É—É–ª–Ω–∞.  
    ‚Ä¢ –•—ç—Ä—ç–≤ XGBoost/LightGBM/CatBoost —Å—É—É–ª–≥–∞–≥–¥–∞–∞–≥“Ø–π –±–æ–ª —Å—É—É–ª–≥–∞–ª–≥“Ø–π–≥—ç—ç—Ä –±—É—Å–∞–¥ –º–æ–¥–µ–ª“Ø“Ø–¥ –∞–∂–∏–ª–ª–∞–Ω–∞.  
    """
)


# ============================================================
# 9. –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç —É–¥–∞–º—à—É—É–ª–∞—Ö (2024 ‚Üí 2020‚Äì2023) + DBSCAN —à–∏–Ω–∂–∏–ª–≥—ç—ç + –ì–∞–∑—Ä—ã–Ω –∑—É—Ä–∞–≥
# ============================================================

st.header("9. –£–¥–∞–º—à—Å–∞–Ω –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –¥—ç—ç—Ä DBSCAN –∫–ª–∞—Å—Ç–µ—Ä—á–∏–ª–∞–ª")

# --- —à–∞–∞—Ä–¥–ª–∞–≥–∞—Ç–∞–π –±–∞–≥–∞–Ω—É—É–¥ ---
req_cols = ["–ó–∞–º—ã–Ω –∫–æ–¥", "–ê–π–º–∞–≥-–î“Ø“Ø—Ä—ç–≥", "–•–æ—Ä–æ–æ-–°—É–º", "–ó”©—Ä—á–∏–ª –≥–∞—Ä—Å–∞–Ω –≥–∞–∑—Ä—ã–Ω —Ö–∞—è–≥", "–ó–∞–º—ã–Ω –±–∞–π—Ä—à–∏–ª "]
missing_cols = [c for c in req_cols if c not in df.columns]



if not (lat_col and lon_col):
    st.info("–ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã–Ω –±–∞–≥–∞–Ω—É—É–¥ –æ–ª–¥—Å–æ–Ω–≥“Ø–π (”®—Ä–≥”©—Ä”©–≥/–£—Ä—Ç—Ä–∞–≥ —ç—Å–≤—ç–ª lat/lon). –≠–Ω—ç —Ö—ç—Å–≥–∏–π–≥ –∞–ª–≥–∞—Å–ª–∞–∞.")
else:
    # --- 9.1 Reference (2024 –æ–Ω) –±—ç–ª—Ç–≥—ç—Ö ---
    df_2024 = df[df["Year"] == 2024].copy()
    df_2024["ref_key"] = (
        df_2024["–ó–∞–º—ã–Ω –∫–æ–¥"].astype(str) + "_" +
        df_2024["–ê–π–º–∞–≥-–î“Ø“Ø—Ä—ç–≥"].astype(str) + "_" +
        df_2024["–•–æ—Ä–æ–æ-–°—É–º"].astype(str) + "_" +
        df_2024["–ó”©—Ä—á–∏–ª –≥–∞—Ä—Å–∞–Ω –≥–∞–∑—Ä—ã–Ω —Ö–∞—è–≥"].astype(str) + "_" +
        df_2024["–ó–∞–º—ã–Ω –±–∞–π—Ä—à–∏–ª "].astype(str)
    )
    df_2024_grouped = (
        df_2024.groupby("ref_key")[[lat_col, lon_col]]
        .agg(lambda x: x.mode()[0])
        .reset_index()
    )
    ref_dict = df_2024_grouped.set_index("ref_key")[[lat_col, lon_col]].to_dict("index")

    # --- 9.2 –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç —É–¥–∞–º—à—É—É–ª–∞—Ö —Ñ—É–Ω–∫—Ü ---
    try:
        from haversine import haversine
    except Exception:
        haversine = None

    def inherit_coords(row, threshold_m=500):
        if row["Year"] == 2024:
            return row[lat_col], row[lon_col]
        key = (
            str(row["–ó–∞–º—ã–Ω –∫–æ–¥"]) + "_" +
            str(row["–ê–π–º–∞–≥-–î“Ø“Ø—Ä—ç–≥"]) + "_" +
            str(row["–•–æ—Ä–æ–æ-–°—É–º"]) + "_" +
            str(row["–ó”©—Ä—á–∏–ª –≥–∞—Ä—Å–∞–Ω –≥–∞–∑—Ä—ã–Ω —Ö–∞—è–≥"]) + "_" +
            str(row["–ó–∞–º—ã–Ω –±–∞–π—Ä—à–∏–ª "])
        )


        if key in ref_dict:
            ref_lat = ref_dict[key][lat_col]
            ref_lon = ref_dict[key][lon_col]
            if haversine is not None and pd.notna(row[lat_col]) and pd.notna(row[lon_col]):
                try:
                    dist = haversine((ref_lat, ref_lon), (row[lat_col], row[lon_col])) * 1000
                except:
                    dist = np.inf
            else:
                dist = np.inf
            if (41 <= row[lat_col] <= 52) and (87 <= row[lon_col] <= 120):
                return (row[lat_col], row[lon_col]) if dist <= threshold_m else (ref_lat, ref_lon)
            else:
                return ref_lat, ref_lon
        else:
            return np.nan, np.nan

    df[[lat_col, lon_col]] = df.apply(lambda row: inherit_coords(row), axis=1, result_type="expand")
    df = df.dropna(subset=[lat_col, lon_col])

    # --- 9.3 DBSCAN –∫–ª–∞—Å—Ç–µ—Ä—á–∏–ª–∞–ª ---
    coords = df[[lat_col, lon_col]].to_numpy()
    if len(coords) > 5:
        eps_val = st.sidebar.slider("DBSCAN eps (—Ä–∞–¥–∏–∞–Ω)", 0.001, 0.02, 0.005, step=0.001)
        db = DBSCAN(eps=eps_val, min_samples=5, metric="haversine")
        df["cluster_inherited"] = db.fit_predict(np.radians(coords))
    else:
        df["cluster_inherited"] = -1

    # --- 9.4 –ö–ª–∞—Å—Ç–µ—Ä —Ç—Ä–µ–Ω–¥ —Ç–æ–æ—Ü–æ—Ö ---
    trend_list = []
    for cl in df["cluster_inherited"].unique():
        if cl == -1:
            continue
        subset = df[df["cluster_inherited"] == cl]
        counts = subset.groupby("Year").size()
        if counts.shape[0] < 2:
            trend = "—Ç–æ–≥—Ç–≤–æ—Ä—Ç–æ–π"
        else:
            diff = counts.diff().dropna()
            if all(diff > 0):
                trend = "”©—Å”©–ª—Ç"
            elif all(diff < 0):
                trend = "–±—É—É—Ä–∞–ª—Ç"
            elif diff.max() > 2 * abs(diff.mean()): 
                trend = "–æ–≥—Ü–æ–º ”©—Å”©–ª—Ç"
            elif diff.min() < -2 * abs(diff.mean()):
                trend = "–æ–≥—Ü–æ–º –±—É—É—Ä–∞–ª—Ç"
            else:
                trend = "—Ç–æ–≥—Ç–≤–æ—Ä—Ç–æ–π"
        trend_list.append({"cluster": cl, "trend": trend, "—Ç–æ–æ": counts.sum()})

    if trend_list:
        df_trend = pd.DataFrame(trend_list).sort_values("—Ç–æ–æ", ascending=False)
        st.subheader("–ö–ª–∞—Å—Ç–µ—Ä–∏–π–Ω —Ç—Ä–µ–Ω–¥“Ø“Ø–¥ (2020‚Äì2024)")
        st.dataframe(df_trend, use_container_width=True)
    else:
        st.warning("‚ö†Ô∏è DBSCAN-–∞–∞—Ä –∫–ª–∞—Å—Ç–µ—Ä —Ç–æ–¥–æ—Ä—Ö–æ–π–ª–æ–≥–¥—Å–æ–Ω–≥“Ø–π (–±“Ø–≥–¥ -1 –±–æ–ª—Å–æ–Ω –±–∞–π–∂ –º–∞–≥–∞–¥–≥“Ø–π). eps/min_samples —Ç–æ—Ö–∏—Ä–≥–æ–æ–≥ —à–∞–ª–≥–∞–Ω–∞ —É—É.")
        df_trend = pd.DataFrame()


    st.subheader("–ö–ª–∞—Å—Ç–µ—Ä–∏–π–Ω —Ç—Ä–µ–Ω–¥“Ø“Ø–¥ (2020‚Äì2024)")
    st.dataframe(df_trend, use_container_width=True)

    # --- 9.5 Binary —Ö—É–≤—å—Å–∞–≥—á–∏–π–Ω –∞—á —Ö–æ–ª–±–æ–≥–¥–æ–ª ---
    from sklearn.ensemble import RandomForestClassifier

    if binary_cols and len(df) > 0 and df["cluster_inherited"].nunique() > 1:
        X = df[binary_cols]
        y = df["cluster_inherited"].astype(str)

        if len(X) > 0:
            rf = RandomForestClassifier(n_estimators=200, random_state=42)
            rf.fit(X, y)
            feature_imp = pd.DataFrame({
                "feature": binary_cols,
                "importance": rf.feature_importances_
            }).sort_values("importance", ascending=False)
            st.subheader("Binary —Ö—É–≤—å—Å–∞–≥—á–∏–π–Ω –∞—á —Ö–æ–ª–±–æ–≥–¥–æ–ª (RandomForest importance)")
            st.dataframe(feature_imp, use_container_width=True)
        else:
            st.info("‚ö†Ô∏è –ö–ª–∞—Å—Ç–µ—Ä –±–æ–ª–æ–Ω binary —Ö—É–≤—å—Å–∞–≥—á–∏–¥ –æ–≥—Ç–ª–æ–ª—Ü–æ—Ö –º”©—Ä –æ–ª–¥—Å–æ–Ω–≥“Ø–π.")
    else:
        st.info("‚ö†Ô∏è Binary (0/1) –±–∞–≥–∞–Ω–∞ —ç—Å–≤—ç–ª –∫–ª–∞—Å—Ç–µ—Ä–∏–π–Ω ”©–≥”©–≥–¥”©–ª –±–∞–π—Ö–≥“Ø–π —Ç—É–ª importance —Ç–æ–æ—Ü–æ–æ–≥“Ø–π.")


    # --- 9.6 –ì–∞–∑—Ä—ã–Ω –∑—É—Ä–∞–≥ –¥—ç—ç—Ä –¥“Ø—Ä—Å–ª—ç—Ö ---
    import folium
    from streamlit_folium import st_folium
    st.subheader("DBSCAN –∫–ª–∞—Å—Ç–µ—Ä—É—É–¥—ã–Ω –≥–∞–∑—Ä—ã–Ω –∑—É—Ä–∞–≥ (2020‚Äì2024)")

    if len(df) > 0:
        m = folium.Map(location=[47.92, 106.92], zoom_start=5, tiles="OpenStreetMap")

        import matplotlib.cm as cm
        import matplotlib.colors as colors
        clusters = df["cluster_inherited"].unique()
        norm = colors.Normalize(vmin=min(clusters), vmax=max(clusters))
        colormap = cm.ScalarMappable(norm=norm, cmap="tab20")

        trend_dict = dict(zip(df_trend["cluster"], df_trend["trend"]))

        for _, row in df.iterrows():
            cl = row["cluster_inherited"]
            popup_txt = (
                f"–û–Ω: {row['Year']}<br>"
                f"–ó–∞–º—ã–Ω –∫–æ–¥: {row['–ó–∞–º—ã–Ω –∫–æ–¥']}<br>"
                f"–ê–π–º–∞–≥-–î“Ø“Ø—Ä—ç–≥: {str(row['–ê–π–º–∞–≥-–î“Ø“Ø—Ä—ç–≥'])}<br>"
                f"–•–æ—Ä–æ–æ-–°—É–º: {str(row['–•–æ—Ä–æ–æ-–°—É–º'])}<br>"
                f"–ë–∞–π—Ä—à–∏–ª: {str(row['–ó–∞–º—ã–Ω –±–∞–π—Ä—à–∏–ª '])}<br>"
                f"–ö–ª–∞—Å—Ç–µ—Ä: {cl}<br>"
                f"–¢—Ä–µ–Ω–¥: {trend_dict.get(cl, 'N/A')}"
            )
            color = "gray" if cl == -1 else colors.to_hex(colormap.to_rgba(cl))
            folium.CircleMarker(
                location=[row[lat_col], row[lon_col]],
                radius=4,
                color=color,
                fill=True,
                fill_opacity=0.7,
                popup=popup_txt
            ).add_to(m)

        st_folium(m, width=900, height=600)
    else:
        st.info("–ì–∞–∑—Ä—ã–Ω –∑—É—Ä–∞–≥—Ç —Ö–∞—Ä—É—É–ª–∞—Ö –¥–∞—Ç–∞ –∞–ª–≥–∞.")


